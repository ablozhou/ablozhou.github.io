<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="performance"
         xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML"
         xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">
<!--
/**
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
  <title>Apache HBase (TM) 性能调优</title>

  <section xml:id="perf.os">
    <title>操作系统</title>
        <section xml:id="perf.os.ram">
          <title>内存</title>
          <para>RAM, RAM, RAM.  不要让 HBase内存不足。</para>
        </section>
        <section xml:id="perf.os.64">
          <title>64位</title>
          <para>使用64位平台(以及64位JVM)</para>
        </section>
        <section xml:id="perf.os.swap">
          <title>Swapping</title>
          <para>注意内存页交换，请将swappiness设置为0.</para>
        </section>
  </section>
  <section xml:id="perf.network">
    <title>网络</title>
    <para>
      也许，避免网络问题降低Hadoop和HBase性能的最重要因素就是所使用的交换机硬件。 当集群规模增长到原来的两或三倍（甚至更多）时，早期有关网络的决定可能在项目中导致主要的性能问题。
    </para>
    <para>
    关于网络配置，要考虑的重要事项有:
        <itemizedlist>
          <listitem>交换机设备的容量</listitem>
          <listitem>互联的系统数量 </listitem>
          <listitem>上行链路的容量</listitem>
        </itemizedlist>
    </para>
    <section xml:id="perf.network.1switch">
      <title>单交换机</title>
      <para>
	单交换机配置最重要的因素，是硬件交换容量。该容量得能应对连接到该交换机的所有系统所产生的流量。一些廉价硬件商品，相对全交换机，具有较低交换能力。
      </para>
    </section>
    <section xml:id="perf.network.2switch">
      <title>多交换机</title>
      <para>
	多交换机在系统结构中是潜在要害。低价硬件的最常用配置是1Gbps上行连接到另一个交换机。 
	该常被忽略的窄点很容易成为集群通讯的瓶颈。特别是MapReduce任务通过该上行连接同时读写大量数据时，会导致链路饱和。
      </para>
      <para>缓解该问题很简单，可以通过多种途径完成：
      <itemizedlist>
        <listitem>针对要创建的集群容量，采用合适硬件。</listitem>
        <listitem>采用更大单交换机配置，如单48端要比2x 24 端口好一些.</listitem>
        <listitem>配置上行链路聚合(port trunking)来利用多网络接口增加交换机带宽。(译者注：port trunk： 将交换机上的多个端口在物理上连接起来，在逻辑上捆绑在一起，形成一个拥有较大带宽的端口，组成一个干路，以达到平衡负载和提供备份线路，扩充带宽的目的。
)</listitem>
      </itemizedlist>
      </para>
    </section>
    <section xml:id="perf.network.multirack">
      <title>多机架</title>
      <para> 多机架配置存在和多交换机类似的潜在问题。该问题导致性能降低的原因主要来自两个方面：
         <itemizedlist>
           <listitem>较低的交换机容量</listitem>
           <listitem>到其他机架的上行链路不给力</listitem>
         </itemizedlist>
如果机架上的交换机有合适的处理能力，可以处理所有主机的全速通信，那么下一个问题就是如何在机架间自动分发到更多的集群上。最简单的避免横跨多机架问题的办法，是采用端口聚合来创建到其他机架的捆绑的上行的连接。然而该方法缺点是，潜在被使用的端口开销。比如：从机架A到机架B创建 8Gbps 端口通道，采用24端口中的8个来和其他机架互通，ROI(投资回报率)很低。采用太少端口意味着不能从集群中传出最多的东西。
      </para>

      <para>
	机架间采用10Gbe 链接将极大增加性能，确保交换机都支持10Gbe 上行连接或支持扩展卡，后者相对上行连接,可以帮你你节省机器端口。
      </para>
    </section>
    <section xml:id="perf.network.ints">
      <title>网络接口</title>
      <para>所有网络接口的功能都正常吗？你确定？请参考故障诊断案例<xref linkend="casestudies.slownode"/>.
      </para>
    </section>
  </section>  <!-- network -->

  <section xml:id="jvm">
    <title>Java</title>

    <section xml:id="gc">
      <title>HBase与垃圾收集</title>

      <section xml:id="gcpause">
        <title>长时间GC停顿</title>

        <para xml:id="mslab">在该PPT中, <link
        xlink:href="http://www.slideshare.net/cloudera/hbase-hug-presentation">
Avoiding Full GCs with MemStore-Local Allocation Buffers</link>, Todd Lipcon描述了在HBase中常见的两种“世界停止”式的GC操作，尤其是在加载的时候，一种是CMS失败模式(译者注:CMS是一种GC的算法),另一种是老一代的堆碎片导致的。

要想定位第一种，可以将CMS执行的时间提前，加入<code>-XX:CMSInitiatingOccupancyFraction</code>参数，把值调低。 可以先从60%或70%开始(这个值调的越低，触发的GC次数就越多，累计消耗的CPU时间就越长)。 要想定位第二种模式，Todd加入了一个实验性的功能，<indexterm><primary>MSLAB</primary></indexterm>,
要打开该功能，在HBase 0.90.x版本中需要显示指定(在0.92.x中，这个是默认项)。 将你的<classname>Configuration</classname>中的<code>hbase.hregion.memstore.mslab.enabled</code>设置为true。 背景信息和更多细节，请参考该链接<footnote><para>最新的JVM在碎片处理上面做了提升，所以请确保使用的近期发布的版本。 点击这则消息<link xlink:href="http://osdir.com/ml/hotspot-gc-use/2011-11/msg00002.html">识别并发模式下由于碎片导致的GC失败</link> </para></footnote>。
注意当该选项被打开后，每个MemStore的实例都至少会获得一个MSLAB的实力。如果你有上千个region或者有大量有很多列族的区，这些为MSLAB分配的内存可能会占用堆中内存的大部分，在某些极端情况下会导致OOME。在这种情况下，关闭MSLAB，或者降低它使用的总内存大小，或者减少每个服务器上的区数量。</para>

        <para> 更多有关GClog的信息，请参考 <xref linkend="trouble.log.gc" />.
        </para>
      </section>
    </section>
  </section>

  <section xml:id="perf.configurations">
    <title>HBase配置</title>

    <para>参见 <xref linkend="recommended_configurations" />.</para>


    <section xml:id="perf.number.of.regions">
      <title>Region的数量</title>

      <para>一个HBase表格中region的数量可以根据 <xref
              linkend="bigger.regions" />调整. 也可以参考 <xref linkend="arch.regions.size" /></para>
    </section>

    <section xml:id="perf.compactions.and.splits">
      <title>数据压缩的管理</title>

      <para>对于大型系统来说，你可能需要考虑 <link
      linkend="disable.splitting">压缩和分割</link> </para>
    </section>

    <section xml:id="perf.handlers">
        <title><varname>hbase.regionserver.handler.count</varname></title>
        <para>参考 <xref linkend="hbase.regionserver.handler.count"/>.
	    </para>
    </section>
    <section xml:id="perf.hfile.block.cache.size">
        <title><varname>hfile.block.cache.size</varname></title>
        <para>参考 <xref linkend="hfile.block.cache.size"/>.
        针对RegionServer进程中的内存管理进行设置。
        </para>
    </section>
    <section xml:id="perf.rs.memstore.upperlimit">
        <title><varname>hbase.regionserver.global.memstore.upperLimit</varname></title>
        <para>参考 <xref linkend="hbase.regionserver.global.memstore.upperLimit"/>.
        这个内存设置是根据RegionServer进程的需要调整的。
        </para>
    </section>
    <section xml:id="perf.rs.memstore.lowerlimit">
        <title><varname>hbase.regionserver.global.memstore.lowerLimit</varname></title>
        <para>参考 <xref linkend="hbase.regionserver.global.memstore.lowerLimit"/>.
        这个内存设置是根据RegionServer进程的需要调整的。

        </para>
    </section>
    <section xml:id="perf.hstore.blockingstorefiles">
        <title><varname>hbase.hstore.blockingStoreFiles</varname></title>
        <para>参考 <xref linkend="hbase.hstore.blockingStoreFiles"/>.
	如果RegionServer中的log存在blocking现象，提高这个值会有帮助。
        </para>
    </section>
    <section xml:id="perf.hregion.memstore.block.multiplier">
        <title><varname>hbase.hregion.memstore.block.multiplier</varname></title>
        <para>参考 <xref linkend="hbase.hregion.memstore.block.multiplier"/>.
	如果有足够多的RAM，增加这个参数会对性能提升有帮助。
        </para>
    </section>
    <section xml:id="hbase.regionserver.checksum.verify">
        <title><varname>hbase.regionserver.checksum.verify</varname></title>
        <para>让HBase将检验码写入数据库并保存到文件系统上时，在每次读取这些数据库时，HBase也都需要查找校验码。
	参考发行注记中的 <link xlink:href="https://issues.apache.org/jira/browse/HBASE-5074">HBASE-5074 在HBase的块缓存中增加校验码支持</link>.
        </para>
    </section>

  </section>




  <section xml:id="perf.zookeeper">
    <title>ZooKeeper</title>
    <para>有关ZooKeeper的配置信息，请参考 <xref linkend="zookeeper"/>中有关使用专用磁盘的部分
    </para>
  </section>
  <section xml:id="perf.schema">
      <title>模式设计</title>

    <section xml:id="perf.number.of.cfs">
      <title>列组的数量</title>
      <para>参考 <xref linkend="number.of.cfs" />.</para>
    </section>
    <section xml:id="perf.schema.keys">
      <title>键和属性值的长度</title>
      <para>参考 <xref linkend="keysize" />.  有关压缩申请终止(compression caveats）可以参考 <xref linkend="perf.compression.however" /> 
      </para>
    </section>
    <section xml:id="schema.regionsize"><title>表中的Region大小</title>
    <para>当某些表需要与缺省设置的区域大小不同时，Region的大小可以通过<link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html">HTableDescriptor</link>中的 <code>setFileSize</code> 为每张表分别设置.
    </para>
    <para>参考 <xref linkend="perf.number.of.regions"/>获取更多信息。
    </para>
    </section>
    <section xml:id="schema.bloom">
    <title>布隆过滤(Bloom Filters)</title>
    <para>Bloom Filters can be enabled per-ColumnFamily.
        Use <code>HColumnDescriptor.setBloomFilterType(NONE | ROW |
        ROWCOL)</code> to enable blooms per Column Family. Default =
        <varname>NONE</varname> for no bloom filters. If
        <varname>ROW</varname>, the hash of the row will be added to the bloom
        on each insert. If <varname>ROWCOL</varname>, the hash of the row +
        column family + column family qualifier will be added to the bloom on
        each key insert.</para>
    <para>See <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link> and
    <xref linkend="blooms"/> for more information or this answer up in quora,
<link xlink:href="http://www.quora.com/How-are-bloom-filters-used-in-HBase">How are bloom filters used in HBase?</link>.
    </para>
    </section>
    <section xml:id="schema.cf.blocksize"><title>ColumnFamily BlockSize</title>
    <para>The blocksize can be configured for each ColumnFamily in a table, and this defaults to 64k.  Larger cell values require larger blocksizes.
    There is an inverse relationship between blocksize and the resulting StoreFile indexes (i.e., if the blocksize is doubled then the resulting
    indexes should be roughly halved).
    </para>
    <para>See <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link>
    and <xref linkend="store"/>for more information.
    </para>
    </section>
    <section xml:id="cf.in.memory">
    <title>In-Memory ColumnFamilies</title>
    <para>ColumnFamilies can optionally be defined as in-memory.  Data is still persisted to disk, just like any other ColumnFamily.
    In-memory blocks have the highest priority in the <xref linkend="block.cache" />, but it is not a guarantee that the entire table
    will be in memory.
    </para>
    <para>See <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html">HColumnDescriptor</link> for more information.
    </para>
    </section>
    <section xml:id="perf.compression">
      <title>Compression</title>
      <para>Production systems should use compression with their ColumnFamily definitions.  See <xref linkend="compression" /> for more information.
      </para>
      <section xml:id="perf.compression.however"><title>However...</title>
         <para>Compression deflates data <emphasis>on disk</emphasis>.  When it's in-memory (e.g., in the
         MemStore) or on the wire (e.g., transferring between RegionServer and Client) it's inflated.
         So while using ColumnFamily compression is a best practice, but it's not going to completely eliminate
         the impact of over-sized Keys, over-sized ColumnFamily names, or over-sized Column names.
         </para>
         <para>See <xref linkend="keysize" /> on for schema design tips, and <xref linkend="keyvalue"/> for more information on HBase stores data internally.
         </para>
      </section>
    </section>
  </section>  <!--  perf schema -->

  <section xml:id="perf.writing">
    <title>Writing to HBase</title>

    <section xml:id="perf.batch.loading">
      <title>Batch Loading</title>
      <para>Use the bulk load tool if you can.  See
        <xref linkend="arch.bulk.load"/>.
        Otherwise, pay attention to the below.
      </para>
    </section>  <!-- batch loading -->

    <section xml:id="precreate.regions">
    <title>
    Table Creation: Pre-Creating Regions
    </title>
<para>
Tables in HBase are initially created with one region by default.  For bulk imports, this means that all clients will write to the same region 
until it is large enough to split and become distributed across the cluster.  A useful pattern to speed up the bulk import process is to pre-create empty regions. 
 Be somewhat conservative in this, because too-many regions can actually degrade performance.  
</para>
	<para>There are two different approaches to pre-creating splits.  The first approach is to rely on the default <code>HBaseAdmin</code> strategy 
	(which is implemented in <code>Bytes.split</code>)...
	</para>
<programlisting>
byte[] startKey = ...;   	// your lowest keuy
byte[] endKey = ...;   		// your highest key
int numberOfRegions = ...;	// # of regions to create
admin.createTable(table, startKey, endKey, numberOfRegions);
</programlisting>
	<para>And the other approach is to define the splits yourself...
	</para>
<programlisting>
byte[][] splits = ...;   // create your own splits
admin.createTable(table, splits);
</programlisting>
<para>
   See <xref linkend="rowkey.regionsplits"/> for issues related to understanding your keyspace and pre-creating regions.
  </para>
  </section>
    <section xml:id="def.log.flush">
    <title>
    Table Creation: Deferred Log Flush
    </title>
<para>
The default behavior for Puts using the Write Ahead Log (WAL) is that <classname>HLog</classname> edits will be written immediately.  If deferred log flush is used,
WAL edits are kept in memory until the flush period.  The benefit is aggregated and asynchronous <classname>HLog</classname>- writes, but the potential downside is that if
 the RegionServer goes down the yet-to-be-flushed edits are lost.  This is safer, however, than not using WAL at all with Puts.
</para>
<para>
Deferred log flush can be configured on tables via <link
      xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html">HTableDescriptor</link>.  The default value of <varname>hbase.regionserver.optionallogflushinterval</varname> is 1000ms.
</para>
    </section>

    <section xml:id="perf.hbase.client.autoflush">
      <title>HBase Client:  AutoFlush</title>

      <para>When performing a lot of Puts, make sure that setAutoFlush is set
      to false on your <link
      xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html">HTable</link>
      instance. Otherwise, the Puts will be sent one at a time to the
      RegionServer. Puts added via <code> htable.add(Put)</code> and <code> htable.add( &lt;List&gt; Put)</code>
      wind up in the same write buffer. If <code>autoFlush = false</code>,
      these messages are not sent until the write-buffer is filled. To
      explicitly flush the messages, call <methodname>flushCommits</methodname>.
      Calling <methodname>close</methodname> on the <classname>HTable</classname>
      instance will invoke <methodname>flushCommits</methodname>.</para>
    </section>
    <section xml:id="perf.hbase.client.putwal">
      <title>HBase Client:  Turn off WAL on Puts</title>
      <para>A frequently discussed option for increasing throughput on <classname>Put</classname>s is to call <code>writeToWAL(false)</code>.  Turning this off means
          that the RegionServer will <emphasis>not</emphasis> write the <classname>Put</classname> to the Write Ahead Log,
          only into the memstore, HOWEVER the consequence is that if there
          is a RegionServer failure <emphasis>there will be data loss</emphasis>.
          If <code>writeToWAL(false)</code> is used, do so with extreme caution.  You may find in actuality that
          it makes little difference if your load is well distributed across the cluster.
      </para>
      <para>In general, it is best to use WAL for Puts, and where loading throughput
          is a concern to use <link linkend="perf.batch.loading">bulk loading</link> techniques instead.
      </para>
    </section>
    <section xml:id="perf.hbase.client.regiongroup">
      <title>HBase Client: Group Puts by RegionServer</title>
      <para>In addition to using the writeBuffer, grouping <classname>Put</classname>s by RegionServer can reduce the number of client RPC calls per writeBuffer flush.
      There is a utility <classname>HTableUtil</classname> currently on TRUNK that does this, but you can either copy that or implement your own verison for
      those still on 0.90.x or earlier.
      </para>
    </section>
    <section xml:id="perf.hbase.write.mr.reducer">
      <title>MapReduce:  Skip The Reducer</title>
      <para>When writing a lot of data to an HBase table from a MR job (e.g., with <link
      xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html">TableOutputFormat</link>), and specifically where Puts are being emitted
      from the Mapper, skip the Reducer step.  When a Reducer step is used, all of the output (Puts) from the Mapper will get spooled to disk, then sorted/shuffled to other
      Reducers that will most likely be off-node.  It's far more efficient to just write directly to HBase.
      </para>
      <para>For summary jobs where HBase is used as a source and a sink, then writes will be coming from the Reducer step (e.g., summarize values then write out result).
      This is a different processing problem than from the the above case.
      </para>
    </section>

  <section xml:id="perf.one.region">
    <title>Anti-Pattern:  One Hot Region</title>
    <para>If all your data is being written to one region at a time, then re-read the
    section on processing <link linkend="timeseries">timeseries</link> data.</para>
    <para>Also, if you are pre-splitting regions and all your data is <emphasis>still</emphasis> winding up in a single region even though
    your keys aren't monotonically increasing, confirm that your keyspace actually works with the split strategy.  There are a
    variety of reasons that regions may appear "well split" but won't work with your data.   As
    the HBase client communicates directly with the RegionServers, this can be obtained via
    <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#getRegionLocation%28byte[]%29">HTable.getRegionLocation</link>.
    </para>
    <para>See <xref linkend="precreate.regions"/>, as well as <xref linkend="perf.configurations"/> </para>
  </section>

  </section>  <!--  writing -->

  <section xml:id="perf.reading">
    <title>Reading from HBase</title>

    <section xml:id="perf.hbase.client.caching">
      <title>Scan Caching</title>

      <para>If HBase is used as an input source for a MapReduce job, for
      example, make sure that the input <link
      xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html">Scan</link>
      instance to the MapReduce job has <methodname>setCaching</methodname> set to something greater
      than the default (which is 1). Using the default value means that the
      map-task will make call back to the region-server for every record
      processed. Setting this value to 500, for example, will transfer 500
      rows at a time to the client to be processed. There is a cost/benefit to
      have the cache value be large because it costs more in memory for both
      client and RegionServer, so bigger isn't always better.</para>
      <section xml:id="perf.hbase.client.caching.mr">
        <title>Scan Caching in MapReduce Jobs</title>
        <para>Scan settings in MapReduce jobs deserve special attention.  Timeouts can result (e.g., UnknownScannerException)
        in Map tasks if it takes longer to process a batch of records before the client goes back to the RegionServer for the
        next set of data.  This problem can occur because there is non-trivial processing occuring per row.  If you process
        rows quickly, set caching higher.  If you process rows more slowly (e.g., lots of transformations per row, writes),
        then set caching lower.
        </para>
        <para>Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase client doing a Scan), but the
        processing that is often performed in MapReduce jobs tends to exacerbate this issue.
        </para>
      </section>
    </section>
    <section xml:id="perf.hbase.client.selection">
      <title>Scan Attribute Selection</title>

      <para>Whenever a Scan is used to process large numbers of rows (and especially when used
      as a MapReduce source), be aware of which attributes are selected.   If <code>scan.addFamily</code> is called
      then <emphasis>all</emphasis> of the attributes in the specified ColumnFamily will be returned to the client.
      If only a small number of the available attributes are to be processed, then only those attributes should be specified
      in the input scan because attribute over-selection is a non-trivial performance penalty over large datasets.
      </para>
    </section>
    <section xml:id="perf.hbase.mr.input">
        <title>MapReduce - Input Splits</title>
        <para>For MapReduce jobs that use HBase tables as a source, if there a pattern where the "slow" map tasks seem to
        have the same Input Split (i.e., the RegionServer serving the data), see the
        Troubleshooting Case Study in <xref linkend="casestudies.slownode"/>.
        </para>
    </section>

    <section xml:id="perf.hbase.client.scannerclose">
      <title>Close ResultScanners</title>

      <para>This isn't so much about improving performance but rather
      <emphasis>avoiding</emphasis> performance problems. If you forget to
      close <link
      xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/ResultScanner.html">ResultScanners</link>
      you can cause problems on the RegionServers. Always have ResultScanner
      processing enclosed in try/catch blocks... <programlisting>
Scan scan = new Scan();
// set attrs...
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
htable.close();</programlisting></para>
    </section>

    <section xml:id="perf.hbase.client.blockcache">
      <title>Block Cache</title>

      <para><link
      xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html">Scan</link>
      instances can be set to use the block cache in the RegionServer via the
      <methodname>setCacheBlocks</methodname> method. For input Scans to MapReduce jobs, this should be
      <varname>false</varname>. For frequently accessed rows, it is advisable to use the block
      cache.</para>
    </section>
    <section xml:id="perf.hbase.client.rowkeyonly">
      <title>Optimal Loading of Row Keys</title>
      <para>When performing a table <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html">scan</link>
            where only the row keys are needed (no families, qualifiers, values or timestamps), add a FilterList with a
            <varname>MUST_PASS_ALL</varname> operator to the scanner using <methodname>setFilter</methodname>. The filter list
            should include both a <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html">FirstKeyOnlyFilter</link>
            and a <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html">KeyOnlyFilter</link>.
            Using this filter combination will result in a worst case scenario of a RegionServer reading a single value from disk
            and minimal network traffic to the client for a single row.
      </para>
    </section>
   <section xml:id="perf.hbase.read.dist">
      <title>Concurrency:  Monitor Data Spread</title>
      <para>When performing a high number of concurrent reads, monitor the data spread of the target tables.  If the target table(s) have
      too few regions then the reads could likely be served from too few nodes.  </para>
      <para>See <xref linkend="precreate.regions"/>, as well as <xref linkend="perf.configurations"/> </para>
   </section>
     <section xml:id="blooms">
     <title>Bloom Filters</title>
         <para>Enabling Bloom Filters can save your having to go to disk and
         can help improve read latencys.</para>
         <para><link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom filters</link> were developed over in <link
    xlink:href="https://issues.apache.org/jira/browse/HBASE-1200">HBase-1200
    Add bloomfilters</link>.<footnote>
        <para>For description of the development process -- why static blooms
        rather than dynamic -- and for an overview of the unique properties
        that pertain to blooms in HBase, as well as possible future
        directions, see the <emphasis>Development Process</emphasis> section
        of the document <link
        xlink:href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf">BloomFilters
        in HBase</link> attached to <link
        xlink:href="https://issues.apache.org/jira/browse/HBASE-1200">HBase-1200</link>.</para>
      </footnote><footnote>
        <para>The bloom filters described here are actually version two of
        blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom
        option based on work done by the <link
        xlink:href="http://www.one-lab.org">European Commission One-Lab
        Project 034819</link>. The core of the HBase bloom work was later
        pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile.
        Version 1 of HBase blooms never worked that well. Version 2 is a
        rewrite from scratch though again it starts with the one-lab
        work.</para>
      </footnote></para>
        <para>See also <xref linkend="schema.bloom" />.
        </para>

     <section xml:id="bloom_footprint">
      <title>Bloom StoreFile footprint</title>

      <para>Bloom filters add an entry to the <classname>StoreFile</classname>
      general <classname>FileInfo</classname> data structure and then two
      extra entries to the <classname>StoreFile</classname> metadata
      section.</para>

      <section>
        <title>BloomFilter in the <classname>StoreFile</classname>
        <classname>FileInfo</classname> data structure</title>

          <para><classname>FileInfo</classname> has a
          <varname>BLOOM_FILTER_TYPE</varname> entry which is set to
          <varname>NONE</varname>, <varname>ROW</varname> or
          <varname>ROWCOL.</varname></para>
      </section>

      <section>
        <title>BloomFilter entries in <classname>StoreFile</classname>
        metadata</title>

          <para><varname>BLOOM_FILTER_META</varname> holds Bloom Size, Hash
          Function used, etc. Its small in size and is cached on
          <classname>StoreFile.Reader</classname> load</para>
          <para><varname>BLOOM_FILTER_DATA</varname> is the actual bloomfilter
          data. Obtained on-demand. Stored in the LRU cache, if it is enabled
          (Its enabled by default).</para>
      </section>
     </section>
	  <section xml:id="config.bloom">
	    <title>Bloom Filter Configuration</title>
        <section>
        <title><varname>io.hfile.bloom.enabled</varname> global kill
        switch</title>

        <para><code>io.hfile.bloom.enabled</code> in
        <classname>Configuration</classname> serves as the kill switch in case
        something goes wrong. Default = <varname>true</varname>.</para>
        </section>

        <section>
        <title><varname>io.hfile.bloom.error.rate</varname></title>

        <para><varname>io.hfile.bloom.error.rate</varname> = average false
        positive rate. Default = 1%. Decrease rate by ½ (e.g. to .5%) == +1
        bit per bloom entry.</para>
        </section>

        <section>
        <title><varname>io.hfile.bloom.max.fold</varname></title>

        <para><varname>io.hfile.bloom.max.fold</varname> = guaranteed minimum
        fold rate. Most people should leave this alone. Default = 7, or can
        collapse to at least 1/128th of original size. See the
        <emphasis>Development Process</emphasis> section of the document <link
        xlink:href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf">BloomFilters
        in HBase</link> for more on what this option means.</para>
        </section>
        </section>
     </section>   <!--  bloom  -->

  </section>  <!--  reading -->

  <section xml:id="perf.deleting">
    <title>Deleting from HBase</title>
     <section xml:id="perf.deleting.queue">
       <title>Using HBase Tables as Queues</title>
       <para>HBase tables are sometimes used as queues.  In this case, special care must be taken to regularly perform major compactions on tables used in
       this manner.  As is documented in <xref linkend="datamodel" />, marking rows as deleted creates additional StoreFiles which then need to be processed
       on reads.  Tombstones only get cleaned up with major compactions.
       </para>
       <para>See also <xref linkend="compaction" /> and <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29">HBaseAdmin.majorCompact</link>.
       </para>
     </section>
     <section xml:id="perf.deleting.rpc">
       <title>Delete RPC Behavior</title>
       <para>Be aware that <code>htable.delete(Delete)</code> doesn't use the writeBuffer.  It will execute an RegionServer RPC with each invocation.
       For a large number of deletes, consider <code>htable.delete(List)</code>.
       </para>
       <para>See <link xlink:href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29"></link>
       </para>
     </section>
  </section>  <!--  deleting -->

  <section xml:id="perf.hdfs"><title>HDFS</title>
   <para>Because HBase runs on <xref linkend="arch.hdfs" /> it is important to understand how it works and how it affects
   HBase.
   </para>
    <section xml:id="perf.hdfs.curr"><title>Current Issues With Low-Latency Reads</title>
      <para>The original use-case for HDFS was batch processing.  As such, there low-latency reads were historically not a priority.
      With the increased adoption of Apache HBase this is changing, and several improvements are already in development.
      See the
      <link xlink:href="https://issues.apache.org/jira/browse/HDFS-1599">Umbrella Jira Ticket for HDFS Improvements for HBase</link>.
      </para>
    </section>
    <section xml:id="perf.hdfs.configs.localread">
    <title>Leveraging local data</title>
<para>Since Hadoop 1.0.0 (also 0.22.1, 0.23.1, CDH3u3 and HDP 1.0) via
<link xlink:href="https://issues.apache.org/jira/browse/HDFS-2246">HDFS-2246</link>,
it is possible for the DFSClient to take a "short circuit" and
read directly from disk instead of going through the DataNode when the
data is local. What this means for HBase is that the RegionServers can
read directly off their machine's disks instead of having to open a
socket to talk to the DataNode, the former being generally much
faster<footnote><para>See JD's <link xlink:href="http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf">Performance Talk</link></para></footnote>.
Also see <link xlink:href="http://search-hadoop.com/m/zV6dKrLCVh1">HBase, mail # dev - read short circuit</link> thread for
more discussion around short circuit reads.
</para>
<para>To enable "short circuit" reads, you must set two configurations.
First, the hdfs-site.xml needs to be amended. Set
the property  <varname>dfs.block.local-path-access.user</varname>
to be the <emphasis>only</emphasis> user that can use the shortcut.
This has to be the user that started HBase.  Then in hbase-site.xml,
set <varname>dfs.client.read.shortcircuit</varname> to be <varname>true</varname>
</para>
<para>
    For optimal performance when short-circuit reads are enabled, it is recommended that HDFS checksums are disabled.
    To maintain data integrity with HDFS checksums disabled, HBase can be configured to write its own checksums into
    its datablocks and verify against these. See <xref linkend="hbase.regionserver.checksum.verify" />.
</para>
<para>
The DataNodes need to be restarted in order to pick up the new
configuration. Be aware that if a process started under another
username than the one configured here also has the shortcircuit
enabled, it will get an Exception regarding an unauthorized access but
the data will still be read.
</para>
  </section>
    <section xml:id="perf.hdfs.comp"><title>Performance Comparisons of HBase vs. HDFS</title>
     <para>A fairly common question on the dist-list is why HBase isn't as performant as HDFS files in a batch context (e.g., as
     a MapReduce source or sink).  The short answer is that HBase is doing a lot more than HDFS (e.g., reading the KeyValues,
     returning the most current row or specified timestamps, etc.), and as such HBase is 4-5 times slower than HDFS in this
     processing context.  Not that there isn't room for improvement (and this gap will, over time, be reduced), but HDFS
      will always be faster in this use-case.
     </para>
    </section>
  </section>

  <section xml:id="perf.ec2"><title>Amazon EC2</title>
   <para>Performance questions are common on Amazon EC2 environments because it is a shared environment.  You will
   not see the same throughput as a dedicated server.  In terms of running tests on EC2, run them several times for the same
   reason (i.e., it's a shared environment and you don't know what else is happening on the server).
   </para>
   <para>If you are running on EC2 and post performance questions on the dist-list, please state this fact up-front that
    because EC2 issues are practically a separate class of performance issues.
   </para>
  </section>

  <section xml:id="perf.casestudy"><title>Case Studies</title>
      <para>For Performance and Troubleshooting Case Studies, see <xref linkend="casestudies"/>.
      </para>
  </section>
</chapter>
