﻿﻿﻿<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      
   <title>HBase 官方文档</title><link rel="stylesheet" href="src/freebsd_docbook.css" type="text/css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.75.2"><meta name="description" content="这是 Apache HBase的官方文档, Hbase是一个分布式,版本化，构建在 Apache Hadoop和 Apache ZooKeeper上的列数据库."><script>window["_GOOG_TRANS_EXT_VER"] = "1";</script></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="book" title="HBase 官方文档"><div class="titlepage"><div><div>
     <h1 class="title"><a href="http://www.hbase.org/"><img src="src/hbase_logo.png" width="266" height="66" alt="Hbase" longdesc="http://www.hbase.org"></a><a name="d613e2"></a>  <a class="link" href="http://www.hbase.org/" target="_top">HBase</a>
    官方文档中文版</h1></div><div>
      <p class="copyright">Copyright © 2012 Apache Software Foundation.      </p>
</div><div><div class="revhistory"><table border="1" width="100%" summary="Revision history"><tbody><tr><th align="left" valign="top" colspan="2"><b>Revision History</b></th></tr><tr><td align="left">Revision 
            0.95-SNAPSHOT  
        </td><td align="left"> 2012-05-23T20:32 </td></tr><tr><td align="left" colspan="2"><span class="copyright">中文版翻译整理 <a href="http://abloz.com">周海汉</a></span></td></tr></tbody></table></div></div><div><div class="abstract" title="Abstract">
          <p>          
          <hr size="1">
          <span class="QUESTION"><strong>译者</strong>：<a href="http://abloz.com">周海汉</a> 基于 <a href="http://www.yankay.com/">颜开翻译</a> 整理更新。HBase新版 0.95 文档和0.90版相比，变化较大，补充更新了很多内容，章节调整较大。感谢盛大公司<a href="http://www.yankay.com/">颜开</a>的辛勤劳动! 英文原文地址在<a href="http://hbase.apache.org/book/book.html">此处</a>。汉化最后更新请到<a href="http://abloz.com/hbase/book.htm">此处</a>(  <a href="http://abloz.com/hbase/book.htm">http://abloz.com/hbase/book.htm</a> )浏览。还有很多没有翻译全的，以及链接错误，请愿意翻译的到<a href="https://code.google.com/p/hbasedoc-cn/">此处报名(https://code.google.com/p/hbasedoc-cn/)</a>并下载修改上传。贡献者将在此文档署名。谢谢! 最终版生成<a href="https://hbasedoc-cn.googlecode.com/files/HBase%20%E5%AE%98%E6%96%B9%E6%96%87%E6%A1%A3.pdf">pdf</a>供下载。
          </p>
          </span>
          <p class="QUESTION">贡献者：</p>
          <p class="QUESTION">周海汉邮箱：ablozhou@gmail.com, 网址：http://abloz.com/<br>
          颜开邮箱: yankaycom@gmail.com, 网址：http://www.yankay.com/</p>
<hr size="1">
        <p class="title"><b>摘要</b></p>
<p>这是
  <a class="link" href="http://www.hbase.org/" target="_top">Apache HBase</a>的官方文档,
  Hbase是一个分布式,版本化(versioned)，构建在
  <a class="link" href="http://hadoop.apache.org/" target="_top">Apache Hadoop</a>和
  <a class="link" href="http://zookeeper.apache.org/" target="_top">Apache ZooKeeper</a>上的列数据库.
</p>
            <p>&nbsp;</p>
</div>
        </div></div><hr></div><div class="toc">
          <p><b>目录</b></p><dl>
          <div>
            <dl>
              <dt><span class="preface"><a href="hbase.html#preface">序</a></span></dt>
              <dt><span class="chapter"><a href="hbase.html#getting_started">1. 入门</a></span></dt>
              <dd>
                <dl>
                  <dt><span class="section"><a href="hbase.html#d613e75">1.1. 介绍</a></span></dt>
                  <dt><span class="section"><a href="hbase.html#quickstart">1.2. 快速开始</a></span></dt></dl></dd>
              <dt><a href="book.htm#configuration">2. 配置</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#java">2.1. Java</a></dt>
                  <dt><a href="book.htm#os">2.2. 操作系统</a></dt>
                  <dt><a href="book.htm#hadoop">2.3. Hadoop</a></dt>
                  <dt><a href="book.htm#standalone_dist">2.4. </a><a href="hbase.html#standalone_dist">HBase运行模式:单机和分布式</a></dt>
                  <dt><a href="book.htm#zookeeper">2.5. ZooKeeper</a></dt>
                  <dt><a href="book.htm#config.files">2.6. 配置文件</a></dt>
                  <dt><a href="book.htm#example_config">2.7. 配置示例</a></dt>
                  <dt><a href="book.htm#important_configurations">2.8. 重要配置</a></dt>
                  <dt><a href="book.htm#config.bloom">2.9. Bloom Filter </a></dt></dl></dd>
              <dt><a href="book.htm#upgrading">3. 升级</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#upgrade0.90">3.1. </a><a href="hbase.html#upgrade0.90">从HBase 0.20.x or 0.89.x 升级到 HBase 0.90.x</a></dt>
                  <dt><a href="book.htm#upgrade0.92">3.2. 从 0.90.x 到 0.92.x</a><a href="book.htm#upgrade0.92"></a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#shell">4. The HBase Shell</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#scripting">4.1. </a><a href="hbase.html#scripting">使用脚本</a></dt>
                  <dt><a href="book.htm#shell_tricks">4.2. </a><a href="hbase.html#shell_tricks">Shell 技巧</a></dt></dl></dd>
              <dt><a href="book.htm#datamodel">5. </a><a href="hbase.html#datamodel">数据模型</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#conceptual.view">5.1. </a><a href="hbase.html#conceptual.view">概念视图</a></dt>
                  <dt><a href="book.htm#physical.view">5.2. </a><a href="hbase.html#physical.view">物理视图</a></dt>
                  <dt><a href="book.htm#table">5.3. </a><a href="hbase.html#table">表</a></dt>
                  <dt><a href="book.htm#row">5.4. </a><a href="hbase.html#row">行</a></dt>
                  <dt><a href="book.htm#columnfamily">5.5. 列族</a></dt>
                  <dt><a href="book.htm#cells">5.6. Cells</a></dt>
                  <dt><a href="book.htm#data_model_operations">5.7. Data Model Operations</a></dt>
                  <dt><a href="book.htm#versions">5.8. </a><a href="hbase.html#versions">版本</a></dt>
                  <dt><a href="book.htm#dm.sort">5.9. 排序</a></dt>
                  <dt><a href="book.htm#dm.column.metadata">5.10. 列元数据</a></dt>
                  <dt><a href="book.htm#joins">5.11. Joins</a></dt></dl></dd>
              <dt><a href="book.htm#schema">6. HBase 和 Schema 设计</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#schema.creation">6.1. Schema </a><a href="hbase.html#schema.creation">创建</a></dt>
                  <dt><a href="book.htm#number.of.cfs">6.2. </a><a href="hbase.html#number.of.cfs">column families的数量</a></dt>
                  <dt><a href="book.htm#rowkey.design">6.3. Rowkey 设计</a></dt>
                  <dt><a href="book.htm#schema.versions">6.4. Number 数量</a></dt>
                  <dt><a href="book.htm#supported.datatypes">6.5. 支持的数据类型</a></dt>
                  <dt><a href="book.htm#schema.joins">6.6. Joins</a></dt>
                  <dt><a href="book.htm#ttl">6.7. 生存时间 (TTL)</a></dt>
                  <dt><a href="book.htm#cf.keep.deleted">6.8. Keeping Deleted Cells</a></dt>
                  <dt><a href="book.htm#secondary.indexes">6.9. Secondary Indexes and Alternate Query Paths</a></dt>
                  <dt><a href="book.htm#schema.smackdown">6.10. Schema Design Smackdown</a></dt>
                  <dt><a href="book.htm#schema.ops">6.11. Operational and Performance Configuration Options</a></dt>
                  <dt><a href="book.htm#constraints">6.12. 限制</a>              </dt>
                </dl>
              </dd>
              <dt><a href="book.htm#mapreduce">7. HBase 和 MapReduce</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#splitter">7.1. Map-Task Spitting</a></dt>
                  <dt><a href="book.htm#mapreduce.example">7.2. HBase MapReduce Examples</a></dt>
                  <dt><a href="book.htm#mapreduce.htable.access">7.3. Accessing Other HBase Tables in a MapReduce Job</a></dt>
                  <dt><a href="book.htm#mapreduce.specex">7.4. Speculative Execution</a></dt></dl></dd>
              <dt><a href="book.htm#security">8.  HBase安全</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#hbase.secure.configuration">8.1. 安全客户端访问 HBase</a></dt>
                  <dt><a href="book.htm#hbase.accesscontrol.configuration">8.2. 访问控制</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#architecture">9. 架构</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#arch.overview">9.1. 概述</a></dt>
                  <dt><a href="book.htm#arch.catalog">9.2. Catalog Tables</a></dt>
                  <dt><a href="book.htm#client">9.3. 客户端</a></dt>
                  <dt><a href="book.htm#client.filter">9.4. Client Request Filters</a></dt>
                  <dt><a href="book.htm#master">9.5. Master</a></dt>
                  <dt><a href="book.htm#regionserver.arch">9.6. RegionServer</a></dt>
                  <dt><a href="book.htm#regions.arch">9.7. Regions</a></dt>
                  <dt><a href="book.htm#arch.bulk.load">9.8. Bulk Loading</a></dt>
                  <dt><a href="book.htm#arch.hdfs">9.9. HDFS</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#external_apis">10. 外部 APIs</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#nonjava.jvm">10.1. 非Java语言和 JVM交互</a></dt>
                  <dt><a href="book.htm#rest">10.2. REST</a></dt>
                  <dt><a href="book.htm#thrift">10.3. Thrift</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#performance">11. 性能调优</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#perf.os">11.1. 操作系统</a></dt>
                  <dt><a href="book.htm#perf.network">11.2. 网络</a></dt>
                  <dt><a href="book.htm#jvm">11.3. Java</a></dt>
                  <dt><a href="book.htm#perf.configurations">11.4. HBase 配置</a></dt>
                  <dt><a href="book.htm#perf.zookeeper">11.5. ZooKeeper</a></dt>
                  <dt><a href="book.htm#perf.schema">11.6. Schema 设计</a></dt>
                  <dt><a href="book.htm#perf.writing">11.7. 写到 HBase</a></dt>
                  <dt><a href="book.htm#perf.reading">11.8. 从 HBase读取</a></dt>
                  <dt><a href="book.htm#perf.deleting">11.9. 从 HBase删除</a></dt>
                  <dt><a href="book.htm#perf.hdfs">11.10. HDFS</a></dt>
                  <dt><a href="book.htm#perf.ec2">11.11. Amazon EC2</a></dt>
                  <dt><a href="book.htm#perf.casestudy">11.12. 案例</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#trouble">12. 故障排除和调试 HBase</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#trouble.general">12.1. 通用指引</a></dt>
                  <dt><a href="book.htm#trouble.log">12.2. Logs</a></dt>
                  <dt><a href="book.htm#trouble.resources">12.3. 资源</a></dt>
                  <dt><a href="book.htm#trouble.tools">12.4. 工具</a></dt>
                  <dt><a href="book.htm#trouble.client">12.5. 客户端</a></dt>
                  <dt><a href="book.htm#trouble.mapreduce">12.6. MapReduce</a></dt>
                  <dt><a href="book.htm#trouble.namenode">12.7. NameNode</a></dt>
                  <dt><a href="book.htm#trouble.network">12.8. 网络</a></dt>
                  <dt><a href="book.htm#trouble.rs">12.9. RegionServer</a></dt>
                  <dt><a href="book.htm#trouble.master">12.10. Master</a></dt>
                  <dt><a href="book.htm#trouble.zookeeper">12.11. ZooKeeper</a></dt>
                  <dt><a href="book.htm#trouble.ec2">12.12. Amazon EC2</a></dt>
                  <dt><a href="book.htm#trouble.versions">12.13. HBase 和 Hadoop 版本相关</a></dt>
                  <dt><a href="book.htm#trouble.casestudy">12.14. 案例</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#casestudies">13. 案例研究</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#casestudies.overview">13.1. 概要</a></dt>
                  <dt><a href="book.htm#casestudies.schema">13.2. Schema 设计</a></dt>
                  <dt><a href="book.htm#casestudies.perftroub">13.3. 性能/故障排除</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#ops_mgt">14. HBase Operational Management</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#tools">14.1. HBase Tools and Utilities</a></dt>
                  <dt><a href="book.htm#ops.regionmgt">14.2. Region Management</a></dt>
                  <dt><a href="book.htm#node.management">14.3. Node Management</a></dt>
                  <dt><a href="book.htm#hbase_metrics">14.4. HBase Metrics</a></dt>
                  <dt><a href="book.htm#ops.monitoring">14.5. HBase Monitoring</a></dt>
                  <dt><a href="book.htm#cluster_replication">14.6. Cluster Replication</a></dt>
                  <dt><a href="book.htm#ops.backup">14.7. HBase Backup</a></dt>
                  <dt><a href="book.htm#ops.capacity">14.8. Capacity Planning</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#developer">15. 创建和开发 HBase</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#repos">15.1. HBase 仓库</a></dt>
                  <dt><a href="book.htm#ides">15.2. IDEs</a></dt>
                  <dt><a href="book.htm#build">15.3. 创建 HBase</a></dt>
                  <dt><a href="book.htm#hbase.site.publishing">15.4. Publishing a new version of hbase.apache.org</a></dt>
                  <dt><a href="book.htm#hbase.tests">15.5. 测试</a></dt>
                  <dt><a href="book.htm#maven.build.commands">15.6. Maven Build Commands</a></dt>
                  <dt><a href="book.htm#getting.involved">15.7. Getting Involved</a></dt>
                  <dt><a href="book.htm#developing">15.8. 开发</a></dt>
                  <dt><a href="book.htm#submitting.patches">15.9. 提交补丁</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#faq">A. FAQ</a></dt>
              <dt><a href="book.htm#hbck.in.depth">B. hbck In Depth</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#d1934e10593">B.1. Running hbck to identify inconsistencies</a></dt>
                  <dt><a href="book.htm#apbs02">B.2. Inconsistencies</a></dt>
                  <dt><a href="book.htm#apbs03">B.3. Localized repairs</a></dt>
                  <dt><a href="book.htm#apbs04">B.4. Region Overlap Repairs</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#compression">C. Compression In HBase</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#compression.test">C.1. CompressionTest Tool</a></dt>
                  <dt><a href="book.htm#hbase.regionserver.codecs">C.2. hbase.regionserver.codecs</a></dt>
                  <dt><a href="book.htm#lzo.compression">C.3. LZO</a></dt>
                  <dt><a href="book.htm#gzip.compression">C.4. GZIP</a></dt>
                  <dt><a href="book.htm#snappy.compression">C.5. SNAPPY</a></dt>
                  <dt><a href="book.htm#changing.compression">C.6. Changing Compression Schemes</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#apd">D. YCSB: The Yahoo! Cloud Serving Benchmark and HBase</a></dt>
              <dt><a href="book.htm#hfilev2">E. HFile format version 2</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#d1934e10848">E.1. Motivation</a></dt>
                  <dt><a href="book.htm#apes02">E.2. HFile format version 1 overview</a></dt>
                  <dt><a href="book.htm#apes03">E.3. HBase file format with inline blocks (version 2)</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#other.info">F. Other Information About HBase</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#other.info.videos">F.1. HBase Videos</a></dt>
                  <dt><a href="book.htm#other.info.pres">F.2. HBase Presentations (Slides)</a></dt>
                  <dt><a href="book.htm#other.info.papers">F.3. HBase Papers</a></dt>
                  <dt><a href="book.htm#other.info.sites">F.4. HBase Sites</a></dt>
                  <dt><a href="book.htm#other.info.books">F.5. HBase Books</a></dt>
                  <dt><a href="book.htm#other.info.books.hadoop">F.6. Hadoop Books</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#hbase.history">G. HBase History</a></dt>
              <dt><a href="book.htm#asf">H. HBase and the Apache Software Foundation</a></dt>
              <dd>
                <dl>
                  <dt><a href="book.htm#asf.devprocess">H.1. ASF Development Process</a></dt>
                  <dt><a href="book.htm#asf.reporting">H.2. ASF Board Reporting</a></dt>
                </dl>
              </dd>
              <dt><a href="book.htm#book_index">Index</a></dt>
            </dl>
          </div>
          <div>
            <p><strong>表列表</strong></p>
            <dl>
              <dt>5.1. <a href="book.htm#d1934e3221">Table webtable</a></dt>
              <dt>5.2. <a href="book.htm#d1934e3305">ColumnFamily anchor</a></dt>
              <dt>5.3. <a href="book.htm#d1934e3344">ColumnFamily contents</a></dt>
              <dt>8.1. <a href="book.htm#d1934e4513">Operation To Permission Mapping</a></dt>
            </dl>
          </div>
<dt>&nbsp;</dt></dl></div><div class="list-of-tables"></div><div class="preface" title="序"><div class="titlepage"><div><div><h2 class="title"><a name="preface"></a>序</h2></div></div></div>
    <p>这本书是 <a class="link" href="http://hbase.apache.org/" target="_top">HBase</a> 的官方指南。
  版本为  <em>0.95-SNAPSHOT</em> 。可以在Hbase官网上找到它。也可以在 <a class="link" href="http://hbase.apache.org/docs/current/api/index" target="_top">javadoc</a>,
  <a class="link" href="https://issues.apache.org/jira/browse/HBASE" target="_top">JIRA</a>
和 <a class="link" href="http://wiki.apache.org/hadoop/Hbase" target="_top">wiki</a> 找到更多的资料。</p><p>此书正在编辑中。 可以向 HBase 官方提供补丁<a class="link" href="https://issues.apache.org/jira/browse/HBASE" target="_top">JIRA</a>.</p><p>这个版本系译者水平限制，没有理解清楚或不需要翻译的地方保留英文原文。</p><div class="note" title="最前面的话" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="headsup"></a>最前面的话</h3><p>
	      若这是你第一次踏入分布式计算的精彩世界，你会感到这是一个有趣的年代。分布式计算是很难的，做一个分布式系统需要很多软硬件和网络的技能。你的集群可以会因为各式各样的错误发生故障。比如Hbase本身的Bug,错误的配置(包括操作系统)，硬件的故障(网卡和磁盘甚至内存)
		  如果你一直在写单机程序的话，你需要重新开始学习。这里就是一个好的起点:

          <a class="link" href="http://en.wikipedia.org/wiki/Fallacies_of_Distributed_Computing" target="_top">分布式计算的谬论</a>.
      </p></div></div><div class="chapter" title="Chapter 1. 入门"><div class="titlepage"><div><div><h2 class="title"><a name="getting_started"></a>Chapter&nbsp;1.&nbsp;入门<a name="getting_started" id="getting_started"></a></h2>
      </div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="book.htm#d613e75">1.1. 介绍</a></span></dt><dt><span class="section"><a href="book.htm#quickstart">1.2. 快速开始</a></span></dt><dd><dl><dt><span class="section"><a href="book.htm#d613e91">1.2.1. 下载解压最新版本</a></span></dt><dt><span class="section"><a href="book.htm#start_hbase">1.2.2. 启动 HBase</a></span></dt><dt><span class="section"><a href="book.htm#shell_exercises">1.2.3. Shell 练习</a></span></dt><dt><span class="section"><a href="book.htm#stopping">1.2.4. 停止 HBase</a></span></dt><dt><span class="section"><a href="book.htm#d613e242">1.2.5. 下一步该做什么</a></span></dt></dl></dd><dd>&nbsp;</dd></dl></div><div class="section" title="1.1. 介绍"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d613e75"></a>1.1.&nbsp;介绍</h2></div></div></div>
      <p><a class="xref" href="book.htm#quickstart" title="1.2. 快速开始">Section&nbsp;1.2, “快速开始”</a>会介绍如何运行一个单机版的Hbase.他运行在本地磁盘上。
    <a class="xref" href="book.htm#notsoquick" title="2. 慢速开始(相对快速开始)">Section&nbsp;2, “配置”</a> 会介绍如何运行一个分布式的Hbase。他运行在HDFS上</p></div><div class="section" title="1.2. 快速开始"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="quickstart"></a>1.2.&nbsp;快速开始</h2></div></div></div><p>本指南介绍了在单机安装Hbase的方法。会引导你通过<span class="command"><strong>shell</strong></span>创建一个表，插入一行，然后删除它，最后停止Hbase。只要10分钟就可以完成以下的操作。</p><div class="section" title="1.2.1. 下载解压最新版本"><div class="titlepage"><div><div><h3 class="title"><a name="d613e91"></a>1.2.1.&nbsp;下载解压最新版本</h3></div></div></div><p>选择一个 <a class="link" href="http://www.apache.org/dyn/closer.cgi/hbase/" target="_top">Apache 下载镜像</a>，下载 <span class="emphasis"><em>HBase Releases</em></span>. 点击
      <code class="filename">stable</code>目录，然后下载后缀为 
      <code class="filename">.tar.gz</code> 的文件; 例如
      <code class="filename">hbase-0.95-SNAPSHOT.tar.gz</code>.</p><p>解压缩，然后进入到那个要解压的目录.</p><pre class="programlisting">$ tar xfz hbase-0.95-SNAPSHOT.tar.gz
$ cd hbase-0.95-SNAPSHOT
</pre><p>现在你已经可以启动Hbase了。但是你可能需要先编辑 <code class="filename">conf/hbase-site.xml</code> 去配置<code class="varname">hbase.rootdir</code>，来选择Hbase将数据写到哪个目录
      . </p><pre class="programlisting">
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;file:///DIRECTORY/hbase&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;

</pre><p> 将 <code class="varname">DIRECTORY</code> 替换成你期望写文件的目录. 默认
      <code class="varname">hbase.rootdir</code> 是指向
      <code class="filename">/tmp/hbase-${user.name}</code> ，也就说你会在重启后丢失数据(重启的时候操作系统会清理<code class="filename">/tmp</code>目录)</p></div><div class="section" title="1.2.2. 启动 HBase"><div class="titlepage"><div><div><h3 class="title"><a name="start_hbase"></a>1.2.2.&nbsp;启动 HBase</h3></div></div></div><p>现在启动Hbase:</p><pre class="programlisting">$ ./bin/start-hbase.sh
starting Master, logging to logs/hbase-user-master-example.org.out</pre><p>现在你运行的是单机模式的Hbaes。所以的服务都运行在一个JVM上，包括Hbase和Zookeeper。Hbase的日志放在<code class="filename">logs</code>目录,当你启动出问题的时候，可以检查这个日志。
      </p><div class="note" title="是否安装了 java ?" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">是否安装了 <span class="application">java</span> ?</h3><p>你需要确认安装了Oracle的1.6 版本的<span class="application">java</span>.如果你在命令行键入java有反应说明你安装了Java。如果没有装，你需要先安装，然后编辑<code class="filename">conf/hbase-env.sh</code>，将其中的<code class="envar">JAVA_HOME</code>指向到你Java的安装目录。</p></div></div><div class="section" title="1.2.3. Shell 练习"><div class="titlepage"><div><div><h3 class="title"><a name="shell_exercises"></a>1.2.3.&nbsp;Shell 练习</h3></div></div></div><p>用<span class="command"><strong>shell</strong></span>连接你的Hbase</p><pre class="programlisting">$ ./bin/hbase shell
HBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.
Type "exit&lt;RETURN&gt;" to leave the HBase Shell
Version: 0.90.0, r1001068, Fri Sep 24 13:55:42 PDT 2010

hbase(main):001:0&gt; </pre><p>输入 <span class="command"><strong>help</strong></span> 然后
      <span class="command"><strong>&lt;RETURN&gt;</strong></span> 可以看到一列shell命令。这里的帮助很详细，要注意的是表名，行和列需要加引号。</p><p>创建一个名为 <code class="varname">test</code> 的表，这个表只有一个 column family 为 <code class="varname">cf</code>。可以列出所有的表来检查创建情况，然后插入些值。
</p><pre class="programlisting">hbase(main):003:0&gt; create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0&gt; list 'table'
test
1 row(s) in 0.0550 seconds
hbase(main):004:0&gt; put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0560 seconds
hbase(main):005:0&gt; put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0370 seconds
hbase(main):006:0&gt; put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0450 seconds</pre><p>以上我们分别插入了3行。第一个行key为<code class="varname">row1</code>, 列为 <code class="varname">cf:a</code>， 值是
      <code class="varname">value1</code>。Hbase中的列是由 column family前缀和列的名字组成的，以冒号间隔。例如这一行的列名就是<code class="varname">a</code>.</p><p>检查插入情况.</p><p>Scan这个表，操作如下</p><pre class="programlisting">hbase(main):007:0&gt; scan 'test'
ROW        COLUMN+CELL
row1       column=cf:a, timestamp=1288380727188, value=value1
row2       column=cf:b, timestamp=1288380738440, value=value2
row3       column=cf:c, timestamp=1288380747365, value=value3
3 row(s) in 0.0590 seconds</pre><p>Get一行，操作如下</p><pre class="programlisting">hbase(main):008:0&gt; get 'test', 'row1'
COLUMN      CELL
cf:a        timestamp=1288380727188, value=value1
1 row(s) in 0.0400 seconds</pre><p>disable 再 drop 这张表，可以清除你刚刚的操作</p><pre class="programlisting">hbase(main):012:0&gt; disable 'test'
0 row(s) in 1.0930 seconds
hbase(main):013:0&gt; drop 'test'
0 row(s) in 0.0770 seconds </pre><p>关闭shell</p><pre class="programlisting">hbase(main):014:0&gt; exit</pre></div><div class="section" title="1.2.4. 停止 HBase"><div class="titlepage"><div><div><h3 class="title"><a name="stopping"></a>1.2.4.&nbsp;停止 HBase</h3></div></div></div><p>运行停止脚本来停止HBase.</p><pre class="programlisting">$ ./bin/stop-hbase.sh
stopping hbase...............</pre></div><div class="section" title="1.2.5. 下一步该做什么"><div class="titlepage"><div><div><h3 class="title"><a name="d613e242"></a>1.2.5.&nbsp;下一步该做什么</h3></div></div></div>
<p>以上步骤仅仅适用于实验和测试。接下来你可以看 <a class="xref" href="book.htm#notsoquick" title="2. 配置">Section&nbsp;2., “配置”</a> ，我们会介绍不同的Hbase运行模式，运行分布式Hbase中需要的软件 和如何配置。</p></div></div><div class="section" title="2. 慢速开始(相对快速开始)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="configuration" id="configuration"></a>2.&nbsp;配置</h2></div></div></div><div class="section" title="2.1. 需要的软件"><div class="titlepage"><div><div><h3 class="title">&nbsp;</h3></div></div></div>
<p>本章是慢速开始配置指导。</p>
    <p>Hbase有如下需要，请仔细阅读本章节以确保所有的需要都被满足。如果需求没有能满足，就有可能遇到莫名其妙的错误甚至丢失数据。</p>
    <p>Hbase使用和Hadoop一样配置系统。To configure a deploy, edit a file of environment variables in conf/hbase-env.sh -- this configuration is used mostly by the launcher shell scripts getting the cluster off the ground -- and then add configuration to an XML file to do things like override HBase defaults, tell HBase what Filesystem to use, and the location of the ZooKeeper ensemble [<a name="d1934e270" href="#zoo1">1</a>] .</p>
    <p>When running in distributed mode, after you make an edit to an HBase configuration, make sure you copy the content of the conf directory to all nodes of the cluster. HBase will not do this for you. Use <strong>rsync</strong>.</p>
    <p> [<a name="zoo1"></a><a href="#d1934e270">1</a>] Be careful editing XML. Make sure you close all elements. Run your file through <strong>xmllint</strong> or similar to ensure well-formedness of your document after an edit session. </p>
<div class="section" title="2.1. java"><div class="titlepage"><div><div><h4 class="title"><a name="java"></a>2.1.&nbsp;java</h4></div></div></div><p>和Hadoop一样，Hbase需要Oracle版本的<a class="link" href="http://www.java.com/download/" target="_top">Java6</a>.除了那个有问题的u18版本其他的都可以用，最好用最新的。
		</p></div>
<div class="section" title="2.2. 操作系统"><div class="titlepage"><div><div><h3 class="title"><a name="os"></a>2.2. 操作系统</h3></div></div></div>
        <div class="section" title="2.1.3. ssh"><div class="titlepage"><div><div><h4 class="title"><a name="ssh"></a>2.2.1.&nbsp;ssh</h4></div></div></div><p>必须安装<span class="command"><strong>ssh</strong></span> ，
        <span class="command"><strong>sshd</strong></span> 也必须运行，这样Hadoop的脚本才可以远程操控其他的Hadoop和Hbase进程。ssh之间必须都打通，不用密码都可以登录，详细方法可以Google一下 ("ssh passwordless login").</p></div><div class="section" title="2.1.4. DNS"><div class="titlepage"><div><div>
          <h4 class="title"><a name="dns"></a>2.2.2.&nbsp;DNS</h4></div></div></div><p>HBase使用本地 hostname 才获得IP地址.
       正反向的DNS都是可以的.</p><p>如果你的机器有多个接口，Hbase会使用hostname指向的主接口.</p><p>如果还不够，你可以设置
        <code class="varname">hbase.regionserver.dns.interface</code> 来指定主接口。当然你的整个集群的配置文件都必须一致，每个主机都使用相同的网络接口 </p><p>还有一种方法是设置
        <code class="varname">hbase.regionserver.dns.nameserver</code>来指定nameserver，不使用系统带的.</p></div>
        <div class="section" title="2.2.3.&nbsp;Loopback IP"><div class="titlepage"><div><div><h3 class="title"><a name="loopback.ip"></a>2.2.3.&nbsp;Loopback IP</h3></div></div></div><p>HBase expects the loopback IP address to be 127.0.0.1.  Ubuntu and some other distributions,
        for example, will default to 127.0.1.1 and this will cause problems for you.
        </p><p><code class="filename">/etc/hosts</code> should look something like this:
</p><pre class="programlisting">            127.0.0.1 localhost
            127.0.0.1 ubuntu.ubuntu-domain ubuntu
</pre><p>
        </p></div>
        <div class="section" title="2.1.5. NTP"><div class="titlepage"><div><div>
          <h4 class="title"><a name="ntp"></a>2.2.4.&nbsp;NTP</h4></div></div></div><p>集群的时钟要保证基本的一致。稍有不一致是可以容忍的，但是很大的不一致会造成奇怪的行为。 运行
        <a class="link" href="http://en.wikipedia.org/wiki/Network_Time_Protocol" target="_top">NTP</a>
        或者其他什么东西来同步你的时间.</p><p>如果你查询的时候或者是遇到奇怪的故障，可以检查一下系统时间是否正确!</p></div><div class="section" title="2.1.6.  ulimit 和 nproc"><div class="titlepage"><div><div>
          <h4 class="title"><a name="ulimit"></a>2.2.5.&nbsp;
          <code class="varname">ulimit</code><a class="indexterm" name="d613e365"></a>
            和
          <code class="varname">nproc</code><a class="indexterm" name="d613e371"></a>
        </h4></div></div></div><p>HBase是数据库，会在同一时间使用很多的文件句柄。大多数linux系统使用的默认值1024是不能满足的，会导致<a class="link" href="http://wiki.apache.org/hadoop/Hbase/FAQ#A6" target="_top">FAQ: Why do I
        see "java.io.IOException...(Too many open files)" in my logs?</a>异常。还可能会发生这样的异常
         </p><pre class="programlisting">      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Exception increateBlockOutputStream java.io.EOFException
      2010-04-06 03:04:37,542 INFO org.apache.hadoop.hdfs.DFSClient: Abandoning block blk_-6935524980745310745_1391901
      </pre><p> 所以你需要修改你的最大文件句柄限制。可以设置到10k. 你还需要修改 hbase 用户的
        <code class="varname">nproc</code>，如果过低会造成 <code class="classname">OutOfMemoryError</code>异常。
        <sup>[<a name="d613e389" href="book.htm#ftn.d613e389" class="footnote">2</a>]</sup>
        <sup>[<a name="d613e396" href="book.htm#ftn.d613e396" class="footnote">3</a>]</sup>.
    </p><p>需要澄清的，这两个设置是针对操作系统的，不是Hbase本身的。有一个常见的错误是Hbase运行的用户，和设置最大值的用户不是一个用户。在Hbase启动的时候，第一行日志会现在ulimit信息，所以你最好检查一下。
        <sup>[<a name="d613e408" href="book.htm#ftn.d613e408" class="footnote">4</a>]</sup></p><div class="section" title="2.1.6.1. 在Ubuntu上设置ulimit"><div class="titlepage"><div><div>
          <h5 class="title"><a name="ulimit_ubuntu"></a>2.2.5.1.&nbsp;在Ubuntu上设置<code class="varname">ulimit</code></h5></div></div></div><p>如果你使用的是Ubuntu,你可以这样设置:</p><p>在文件 <code class="filename">/etc/security/limits.conf</code> 添加一行，如: </p><pre class="programlisting">hadoop  -       nofile  32768</pre><p>
          可以把 <code class="varname">hadoop</code> 替换成你运行Hbase和Hadoop的用户。如果你用两个用户，你就需要配两个。还有配nproc hard 和 soft
          limits.  如: </p><pre class="programlisting">hadoop soft/hard nproc 32000</pre><p>.</p><p>在 <code class="filename">/etc/pam.d/common-session</code> 加上这一行: </p><pre class="programlisting">session required  pam_limits.so</pre><p>
          否则在 <code class="filename">/etc/security/limits.conf</code>上的配置不会生效.</p><p>还有注销再登录，这些配置才能生效!</p></div></div><div class="section" title="2.1.7. dfs.datanode.max.xcievers"><div class="titlepage"><div><div></div></div></div></div><div class="section" title="2.1.8. Windows"><div class="titlepage"><div><div>
            <h4 class="title"><a name="windows"></a>2.2.6.&nbsp;Windows</h4></div></div></div><p>HBase没有怎么在Windows下测试过。所以不推荐在Windows下运行.</p><p>如果你实在是想运行，需要安装<a class="link" href="http://cygwin.com/" target="_top">Cygwin</a> 还虚拟一个unix环境.详情请看 <a class="link" href="http://hbase.apache.org/cygwin.html" target="_top">Windows
        安装指导</a> . 或者 
        <a class="link" href="http://search-hadoop.com/?q=hbase+windows&fc_project=HBase&fc_type=mail+_hash_+dev" target="_top">搜索邮件列表</a>找找最近的关于windows的注意点</p>
            <div class="section" title="2.1.2. hadoop">
              <div class="titlepage">
                <div>
                  <div>
                    <h4 class="title"><a name="hadoop"></a>2.3.&nbsp;<a class="link" href="http://hadoop.apache.org/" target="_top">hadoop</a><a class="indexterm" name="d613e269"></a></h4>
                  </div>
                </div>
              </div>
              <h3>Please read all of this section</h3>
              <p>Please read this section to the end. Up front we wade through the weeds of Hadoop versions. Later we talk of what you must do in HBase to make it work w/ a particular Hadoop version.</p>
<p>&nbsp;</p>
              <p>除非运行在实现了持久化同步(sync)的HDFS上，HBase 将丢失所有数据。   Hadoop 0.20.2, Hadoop 0.20.203.0,及 Hadoop 0.20.204.0  <strong>不具有</strong>上述特性。当前Hadoop仅在<a class="link" href="http://hadoop.apache.org/common/releases.html" target="_top">Hadoop
                0.20.205.x</a> 或更高版本--包含hadoop 1.0.0 --具有持久化sync. Sync 必须显式开启。即  dfs.support.append 同时在客户端和服务器端设为真，客户端：   hbase-site.xml ，服务器端：  hdfs-site.xml   </p>
              <p>(The sync facility HBase needs is a subset of the append code path).</p>
              <pre>  &lt;property&gt;      &lt;name&gt;dfs.support.append&lt;/name&gt;      &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;          </pre>
              <p>修改后必须重启集群。  Ignore the chicken-little comment you'll find in the hdfs-default.xml in the description for thedfs.support.append configuration; it says it is not enabled because there are “... bugs in the 'append code' and is not supported in any production cluster.”. This comment is stale, from another era, and while I'm sure there are bugs, the sync/append code has been running in production at large scale deploys and is on by default in the offerings of hadoop by commercial vendors  </p>
<p>你还可以用 Cloudera's <a class="link" href="http://archive.cloudera.com/docs/" target="_top">CDH3</a> 或   <a href="http://www.mapr.com/" target="_top">MapR</a> 。  Cloudera  的CDH3 是Apache hadoop 0.20.x的补丁增强，包含所有   <a href="http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-append/" target="_top">branch-0.20-append</a>  附加的持久化Sync.  Use the released, most recent version of CDH3.  </p>
<p> <a href="http://www.mapr.com/" target="_top">MapR</a> includes a commercial, reimplementation of HDFS. It has a durable sync as well as some other interesting features that are not yet in Apache Hadoop. Their <a href="http://www.mapr.com/products/mapr-editions/m3-edition" target="_top">M3</a> product is free to use and unlimited. </p>
              <p>因为Hbase建立在Hadoop之上，所以他用到了hadoop.jar,这个Jar在 <code class="filename">lib</code> 里面。这个jar是hbase自己打了branch-0.20-append 补丁的hadoop.jar.  Hadoop使用的hadoop.jar和Hbase使用的 <span class="emphasis"><em>必须</em></span> 一致。所以你需要将 Hbase <code class="filename">lib</code> 目录下的hadoop.jar替换成Hadoop里面的那个，防止版本冲突。比方说CDH的版本没有HDFS-724而branch-0.20-append里面有，这个HDFS-724补丁修改了RPC协议。如果不替换，就会有版本冲突，继而造成严重的出错，Hadoop会看起来挂了。</p>
              <div class="note" title="我可以用Hbase里面的支持sync的hadoop.jar替代Hadoop里面的那个吗?" style="margin-left: 0.5in; margin-right: 0.5in;">
                <h3 class="title">我可以用Hbase里面的支持<span class="emphasis"><em>sync</em></span>的hadoop.jar替代Hadoop里面的那个吗?</h3>
                <p>你可以这么干。详细可以参见这个<a class="link" href="http://www.apacheserver.net/Using-Hadoop-bundled-in-lib-directory-HBase-at1136240.htm" target="_top">邮件列表</a>.</p>
              </div>
              <div class="section" title="2.3.1. Hadoop 安全性" >
                <h4 class="title">2.3.1. Hadoop 安全性</h4>
                <p>HBase运行在Hadoop 0.20.x上，就可以使用其中的安全特性 -- 只要你用这两个版本0.20S 和CDH3B3，然后把hadoop.jar替换掉就可以了.</p>
              </div>
            </div>
          </div></div>
          <div class="section" title="2.3.2.&nbsp;dfs.datanode.max.xcievers"><div class="titlepage"><div><div>
            <h4 class="title"><a name="dfs.datanode.max.xcievers"></a>2.3.2.&nbsp;<code class="varname">dfs.datanode.max.xcievers</code><a class="indexterm" name="d613e451"></a></h4></div></div></div><p>一个 Hadoop HDFS Datanode 有一个同时处理文件的上限. 这个参数叫 <code class="varname">xcievers</code> (Hadoop的作者把这个单词拼错了). 在你加载之前，先确认下你有没有配置这个文件<code class="filename">conf/hdfs-site.xml</code>里面的<code class="varname">xceivers</code>参数，至少要有4096:
        </p><pre class="programlisting">      &lt;property&gt;
        &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
        &lt;value&gt;4096&lt;/value&gt;
      &lt;/property&gt;
      </pre><p>对于HDFS修改配置要记得重启.</p><p>如果没有这一项配置，你可能会遇到奇怪的失败。你会在Datanode的日志中看到xcievers exceeded，但是运行起来会报 missing blocks错误。例如:
        <code class="code">10/12/08 20:10:31 INFO hdfs.DFSClient: Could not obtain block
        blk_XXXXXXXXXXXXXXXXXXXXXX_YYYYYYYY from any node:
        java.io.IOException: No live nodes contain current block. Will get new
        block locations from namenode and retry...</code>
        <sup>[<a name="d613e474" href="book.htm#ftn.d613e474" class="footnote">5</a>]</sup></p></div>
          <div class="section" title="2.2. HBase运行模式:单机和分布式"><div class="titlepage"><div><div>
            <h3 class="title"><a name="standalone_dist"></a>2.4.&nbsp;HBase运行模式:单机和分布式</h3></div></div></div>
          <p>HBase有两个运行模式: <a class="xref" href="book.htm#standalone" title="2.2.1. 单机模式">Section&nbsp;2.4.1, “单机模式”</a> 和 <a class="xref" href="book.htm#distributed" title="2.2.2. 分布式模式">Section&nbsp;2.4.2, “分布式模式”</a>. 默认是单机模式，如果要分布式模式你需要编辑 <code class="filename">conf</code>
      文件夹中的配置文件.</p><p>不管是什么模式，你都需要编辑
      <code class="code">conf/hbase-env.sh</code>来告知Hbase
      <span class="command"><strong>java</strong></span>的安装路径.在这个文件里你还可以设置Hbase的运行环境，诸如 heapsize和其他
      <span class="application">JVM</span>有关的选项, 还有Log文件地址，等等. 设置 <code class="varname">JAVA_HOME</code>指向
      <span class="command"><strong>java</strong></span>安装的路径.</p><div class="section" title="2.2.1. 单机模式"><div class="titlepage"><div><div>
        <h4 class="title"><a name="standalone"></a>2.4.1.&nbsp;单机模式</h4></div></div></div><p>这是默认的模式，在 <a class="xref" href="book.htm#quickstart" title="1.2. 快速开始">Section&nbsp;1.2, “快速开始”</a> 一章中介绍的就是这个模式. 在单机模式中，Hbase使用本地文件系统，而不是HDFS ，所以的服务和zooKeeper都运作在一个JVM中。zookeep监听一个端口，这样客户端就可以连接Hbase了。</p></div><div class="section" title="2.2.2. 分布式模式"><div class="titlepage"><div><div>
          <h4 class="title"><a name="distributed"></a>2.4.2.&nbsp;分布式模式</h4></div></div></div><p>分布式模式分两种。<span class="emphasis"><em>伪分布式模式</em></span>是把进程运行在一台机器上，但不是一个JVM.而<span class="emphasis"><em>完全分布式模式</em></span>就是把整个服务被分布在各个节点上了 <sup>[<a name="d613e543" href="book.htm#ftn.d613e543" class="footnote">6</a>]</sup>.</p><p>分布式模式需要使用 <span class="emphasis"><em>Hadoop
        Distributed File System</em></span> (HDFS).可以参见 <a class="link" href="http://hadoop.apache.org/common/docs/current/api/overview-summary.html#overview_description" target="_top">
        HDFS需求和指导</a>来获得关于安装HDFS的指导。在操作Hbase之前，你要确认HDFS可以正常运作。</p><p>在我们安装之后，你需要确认你的<span class="emphasis"><em>伪分布式模式</em></span>或者
        <span class="emphasis"><em>完全分布式模式</em></span>的配置是否正确。这两个模式可以使用同一个验证脚本<a class="xref" href="book.htm#confirm" title="2.2.3. 运行和确认你的安装">Section&nbsp;2.2.3, “运行和确认你的安装”</a>。</p><div class="section" title="2.2.2.1. 伪分布式模式"><div class="titlepage"><div><div>
          <h5 class="title"><a name="pseudo"></a>2.4.2.1.&nbsp;伪分布式模式</h5></div></div></div><p>伪分布式模式是一个相对简单的分布式模式。这个模式是用来测试的。不能把这个模式用于生产环节，也不能用于测试性能。</p>
          <p>你确认HDFS安装成功之后，就可以先编辑
          <code class="filename">conf/hbase-site.xml</code>。在这个文件你可以加入自己的配置，这个配置会覆盖
          <a class="xref" href="book.htm#hbase_default_configurations" title="3.1.1. HBase 默认配置">Section&nbsp;2.6.1.1, “HBase 默认配置”</a> and <a class="xref" href="book.htm#hdfs_client_conf" title="2.2.2.2.3. HDFS客户端配置">Section&nbsp;2.4.2.2.3, “HDFS客户端配置”</a>. 运行Hbase需要设置<code class="varname">hbase.rootdir</code> 属性.该属性是指Hbase在HDFS中使用的目录的位置。例如，要想
          <code class="filename">/hbase</code> 目录，让namenode 监听locahost的9000端口，只有一份数据拷贝(HDFS默认是3份拷贝)。可以在
          <code class="filename">hbase-site.xml</code> 写上如下内容</p><pre class="programlisting">&lt;configuration&gt;
  ...
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;
    &lt;description&gt;The directory shared by RegionServers.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
    &lt;description&gt;The replication count for HLog &amp; HFile storage. Should not be greater than HDFS datanode count.
    &lt;/description&gt;
  &lt;/property&gt;
  ...
&lt;/configuration&gt;
</pre><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>让Hbase自己创建 <code class="varname">hbase.rootdir</code>
            目录，如果你自己建这个目录，会有一个warning，Hbase会试图在里面进行migration操作，但是缺少必须的文件。</p></div><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>上面我们绑定到 <code class="varname">localhost</code>. 也就是说除了本机，其他机器连不上Hbase。所以你需要设置成别的，才能使用它。</p></div>
            <p>现在可以跳到 <a class="xref" href="book.htm#confirm" title="2.2.3. 运行和确认你的安装">Section&nbsp;2.4.3, “运行和确认你的安装”</a> 来运行和确认你的伪分布式模式安装了。 <sup>[<a name="d613e606" href="book.htm#ftn.d613e606" class="footnote">7</a>]</sup></p></div><div class="section" title="2.2.2.2. 完全分布式模式"><div class="titlepage"><div><div>
              <div title="2.4.2.1. Pseudo-distributed">
                <div title="2.4.2.1.1. Pseudo-distributed Configuration Files">
                  <div>
                    <div>
                      <div>
                        <h5>2.4.2.1.1. Pseudo-distributed Configuration Files</h5>
                      </div>
                    </div>
                  </div>
                  <p>The following are exmaple configuration files from a pseudo-distributed setup.</p>
                  hdfs-site.xml
                  <pre>&lt;configuration&gt;    ...    &lt;property&gt;      &lt;name&gt;dfs.name.dir&lt;/name&gt;      &lt;value&gt;/Users/local/user.name/hdfs-data-name&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;      &lt;name&gt;dfs.data.dir&lt;/name&gt;      &lt;value&gt;/Users/local/user.name/hdfs-data&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;      &lt;name&gt;dfs.replication&lt;/name&gt;      &lt;value&gt;1&lt;/value&gt;    &lt;/property&gt;    ...  &lt;/configuration&gt;  </pre>
                  hbase-site.xml
                  <pre>&lt;configuration&gt;    ...    &lt;property&gt;      &lt;name&gt;hbase.rootdir&lt;/name&gt;      &lt;value&gt;hdfs://localhost:8020/hbase&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;      &lt;value&gt;localhost&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;      &lt;value&gt;true&lt;/value&gt;    &lt;/property&gt;    ...  &lt;/configuration&gt;  </pre>
                </div>
                <div title="2.4.2.1.2. Pseudo-distributed Extras">
                  <div>
                    <div>
                      <div>
                        <h5><a name="pseudo.extras"></a>2.4.2.1.2. Pseudo-distributed Extras</h5>
                      </div>
                    </div>
                  </div>
                  <div title="2.4.2.1.2.1. Startup">
                    <div>
                      <div>
                        <div>
                          <h6><a name="pseudo.extras.start"></a>2.4.2.1.2.1. Startup</h6>
                        </div>
                      </div>
                    </div>
                    <p>To start up the initial HBase cluster...</p>
                    <pre>% bin/start-hbase.sh</pre>
                    <p>To start up an extra backup master(s) on the same server run...</p>
                    <pre>% bin/local-master-backup.sh start 1</pre>
                    <p>... the '1' means use ports 60001 &amp; 60011, and this backup master's logfile will be at logs/hbase-${USER}-1-master-${HOSTNAME}.log.</p>
                    <p>To startup multiple backup masters run...</p>
                    <pre>% bin/local-master-backup.sh start 2 3</pre>
                    <p>You can start up to 9 backup masters (10 total).</p>
                    <p>To start up more regionservers...</p>
                    <pre>% bin/local-regionservers.sh start 1</pre>
                    <p>where '1' means use ports 60201 &amp; 60301 and its logfile will be at logs/hbase-${USER}-1-regionserver-${HOSTNAME}.log.</p>
                    <p>To add 4 more regionservers in addition to the one you just started by running...</p>
                    <pre>% bin/local-regionservers.sh start 2 3 4 5</pre>
                    <p>This supports up to 99 extra regionservers (100 total).</p>
                  </div>
                  <div title="2.4.2.1.2.2. Stop">
                    <div>
                      <div>
                        <div>
                          <h6><a name="pseudo.options.stop"></a>2.4.2.1.2.2. Stop</h6>
                        </div>
                      </div>
                    </div>
                    <p>Assuming you want to stop master backup # 1, run...</p>
                    <pre>% cat /tmp/hbase-${USER}-1-master.pid |xargs kill -9</pre>
                    <p>Note that bin/local-master-backup.sh stop 1 will try to stop the cluster along with the master.</p>
                    <p>To stop an individual regionserver, run...</p>
                    <pre>% bin/local-regionservers.sh stop 1  	                </pre>
                  </div>
                </div>
              </div>
              <div title="2.4.2.2. Fully-distributed">
                <div>
                  <div>
                    <div>
                      <h4><a name="fully_dist"></a>2.4.2.2. <span class="title">完全分布式模式</span></h4>
                    </div>
                  </div>
                </div>
              </div>
            </div></div></div><p>要想运行完全分布式模式，你要进行如下配置，先在
          <code class="filename">hbase-site.xml</code>, 加一个属性
          <code class="varname">hbase.cluster.distributed</code> 设置为
          <code class="varname">true</code> 然后把
          <code class="varname">hbase.rootdir</code> 设置为HDFS的NameNode的位置。
          例如，你的namenode运行在namenode.example.org，端口是9000 你期望的目录是
          <code class="filename">/hbase</code>,使用如下的配置</p><pre class="programlisting">&lt;configuration&gt;
  ...
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://namenode.example.org:9000/hbase&lt;/value&gt;
    &lt;description&gt;The directory shared by RegionServers.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    &lt;/description&gt;
  &lt;/property&gt;
  ...
&lt;/configuration&gt;
</pre><div class="section" title="2.2.2.2.1. regionservers"><div class="titlepage"><div><div>
  <h6 class="title"><a name="regionserver"></a>2.4.2.2.1.&nbsp;<code class="filename">regionservers</code></h6></div></div></div>
<p>完全分布式模式的还需要修改<code class="filename">conf/regionservers</code>. 在
            <a class="xref" href="book.htm#regionservers" title="2.3.1.2. regionservers">Section&nbsp;2.7.1.2, “<code class="filename">regionservers</code>”</a> 列出了你希望运行的全部
            <span class="application">HRegionServer</span>，一行写一个host (就像Hadoop里面的 <code class="filename">slaves</code>
            一样). 列在这里的server会随着集群的启动而启动，集群的停止而停止.</p></div><div class="section" title="2.2.2.2.2. ZooKeeper"><div class="titlepage"><div><div>
              <h6 class="title"><a></a>2.4.2.2.2.&nbsp;ZooKeeper 和 HBase<a class="indexterm" name="d613e654"></a></h6></div></div></div><p>&nbsp;</p><div class="section" title="2.2.2.2.2.1. 使用现有的ZooKeeper例子"></div></div><div class="section" title="2.2.2.2.3. HDFS客户端配置"><div class="titlepage"><div><div>
                <h6 class="title"><a name="hdfs_client_conf"></a>2.4.2.2.3.&nbsp;HDFS客户端配置</h6></div></div></div><p>如果你希望Hadoop集群上做<span class="emphasis"><em>HDFS 客户端配置
            </em></span>，例如你的HDFS客户端的配置和服务端的不一样。按照如下的方法配置，HBase就能看到你的配置信息:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>在<code class="filename">hbase-env.sh</code>里将<code class="varname">HBASE_CLASSPATH</code>环境变量加上<code class="varname">HADOOP_CONF_DIR</code>
                。</p></li><li class="listitem"><p>在<code class="filename">${HBASE_HOME}/conf</code>下面加一个 <code class="filename">hdfs-site.xml</code> (或者
                <code class="filename">hadoop-site.xml</code>) ，最好是软连接</p></li><li class="listitem"><p>如果你的HDFS客户端的配置不多的话，你可以把这些加到 <code class="filename">hbase-site.xml</code>上面.</p></li></ul></div><p>例如HDFS的配置
            <code class="varname">dfs.replication</code>.你希望复制5份，而不是默认的3份。如果你不照上面的做的话，Hbase只会复制3份。</p></div></div></div><div class="section" title="2.2.3. 运行和确认你的安装"><div class="titlepage"><div><div><h4 class="title"><a name="confirm"></a>2.2.3.&nbsp;运行和确认你的安装</h4></div></div></div><p>首先确认你的HDFS是运行着的。你可以运行<code class="varname">HADOOP_HOME</code>中的 <code class="filename">bin/start-hdfs.sh</code> 来启动HDFS.你可以通过<span class="command"><strong>put</strong></span>命令来测试放一个文件，然后有<span class="command"><strong>get</strong></span>命令来读这个文件。通常情况下Hbase是不会运行mapreduce的。所以比不需要检查这些。</p><p><span class="emphasis"><em>如果</em></span>你自己管理ZooKeeper集群，你需要确认它是运行着的。如果是Hbase托管，ZoopKeeper会随Hbase启动。</p><p>用如下命令启动Hbase:</p><pre class="programlisting">bin/start-hbase.sh</pre>

         这个脚本在<code class="varname">HBASE_HOME</code>目录里面。

        <p>你现在已经启动Hbase了。Hbase把log记在 <code class="filename">logs</code> 子目录里面. 当Hbase启动出问题的时候，可以看看Log. </p><p>Hbase也有一个界面，上面会列出重要的属性。默认是在Master的60010端口上H (HBase RegionServers 会默认绑定
        60020端口，在端口60030上有一个展示信息的界面 ).如果Master运行在
        <code class="varname">master.example.org</code>，端口是默认的话，你可以用浏览器在 
        <code class="filename">http://master.example.org:60010</code>看到主界面.
        .</p><p>一旦Hbase启动，参见<a class="xref" href="book.htm#shell_exercises" title="1.2.3. Shell 练习">Section&nbsp;1.2.3, “Shell 练习”</a>可以看到如何建表，插入数据，scan你的表，还有disable这个表，最后把它删掉。</p><p>可以在Hbase Shell停止Hbase
        </p><pre class="programlisting">$ ./bin/stop-hbase.sh
stopping hbase...............</pre><p> 停止操作需要一些时间，你的集群越大，停的时间可能会越长。如果你正在运行一个分布式的操作，要确认在Hbase彻底停止之前，Hadoop不能停.</p></div></div><div class="section" title="2.3. 配置例子"><div class="titlepage"><div><div>
  <div>
    <div>
      <div>
        <h2><a name="zookeeper"></a>2.5. ZooKeeper<a name="d1934e929"></a></h2>
      </div>
    </div>
  </div>
  <p>一个分布式运行的Hbase依赖一个zookeeper集群。所有的节点和客户端都必须能够访问zookeeper。默认的情况下Hbase会管理一个zookeep集群。这个集群会随着Hbase的启动而启动。当然，你也可以自己管理一个zookeeper集群，但需要配置Hbase。你需要修改<code class="filename">conf/hbase-env.sh</code>里面的<code class="varname">HBASE_MANAGES_ZK</code> 来切换。这个值默认是true的，作用是让Hbase启动的时候同时也启动zookeeper.</p>
  <p>当Hbase管理zookeeper的时候，你可以通过修改<code class="filename">zoo.cfg</code>来配置zookeeper，一个更加简单的方法是在 <code class="filename">conf/hbase-site.xml</code>里面修改zookeeper的配置。Zookeep的配置是作为property写在 <code class="filename">hbase-site.xml</code>里面的。option的名字是 <code class="varname">hbase.zookeeper.property</code>. 打个比方， <code class="varname">clientPort</code> 配置在xml里面的名字是 <code class="varname">hbase.zookeeper.property.clientPort</code>.
    所有的默认值都是Hbase决定的，包括zookeeper, 参见 <a class="xref" href="book.htm#hbase_default_configurations" title="3.1.1. HBase 默认配置">Section&nbsp;2.6.1.1, “HBase 默认配置”</a>. 可以查找 <code class="varname">hbase.zookeeper.property</code> 前缀，找到关于zookeeper的配置。 <sup>[<a name="d613e690" href="#ftn.d1934e968" class="footnote">13</a>]</sup></p>
  <p>对于zookeepr的配置，你至少要在 <code class="filename">hbase-site.xml</code>中列出zookeepr的ensemble servers，具体的字段是 <code class="varname">hbase.zookeeper.quorum</code>. 该这个字段的默认值是 <code class="varname">localhost</code>，这个值对于分布式应用显然是不可以的. (远程连接无法使用). </p>
  <div class="note" title="我需要运行几个zookeeper?" style="margin-left: 0.5in; margin-right: 0.5in;">
    <h3 class="title"><a name="how_many_zks"></a>我需要运行几个zookeeper?</h3>
    <p>你运行一个zookeeper也是可以的，但是在生产环境中，你最好部署3，5，7个节点。部署的越多，可靠性就越高，当然只能部署奇数个，偶数个是不可以的。你需要给每个zookeeper 1G左右的内存，如果可能的话，最好有独立的磁盘。 (独立磁盘可以确保zookeeper是高性能的。).如果你的集群负载很重，不要把Zookeeper和RegionServer运行在同一台机器上面。就像DataNodes 和 TaskTrackers一样</p>
  </div>
  <p>举个例子，Hbase管理着的ZooKeeper集群在节点 <span class="emphasis"><em>rs{1,2,3,4,5}.example.com</em></span>, 监听2222 端口(默认是2181)，并确保<code class="filename">conf/hbase-env.sh</code>文件中 <code class="varname">HBASE_MANAGE_ZK</code>的值是 <code class="varname">true</code> ，再编辑 <code class="filename">conf/hbase-site.xml</code> 设置 <code class="varname">hbase.zookeeper.property.clientPort</code> 和 <code class="varname">hbase.zookeeper.quorum</code>。你还可以设置 <code class="varname">hbase.zookeeper.property.dataDir</code>属性来把ZooKeeper保存数据的目录地址改掉。默认值是 <code class="filename">/tmp</code> ，这里在重启的时候会被操作系统删掉，可以把它修改到 <code class="filename">/user/local/zookeeper</code>. </p>
  <pre class="programlisting">  &lt;configuration&gt;
    ...
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;
      &lt;value&gt;2222&lt;/value&gt;
      &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
      The port at which the clients will connect.
      &lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
      &lt;value&gt;rs1.example.com,rs2.example.com,rs3.example.com,rs4.example.com,rs5.example.com&lt;/value&gt;
      &lt;description&gt;Comma separated list of servers in the ZooKeeper Quorum.
      For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".
      By default this is set to localhost for local and pseudo-distributed modes
      of operation. For a fully-distributed setup, this should be set to a full
      list of ZooKeeper quorum servers. If HBASE_MANAGES_ZK is set in hbase-env.sh
      this is the list of servers which we will start/stop ZooKeeper on.
      &lt;/description&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
      &lt;value&gt;/usr/local/zookeeper&lt;/value&gt;
      &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
      The directory where the snapshot is stored.
      &lt;/description&gt;
    &lt;/property&gt;
    ...
  &lt;/configuration&gt;</pre>
  <div class="section" title="2.2.2.2.2.1. 使用现有的ZooKeeper例子">
    <div class="titlepage">
      <div>
        <div>
          <h3 class="title"><a name="d613e752"></a>2.5.1.&nbsp;使用现有的ZooKeeper例子</h3>
        </div>
      </div>
    </div>
    <p>让Hbase使用一个现有的不被Hbase托管的Zookeep集群，需要设置 <code class="filename">conf/hbase-env.sh</code>文件中的<code class="varname">HBASE_MANAGES_ZK</code> 属性为 false </p>
    <pre class="programlisting">  ...
  # Tell HBase whether it should manage it's own instance of Zookeeper or not.
  export HBASE_MANAGES_ZK=false</pre>
    <p> 接下来，指明Zookeeper的host和端口。可以在 <code class="filename">hbase-site.xml</code>中设置, 也可以在Hbase的<code class="filename">CLASSPATH</code>下面加一个<code class="filename">zoo.cfg</code>配置文件。
      HBase 会优先加载 <code class="filename">zoo.cfg</code> 里面的配置，把<code class="filename">hbase-site.xml</code>里面的覆盖掉.</p>
    <p>当Hbase托管ZooKeeper的时候，Zookeeper集群的启动是Hbase启动脚本的一部分。但现在，你需要自己去运行。你可以这样做</p>
    <pre class="programlisting">${HBASE_HOME}/bin/hbase-daemons.sh {start,stop} zookeeper
  </pre>
    <p>你可以用这条命令启动ZooKeeper而不启动Hbase. <code class="varname">HBASE_MANAGES_ZK</code> 的值是 <code class="varname">false</code>，
      如果你想在Hbase重启的时候不重启ZooKeeper,你可以这样做</p>
    <p>对于独立Zoopkeeper的问题，你可以在 <a class="link" href="http://hadoop.apache.org/zookeeper/docs/current/zookeeperStarted.html" target="_top">Zookeeper启动</a>得到帮助.</p>
  </div>
  <div title="2.5.1. Using existing ZooKeeper ensemble">
    <div>
      <div>
        <div></div>
      </div>
    </div>
  </div>
  <div class="section" title="2.5.&nbsp;ZooKeeper"><div class="section" title="2.5.2.&nbsp;SASL Authentication with ZooKeeper"><div class="titlepage"><div><div><h3 class="title"><a name="zk.sasl.auth"></a>2.5.2.&nbsp;SASL Authentication with ZooKeeper</h3></div></div></div><p>Newer releases of HBase (&gt;= 0.92) will
              support connecting to a ZooKeeper Quorum that supports
              SASL authentication (which is available in Zookeeper
              versions 3.4.0 or later).</p><p>This describes how to set up HBase to mutually
              authenticate with a ZooKeeper Quorum. ZooKeeper/HBase
              mutual authentication (<a class="link" href="https://issues.apache.org/jira/browse/HBASE-2418" target="_top">HBASE-2418</a>)
              is required as part of a complete secure HBase configuration
              (<a class="link" href="https://issues.apache.org/jira/browse/HBASE-3025" target="_top">HBASE-3025</a>).

              For simplicity of explication, this section ignores
              additional configuration required (Secure HDFS and Coprocessor
              configuration).  It's recommended to begin with an
              HBase-managed Zookeeper configuration (as opposed to a
              standalone Zookeeper quorum) for ease of learning.
              </p><div class="section" title="2.5.2.1.&nbsp;Operating System Prerequisites"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e1097"></a>2.5.2.1.&nbsp;Operating System Prerequisites</h4></div></div></div></div><p>
                  You need to have a working Kerberos KDC setup. For
                  each <code class="code">$HOST</code> that will run a ZooKeeper
                  server, you should have a principle
                  <code class="code">zookeeper/$HOST</code>.  For each such host,
                  add a service key (using the <code class="code">kadmin</code> or
                  <code class="code">kadmin.local</code> tool's <code class="code">ktadd</code>
                  command) for <code class="code">zookeeper/$HOST</code> and copy
                  this file to <code class="code">$HOST</code>, and make it
                  readable only to the user that will run zookeeper on
                  <code class="code">$HOST</code>. Note the location of this file,
                  which we will use below as
                  <code class="filename">$PATH_TO_ZOOKEEPER_KEYTAB</code>.
              </p><p>
                Similarly, for each <code class="code">$HOST</code> that will run
                an HBase server (master or regionserver), you should
                have a principle: <code class="code">hbase/$HOST</code>. For each
                host, add a keytab file called
                <code class="filename">hbase.keytab</code> containing a service
                key for <code class="code">hbase/$HOST</code>, copy this file to
                <code class="code">$HOST</code>, and make it readable only to the
                user that will run an HBase service on
                <code class="code">$HOST</code>. Note the location of this file,
                which we will use below as
                <code class="filename">$PATH_TO_HBASE_KEYTAB</code>.
              </p><p>
                Each user who will be an HBase client should also be
                given a Kerberos principal. This principal should
                usually have a password assigned to it (as opposed to,
                as with the HBase servers, a keytab file) which only
                this user knows. The client's principal's
                <code class="code">maxrenewlife</code> should be set so that it can
                be renewed enough so that the user can complete their
                HBase client processes. For example, if a user runs a
                long-running HBase client process that takes at most 3
                days, we might create this user's principal within
                <code class="code">kadmin</code> with: <code class="code">addprinc -maxrenewlife
                3days</code>. The Zookeeper client and server
                libraries manage their own ticket refreshment by
                running threads that wake up periodically to do the
                refreshment.
              </p><p>On each host that will run an HBase client
                (e.g. <code class="code">hbase shell</code>), add the following
                file to the HBase home directory's <code class="filename">conf</code>
                directory:</p><pre class="programlisting">                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=false
                    useTicketCache=true;
                  };
                </pre><p>We'll refer to this JAAS configuration file as
                <code class="filename">$CLIENT_CONF</code> below.</p><div class="section" title="2.5.2.2.&nbsp;HBase-managed Zookeeper Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e1178"></a>2.5.2.2.&nbsp;HBase-managed Zookeeper Configuration</h4></div></div></div><p>On each node that will run a zookeeper, a
                master, or a regionserver, create a <a class="link" href="http://docs.oracle.com/javase/1.4.2/docs/guide/security/jgss/tutorials/LoginConfigFile.html" target="_top">JAAS</a>
                configuration file in the conf directory of the node's
                <code class="filename">HBASE_HOME</code> directory that looks like the
                following:</p><pre class="programlisting">                  Server {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    keyTab="$PATH_TO_ZOOKEEPER_KEYTAB"
                    storeKey=true
                    useTicketCache=false
                    principal="zookeeper/$HOST";
                  };
                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    useTicketCache=false
                    keyTab="$PATH_TO_HBASE_KEYTAB"
                    principal="hbase/$HOST";
                  };
                </pre>
                
                where the <code class="filename">$PATH_TO_HBASE_KEYTAB</code> and
                <code class="filename">$PATH_TO_ZOOKEEPER_KEYTAB</code> files are what
                you created above, and <code class="code">$HOST</code> is the hostname for that
                node.

                <p>The <code class="code">Server</code> section will be used by
                the Zookeeper quorum server, while the
                <code class="code">Client</code> section will be used by the HBase
                master and regionservers. The path to this file should
                be substituted for the text <code class="filename">$HBASE_SERVER_CONF</code>
                in the <code class="filename">hbase-env.sh</code>
                listing below.</p><p>
                  The path to this file should be substituted for the
                  text <code class="filename">$CLIENT_CONF</code> in the
                  <code class="filename">hbase-env.sh</code> listing below.
                </p><p>Modify your <code class="filename">hbase-env.sh</code> to include the
                following:</p><pre class="programlisting">                  export HBASE_OPTS="-Djava.security.auth.login.config=$CLIENT_CONF"
                  export HBASE_MANAGES_ZK=true
                  export HBASE_ZOOKEEPER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_MASTER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_REGIONSERVER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                </pre>

                where <code class="filename">$HBASE_SERVER_CONF</code> and
                <code class="filename">$CLIENT_CONF</code> are the full paths to the
                JAAS configuration files created above.

                <p>Modify your <code class="filename">hbase-site.xml</code> on each node
                that will run zookeeper, master or regionserver to contain:</p><pre class="programlisting">                  &lt;configuration&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
                      &lt;value&gt;$ZK_NODES&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.authProvider.1&lt;/name&gt;
                      &lt;value&gt;org.apache.zookeeper.server.auth.SASLAuthenticationProvider&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.kerberos.removeHostFromPrincipal&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.property.kerberos.removeRealmFromPrincipal&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                  &lt;/configuration&gt;
                  </pre><p>where <code class="code">$ZK_NODES</code> is the
                comma-separated list of hostnames of the Zookeeper
                Quorum hosts.</p><p>Start your hbase cluster by running one or more
                of the following set of commands on the appropriate
                hosts:
                </p><pre class="programlisting">                  bin/hbase zookeeper start
                  bin/hbase master start
                  bin/hbase regionserver start
                </pre></div><div class="section" title="2.5.2.3.&nbsp;External Zookeeper Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e1253"></a>2.5.2.3.&nbsp;External Zookeeper Configuration</h4></div></div></div><p>Add a JAAS configuration file that looks like:

                </p><pre class="programlisting">                  Client {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    useTicketCache=false
                    keyTab="$PATH_TO_HBASE_KEYTAB"
                    principal="hbase/$HOST";
                  };
                </pre><p>

                where the <code class="filename">$PATH_TO_HBASE_KEYTAB</code> is the keytab 
                created above for HBase services to run on this host, and <code class="code">$HOST</code> is the
                hostname for that node. Put this in the HBase home's
                configuration directory. We'll refer to this file's
                full pathname as <code class="filename">$HBASE_SERVER_CONF</code> below.</p><p>Modify your hbase-env.sh to include the following:</p><pre class="programlisting">                  export HBASE_OPTS="-Djava.security.auth.login.config=$CLIENT_CONF"
                  export HBASE_MANAGES_ZK=false
                  export HBASE_MASTER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                  export HBASE_REGIONSERVER_OPTS="-Djava.security.auth.login.config=$HBASE_SERVER_CONF"
                </pre><p>Modify your <code class="filename">hbase-site.xml</code> on each node
                that will run a master or regionserver to contain:</p><pre class="programlisting">                  &lt;configuration&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
                      &lt;value&gt;$ZK_NODES&lt;/value&gt;
                    &lt;/property&gt;
                    &lt;property&gt;
                      &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
                      &lt;value&gt;true&lt;/value&gt;
                    &lt;/property&gt;
                  &lt;/configuration&gt;
                  
                </pre><p>where <code class="code">$ZK_NODES</code> is the
                comma-separated list of hostnames of the Zookeeper
                Quorum hosts.</p><p>
                  Add a <code class="filename">zoo.cfg</code> for each Zookeeper Quorum host containing:
                  </p><pre class="programlisting">                      authProvider.1=org.apache.zookeeper.server.auth.SASLAuthenticationProvider
                      kerberos.removeHostFromPrincipal=true
                      kerberos.removeRealmFromPrincipal=true
                  </pre><p>

                  Also on each of these hosts, create a JAAS configuration file containing:

                  </p><pre class="programlisting">                  Server {
                    com.sun.security.auth.module.Krb5LoginModule required
                    useKeyTab=true
                    keyTab="$PATH_TO_ZOOKEEPER_KEYTAB"
                    storeKey=true
                    useTicketCache=false
                    principal="zookeeper/$HOST";
                  };
                  </pre><p>

                  where <code class="code">$HOST</code> is the hostname of each
                  Quorum host. We will refer to the full pathname of
                  this file as <code class="filename">$ZK_SERVER_CONF</code> below.

                </p><p>
                  Start your Zookeepers on each Zookeeper Quorum host with:
                  
                  </p><pre class="programlisting">                    SERVER_JVMFLAGS="-Djava.security.auth.login.config=$ZK_SERVER_CONF" bin/zkServer start
                  </pre><p>

                </p><p>
                  Start your HBase cluster by running one or more of the following set of commands on the appropriate nodes:
                </p><pre class="programlisting">                  bin/hbase master start
                  bin/hbase regionserver start
                </pre></div><div class="section" title="2.5.2.4.&nbsp;Zookeeper Server Authentication Log Output"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e1312"></a>2.5.2.4.&nbsp;Zookeeper Server Authentication Log Output</h4></div></div></div><p>If the configuration above is successful,
                you should see something similar to the following in
                your Zookeeper server logs:
                </p><pre class="programlisting">11/12/05 22:43:39 INFO zookeeper.Login: successfully logged in.
11/12/05 22:43:39 INFO server.NIOServerCnxnFactory: binding to port 0.0.0.0/0.0.0.0:2181
11/12/05 22:43:39 INFO zookeeper.Login: TGT refresh thread started.
11/12/05 22:43:39 INFO zookeeper.Login: TGT valid starting at:        Mon Dec 05 22:43:39 UTC 2011
11/12/05 22:43:39 INFO zookeeper.Login: TGT expires:                  Tue Dec 06 22:43:39 UTC 2011
11/12/05 22:43:39 INFO zookeeper.Login: TGT refresh sleeping until: Tue Dec 06 18:36:42 UTC 2011
..
11/12/05 22:43:59 INFO auth.SaslServerCallbackHandler: 
  Successfully authenticated client: authenticationID=hbase/ip-10-166-175-249.us-west-1.compute.internal@HADOOP.LOCALDOMAIN; 
  authorizationID=hbase/ip-10-166-175-249.us-west-1.compute.internal@HADOOP.LOCALDOMAIN.
11/12/05 22:43:59 INFO auth.SaslServerCallbackHandler: Setting authorizedID: hbase
11/12/05 22:43:59 INFO server.ZooKeeperServer: adding SASL authorization for authorizationID: hbase
                </pre><p>
                  
                </p></div><div class="section" title="2.5.2.5.&nbsp;Zookeeper Client Authentication Log Output"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e1320"></a>2.5.2.5.&nbsp;Zookeeper Client Authentication Log Output</h4></div></div></div><p>On the Zookeeper client side (HBase master or regionserver),
                you should see something similar to the following:

                </p><pre class="programlisting">11/12/05 22:43:59 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=ip-10-166-175-249.us-west-1.compute.internal:2181 sessionTimeout=180000 watcher=master:60000
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Opening socket connection to server /10.166.175.249:2181
11/12/05 22:43:59 INFO zookeeper.RecoverableZooKeeper: The identifier of this process is 14851@ip-10-166-175-249
11/12/05 22:43:59 INFO zookeeper.Login: successfully logged in.
11/12/05 22:43:59 INFO client.ZooKeeperSaslClient: Client will use GSSAPI as SASL mechanism.
11/12/05 22:43:59 INFO zookeeper.Login: TGT refresh thread started.
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Socket connection established to ip-10-166-175-249.us-west-1.compute.internal/10.166.175.249:2181, initiating session
11/12/05 22:43:59 INFO zookeeper.Login: TGT valid starting at:        Mon Dec 05 22:43:59 UTC 2011
11/12/05 22:43:59 INFO zookeeper.Login: TGT expires:                  Tue Dec 06 22:43:59 UTC 2011
11/12/05 22:43:59 INFO zookeeper.Login: TGT refresh sleeping until: Tue Dec 06 18:30:37 UTC 2011
11/12/05 22:43:59 INFO zookeeper.ClientCnxn: Session establishment complete on server ip-10-166-175-249.us-west-1.compute.internal/10.166.175.249:2181, sessionid = 0x134106594320000, negotiated timeout = 180000
                </pre><p>
                </p></div><div class="section" title="2.5.2.6.&nbsp;Configuration from Scratch"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e1328"></a>2.5.2.6.&nbsp;Configuration from Scratch</h4></div></div></div>

                This has been tested on the current standard Amazon
                Linux AMI.  First setup KDC and principals as
                described above. Next checkout code and run a sanity
                check.
                
                <pre class="programlisting">                git clone git://git.apache.org/hbase.git
                cd hbase
                mvn -Psecurity,localTests clean test -Dtest=TestZooKeeperACL
                </pre>

                Then configure HBase as described above.
                Manually edit target/cached_classpath.txt (see below)..

                <pre class="programlisting">                bin/hbase zookeeper &amp;
                bin/hbase master &amp;
                bin/hbase regionserver &amp;
                </pre></div><div class="section" title="2.5.2.7.&nbsp;Future improvements"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e1337"></a>2.5.2.7.&nbsp;Future improvements</h4></div></div></div><div class="section" title="2.5.2.7.1.&nbsp;Fix target/cached_classpath.txt"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e1340"></a>2.5.2.7.1.&nbsp;Fix target/cached_classpath.txt</h5></div></div></div><p>
                You must override the standard hadoop-core jar file from the
                <code class="code">target/cached_classpath.txt</code>
                file with the version containing the HADOOP-7070 fix. You can use the following script to do this:

                </p><pre class="programlisting">                  echo `find ~/.m2 -name "*hadoop-core*7070*SNAPSHOT.jar"` ':' `cat target/cached_classpath.txt` | sed 's/ //g' &gt; target/tmp.txt 
                  mv target/tmp.txt target/cached_classpath.txt
                </pre><p>

                </p></div><div class="section" title="2.5.2.7.2.&nbsp;Set JAAS configuration programmatically"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e1351"></a>2.5.2.7.2.&nbsp;Set JAAS configuration
                  programmatically</h5></div></div></div> 


                  This would avoid the need for a separate Hadoop jar
                  that fixes <a class="link" href="https://issues.apache.org/jira/browse/HADOOP-7070" target="_top">HADOOP-7070</a>.
                </div><div class="section" title="2.5.2.7.3.&nbsp;Elimination of kerberos.removeHostFromPrincipal and kerberos.removeRealmFromPrincipal"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e1358"></a>2.5.2.7.3.&nbsp;Elimination of 
                  <code class="code">kerberos.removeHostFromPrincipal</code> and 
                  <code class="code">kerberos.removeRealmFromPrincipal</code></h5></div></div></div></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d1934e968" href="#d1934e968" class="para">13</a>] </sup>For the full list of ZooKeeper configurations, see
                ZooKeeper's <code class="filename">zoo.cfg</code>. HBase does not ship
                with a <code class="filename">zoo.cfg</code> so you will need to browse
                the <code class="filename">conf</code> directory in an appropriate
            ZooKeeper download.</p></div></div></div>
  <div title="2.5.2. SASL Authentication with ZooKeeper">
    <div>
      <div>
        <div></div>
      </div>
    </div>
  </div>
  <div> </div>
<h3 class="title"><a name="config.files" id="config.files"></a>2.6.&nbsp;配置文件</h3></div></div></div><div class="section" title="2.3.1. 简单的分布式Hbase安装"><div class="titlepage"><div><div>
  <div class="titlepage">
    <div>
      <div>
        <h2 class="title">&nbsp;</h2>
      </div>
    </div>
  </div>
<p> Hbase的配置系统和Hadoop一样。在<code class="filename">conf/hbase-env.sh</code>配置系统的部署信息和环境变量。 -- 这个配置会被启动shell使用 -- 然后在XML文件里配置信息，覆盖默认的配置。告知Hbase使用什么目录地址，ZooKeeper的位置等等信息。 <sup>[<a name="d613e1048" href="book.htm#ftn.d613e1048" class="footnote">10</a>]</sup> . </p>
  <p>当你使用分布式模式的时间，当你编辑完一个文件之后，记得要把这个文件复制到整个集群的<code class="filename">conf</code> 目录下。Hbase不会帮你做这些，你得用 <span class="command"><strong>rsync</strong></span>.</p>
  <div class="section" title="3.1. hbase-site.xml 和 hbase-default.xml">
    <div class="titlepage">
      <div>
        <div>
          <h3 class="title" style="clear: both"><a name="hbase.site"></a>2.6.1.&nbsp;<code class="filename">hbase-site.xml</code> 和 <code class="filename">hbase-default.xml</code></h3>
        </div>
      </div>
    </div>
    <p>正如Hadoop放置HDFS的配置文件<code class="filename">hdfs-site.xml</code>，Hbase的配置文件是 <code class="filename">conf/hbase-site.xml</code>.
      你可以在 <a class="xref" href="book.htm#hbase_default_configurations" title="3.1.1. HBase 默认配置">Section&nbsp;3.1.1, “HBase 默认配置”</a>找到配置的属性列表。你也可以看有代码里面的<code class="filename">hbase-default.xml</code>文件，他在<code class="filename">src/main/resources</code>目录下。 </p>
    <p> 不是所有的配置都在 <code class="filename">hbase-default.xml</code>出现.只要改了代码，配置就有可能改变，所以唯一了解这些被改过的配置的办法是读源代码本身。 </p>
    <p> 要注意的是，要重启集群才能是配置生效。 </p>
    <div class="section" title="3.1.1. HBase 默认配置">
      <div class="titlepage">
        <div>
          <div>
            <h3 class="title"><a name="hbase_default_configurations"></a>2.6.1.1.&nbsp;HBase 默认配置</h3>
          </div>
        </div>
      </div>
      <p></p>
      <div class="glossary" title="HBase 默认配置">
        <div class="titlepage">
          <div>
            <div>
              <h4 class="title"><a name="hbase.default.configuration"></a>HBase 默认配置</h4>
            </div>
          </div>
        </div>
        <p> 该文档是用hbase默认配置文件生成的，文件源是 <code class="filename">hbase-default.xml</code>(因翻译需要，被译者修改成中文注释). </p>
        <dl>
          <dt><a name="hbase.rootdir"></a><code class="varname">hbase.rootdir</code></dt>
          <dd>
            <p>这个目录是region server的共享目录，用来持久化Hbase。URL需要是'完全正确'的，还要包含文件系统的scheme。例如，要表示hdfs中的'/hbase'目录，namenode 运行在namenode.example.org的9090端口。则需要设置为hdfs://namenode.example.org:9000/hbase。默认情况下Hbase是写到/tmp的。不改这个配置，数据会在重启的时候丢失。 </p>
            <p>默认: <code class="varname">file:///tmp/hbase-${user.name}/hbase</code></p>
          </dd>
          <dt><a name="hbase.master.port"></a><code class="varname">hbase.master.port</code></dt>
          <dd>
            <p>Hbase的Master的端口.</p>
            <p>默认: <code class="varname">60000</code></p>
          </dd>
          <dt><a name="hbase.cluster.distributed"></a><code class="varname">hbase.cluster.distributed</code></dt>
          <dd>
            <p>Hbase的运行模式。false是单机模式，true是分布式模式。若为false,Hbase和Zookeeper会运行在同一个JVM里面。 </p>
            <p>默认: <code class="varname">false</code></p>
          </dd>
          <dt><a name="hbase.tmp.dir"></a><code class="varname">hbase.tmp.dir</code></dt>
          <dd>
            <p>本地文件系统的临时文件夹。可以修改到一个更为持久的目录上。(/tmp会在重启时清楚) </p>
            <p>默认: <code class="varname">/tmp/hbase-${user.name}</code></p>
          </dd>
          <dt><a name="hbase.master.info.port"></a><code class="varname">hbase.master.info.port</code></dt>
          <dd>
            <p>HBase Master web 界面端口.
              设置为-1 意味着你不想让他运行。 </p>
            <p>默认: <code class="varname">60010</code></p>
          </dd>
          <dt><a name="hbase.master.info.bindAddress"></a><code class="varname">hbase.master.info.bindAddress</code></dt>
          <dd>
            <p> HBase Master web 界面绑定的端口 </p>
            <p>默认: <code class="varname">0.0.0.0</code></p>
          </dd>
          <dt><a name="hbase.client.write.buffer"></a><code class="varname">hbase.client.write.buffer</code></dt>
          <dd>
            <p>HTable客户端的写缓冲的默认大小。这个值越大，需要消耗的内存越大。因为缓冲在客户端和服务端都有实例，所以需要消耗客户端和服务端两个地方的内存。得到的好处是，可以减少RPC的次数。可以这样估算服务器端被占用的内存： hbase.client.write.buffer * hbase.regionserver.handler.count </p>
            <p>默认: <code class="varname">2097152</code></p>
          </dd>
          <dt><a name="hbase.regionserver.port"></a><code class="varname">hbase.regionserver.port</code></dt>
          <dd>
            <p>HBase RegionServer绑定的端口 </p>
            <p>默认: <code class="varname">60020</code></p>
          </dd>
          <dt><a name="hbase.regionserver.info.port"></a><code class="varname">hbase.regionserver.info.port</code></dt>
          <dd>
            <p> HBase RegionServer web 界面绑定的端口
              设置为 -1 意味这你不想与运行 RegionServer 界面. </p>
            <p>默认: <code class="varname">60030</code></p>
          </dd>
          <dt><a name="hbase.regionserver.info.port.auto"></a><code class="varname">hbase.regionserver.info.port.auto</code></dt>
          <dd>
            <p>Master或RegionServer是否要动态搜一个可以用的端口来绑定界面。当hbase.regionserver.info.port已经被占用的时候，可以搜一个空闲的端口绑定。这个功能在测试的时候很有用。默认关闭。 </p>
            <p>默认: <code class="varname">false</code></p>
          </dd>
          <dt><a name="hbase.regionserver.info.bindAddress"></a><code class="varname">hbase.regionserver.info.bindAddress</code></dt>
          <dd>
            <p>HBase RegionServer web 界面的IP地址 </p>
            <p>默认: <code class="varname">0.0.0.0</code></p>
          </dd>
          <dt><a name="hbase.regionserver.class"></a><code class="varname">hbase.regionserver.class</code></dt>
          <dd>
            <p> RegionServer 使用的接口。客户端打开代理来连接region server的时候会使用到。 </p>
            <p>默认: <code class="varname">org.apache.hadoop.hbase.ipc.HRegionInterface</code></p>
          </dd>
          <dt><a name="hbase.client.pause"></a><code class="varname">hbase.client.pause</code></dt>
          <dd>
            <p>通常的客户端暂停时间。最多的用法是客户端在重试前的等待时间。比如失败的get操作和region查询操作等都很可能用到。</p>
            <p>默认: <code class="varname">1000</code></p>
          </dd>
          <dt><a name="hbase.client.retries.number"></a><code class="varname">hbase.client.retries.number</code></dt>
          <dd>
            <p>最大重试次数。例如 region查询，Get操作，Update操作等等都可能发生错误，需要重试。这是最大重试错误的值。 </p>
            <p>默认: <code class="varname">10</code></p>
          </dd>
          <dt><a name="hbase.client.scanner.caching"></a><code class="varname">hbase.client.scanner.caching</code></dt>
          <dd>
            <p>当调用Scanner的next方法，而值又不在缓存里的时候，从服务端一次获取的行数。越大的值意味着Scanner会快一些，但是会占用更多的内存。当缓冲被占满的时候，next方法调用会越来越慢。慢到一定程度，可能会导致超时。例如超过了hbase.regionserver.lease.period。 </p>
            <p>默认: <code class="varname">1</code></p>
          </dd>
          <dt><a name="hbase.client.keyvalue.maxsize"></a><code class="varname">hbase.client.keyvalue.maxsize</code></dt>
          <dd>
            <p>一个KeyValue实例的最大size.这个是用来设置存储文件中的单个entry的大小上界。因为一个KeyValue是不能分割的，所以可以避免因为数据过大导致region不可分割。明智的做法是把它设为可以被最大region size整除的数。如果设置为0或者更小，就会禁用这个检查。默认10MB。 </p>
            <p>默认: <code class="varname">10485760</code></p>
          </dd>
          <dt><a name="hbase.regionserver.lease.period"></a><code class="varname">hbase.regionserver.lease.period</code></dt>
          <dd>
            <p>客户端租用HRegion server 期限，即超时阀值。单位是毫秒。默认情况下，客户端必须在这个时间内发一条信息，否则视为死掉。</p>
            <p>默认: <code class="varname">60000</code></p>
          </dd>
          <dt><a name="hbase.regionserver.handler.count"></a><code class="varname">hbase.regionserver.handler.count</code></dt>
          <dd>
            <p>RegionServers受理的RPC Server实例数量。对于Master来说，这个属性是Master受理的handler数量 </p>
            <p>默认: <code class="varname">10</code></p>
          </dd>
          <dt><a name="hbase.regionserver.msginterval"></a><code class="varname">hbase.regionserver.msginterval</code></dt>
          <dd>
            <p> RegionServer 发消息给 Master 时间间隔，单位是毫秒 </p>
            <p>默认: <code class="varname">3000</code></p>
          </dd>
          <dt><a name="hbase.regionserver.optionallogflushinterval"></a><code class="varname">hbase.regionserver.optionallogflushinterval</code></dt>
          <dd>
            <p>将Hlog同步到HDFS的间隔。如果Hlog没有积累到一定的数量，到了时间，也会触发同步。默认是1秒，单位毫秒。 </p>
            <p>默认: <code class="varname">1000</code></p>
          </dd>
          <dt><a name="hbase.regionserver.regionSplitLimit"></a><code class="varname">hbase.regionserver.regionSplitLimit</code></dt>
          <dd>
            <p>region的数量到了这个值后就不会在分裂了。这不是一个region数量的硬性限制。但是起到了一定指导性的作用，到了这个值就该停止分裂了。默认是MAX_INT.就是说不阻止分裂。</p>
            <p>默认: <code class="varname">2147483647</code></p>
          </dd>
          <dt><a name="hbase.regionserver.logroll.period"></a><code class="varname">hbase.regionserver.logroll.period</code></dt>
          <dd>
            <p>提交commit log的间隔，不管有没有写足够的值。</p>
            <p>默认: <code class="varname">3600000</code></p>
          </dd>
          <dt><a name="hbase.regionserver.hlog.reader.impl"></a><code class="varname">hbase.regionserver.hlog.reader.impl</code></dt>
          <dd>
            <p> HLog file reader 的实现.</p>
            <p>默认: <code class="varname">org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader</code></p>
          </dd>
          <dt><a name="hbase.regionserver.hlog.writer.impl"></a><code class="varname">hbase.regionserver.hlog.writer.impl</code></dt>
          <dd>
            <p> HLog file writer 的实现.</p>
            <p>默认: <code class="varname">org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter</code></p>
          </dd>
          <dt><a name="hbase.regionserver.thread.splitcompactcheckfrequency"></a><code class="varname">hbase.regionserver.thread.splitcompactcheckfrequency</code></dt>
          <dd>
            <p> region server 多久执行一次split/compaction 检查. </p>
            <p>默认: <code class="varname">20000</code></p>
          </dd>
          <dt><a name="hbase.regionserver.nbreservationblocks"></a><code class="varname">hbase.regionserver.nbreservationblocks</code></dt>
          <dd>
            <p>储备的内存block的数量(译者注:就像石油储备一样)。当发生out of memory 异常的时候，我们可以用这些内存在RegionServer停止之前做清理操作。 </p>
            <p>默认: <code class="varname">4</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.dns.interface"></a><code class="varname">hbase.zookeeper.dns.interface</code></dt>
          <dd>
            <p>当使用DNS的时候，Zookeeper用来上报的IP地址的网络接口名字。 </p>
            <p>默认: <code class="varname">default</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.dns.nameserver"></a><code class="varname">hbase.zookeeper.dns.nameserver</code></dt>
          <dd>
            <p>当使用DNS的时候，Zookeepr使用的DNS的域名或者IP 地址，Zookeeper用它来确定和master用来进行通讯的域名. </p>
            <p>默认: <code class="varname">default</code></p>
          </dd>
          <dt><a name="hbase.regionserver.dns.interface"></a><code class="varname">hbase.regionserver.dns.interface</code></dt>
          <dd>
            <p>当使用DNS的时候，RegionServer用来上报的IP地址的网络接口名字。 </p>
            <p>默认: <code class="varname">default</code></p>
          </dd>
          <dt><a name="hbase.regionserver.dns.nameserver"></a><code class="varname">hbase.regionserver.dns.nameserver</code></dt>
          <dd>
            <p>当使用DNS的时候，RegionServer使用的DNS的域名或者IP 地址，RegionServer用它来确定和master用来进行通讯的域名. </p>
            <p>默认: <code class="varname">default</code></p>
          </dd>
          <dt><a name="hbase.master.dns.interface"></a><code class="varname">hbase.master.dns.interface</code></dt>
          <dd>
            <p>当使用DNS的时候，Master用来上报的IP地址的网络接口名字。 </p>
            <p>默认: <code class="varname">default</code></p>
          </dd>
          <dt><a name="hbase.master.dns.nameserver"></a><code class="varname">hbase.master.dns.nameserver</code></dt>
          <dd>
            <p>当使用DNS的时候，RegionServer使用的DNS的域名或者IP 地址，Master用它来确定用来进行通讯的域名. </p>
            <p>默认: <code class="varname">default</code></p>
          </dd>
          <dt><a name="hbase.balancer.period%0A    "></a><code class="varname">hbase.balancer.period </code></dt>
          <dd>
            <p>Master执行region balancer的间隔。 </p>
            <p>默认: <code class="varname">300000</code></p>
          </dd>
          <dt><a name="hbase.regions.slop"></a><code class="varname">hbase.regions.slop</code></dt>
          <dd>
            <p>当任一regionserver有average + (average * slop)个region是会执行Rebalance </p>
            <p>默认: <code class="varname">0</code></p>
          </dd>
          <dt><a name="hbase.master.logcleaner.ttl"></a><code class="varname">hbase.master.logcleaner.ttl</code></dt>
          <dd>
            <p>Hlog存在于.oldlogdir 文件夹的最长时间,
              超过了就会被 Master 的线程清理掉. </p>
            <p>默认: <code class="varname">600000</code></p>
          </dd>
          <dt><a name="hbase.master.logcleaner.plugins"></a><code class="varname">hbase.master.logcleaner.plugins</code></dt>
          <dd>
            <p>LogsCleaner服务会执行的一组LogCleanerDelegat。值用逗号间隔的文本表示。这些WAL/HLog cleaners会按顺序调用。可以把先调用的放在前面。你可以实现自己的LogCleanerDelegat，加到Classpath下，然后在这里写下类的全称。一般都是加在默认值的前面。 </p>
            <p>默认: <code class="varname">org.apache.hadoop.hbase.master.TimeToLiveLogCleaner</code></p>
          </dd>
          <dt><a name="hbase.regionserver.global.memstore.upperLimit"></a><code class="varname">hbase.regionserver.global.memstore.upperLimit</code></dt>
          <dd>
            <p>单个region server的全部memtores的最大值。超过这个值，一个新的update操作会被挂起，强制执行flush操作。 </p>
            <p>默认: <code class="varname">0.4</code></p>
          </dd>
          <dt><a name="hbase.regionserver.global.memstore.lowerLimit"></a><code class="varname">hbase.regionserver.global.memstore.lowerLimit</code></dt>
          <dd>
            <p>当强制执行flush操作的时候，当低于这个值的时候，flush会停止。默认是堆大小的 35% .
              如果这个值和 hbase.regionserver.global.memstore.upperLimit 相同就意味着当update操作因为内存限制被挂起时，会尽量少的执行flush(译者注:一旦执行flush，值就会比下限要低，不再执行) </p>
            <p>默认: <code class="varname">0.35</code></p>
          </dd>
          <dt><a name="hbase.server.thread.wakefrequency"></a><code class="varname">hbase.server.thread.wakefrequency</code></dt>
          <dd>
            <p>service工作的sleep间隔，单位毫秒。
              可以作为service线程的sleep间隔，比如log roller. </p>
            <p>默认: <code class="varname">10000</code></p>
          </dd>
          <dt><a name="hbase.hregion.memstore.flush.size"></a><code class="varname">hbase.hregion.memstore.flush.size</code></dt>
          <dd>
            <p> 当memstore的大小超过这个值的时候，会flush到磁盘。这个值被一个线程每隔hbase.server.thread.wakefrequency检查一下。 </p>
            <p>默认: <code class="varname">67108864</code></p>
          </dd>
          <dt><a name="hbase.hregion.preclose.flush.size"></a><code class="varname">hbase.hregion.preclose.flush.size</code></dt>
          <dd>
            <p> 当一个region中的memstore的大小大于这个值的时候，我们又触发了close.会先运行“pre-flush”操作，清理这个需要关闭的memstore，然后将这个region下线。当一个region下线了，我们无法再进行任何写操作。如果一个memstore很大的时候，flush操作会消耗很多时间。"pre-flush"操作意味着在region下线之前，会先把memstore清空。这样在最终执行close操作的时候，flush操作会很快。 </p>
            <p>默认: <code class="varname">5242880</code></p>
          </dd>
          <dt><a name="hbase.hregion.memstore.block.multiplier"></a><code class="varname">hbase.hregion.memstore.block.multiplier</code></dt>
          <dd>
            <p> 如果memstore有hbase.hregion.memstore.block.multiplier倍数的hbase.hregion.flush.size的大小，就会阻塞update操作。这是为了预防在update高峰期会导致的失控。如果不设上界，flush的时候会花很长的时间来合并或者分割，最坏的情况就是引发out of memory异常。(译者注:内存操作的速度和磁盘不匹配，需要等一等。原文似乎有误) </p>
            <p>默认: <code class="varname">2</code></p>
          </dd>
          <dt><a name="hbase.hregion.memstore.mslab.enabled"></a><code class="varname">hbase.hregion.memstore.mslab.enabled</code></dt>
          <dd>
            <p> 体验特性：启用memStore分配本地缓冲区。这个特性是为了防止在大量写负载的时候堆的碎片过多。这可以减少GC操作的频率。(GC有可能会Stop the world)(译者注：实现的原理相当于预分配内存，而不是每一个值都要从堆里分配) </p>
            <p>默认: <code class="varname">false</code></p>
          </dd>
          <dt><a name="hbase.hregion.max.filesize"></a><code class="varname">hbase.hregion.max.filesize</code></dt>
          <dd>
            <p> 最大HStoreFile大小。若某个Column families的HStoreFile增长达到这个值，这个Hegion会被切割成两个。
              Default: 256M. </p>
            <p>默认: <code class="varname">268435456</code></p>
          </dd>
          <dt><a name="hbase.hstore.compactionThreshold"></a><code class="varname">hbase.hstore.compactionThreshold</code></dt>
          <dd>
            <p> 当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，把这HStoreFiles写成一个。这个值越大，需要合并的时间就越长。 </p>
            <p>默认: <code class="varname">3</code></p>
          </dd>
          <dt><a name="hbase.hstore.blockingStoreFiles"></a><code class="varname">hbase.hstore.blockingStoreFiles</code></dt>
          <dd>
            <p> 当一个HStore含有多于这个值的HStoreFiles(每一个memstore flush产生一个HStoreFile)的时候，会执行一个合并操作，update会阻塞直到合并完成，直到超过了hbase.hstore.blockingWaitTime的值 </p>
            <p>默认: <code class="varname">7</code></p>
          </dd>
          <dt><a name="hbase.hstore.blockingWaitTime"></a><code class="varname">hbase.hstore.blockingWaitTime</code></dt>
          <dd>
            <p> hbase.hstore.blockingStoreFiles所限制的StoreFile数量会导致update阻塞，这个时间是来限制阻塞时间的。当超过了这个时间，HRegion会停止阻塞update操作，不过合并还有没有完成。默认为90s. </p>
            <p>默认: <code class="varname">90000</code></p>
          </dd>
          <dt><a name="hbase.hstore.compaction.max"></a><code class="varname">hbase.hstore.compaction.max</code></dt>
          <dd>
            <p>每个“小”合并的HStoreFiles最大数量。 </p>
            <p>默认: <code class="varname">10</code></p>
          </dd>
          <dt><a name="hbase.hregion.majorcompaction"></a><code class="varname">hbase.hregion.majorcompaction</code></dt>
          <dd>
            <p>一个Region中的所有HStoreFile的major compactions的时间间隔。默认是1天。
              设置为0就是禁用这个功能。 </p>
            <p>默认: <code class="varname">86400000</code></p>
          </dd>
          <dt><a name="hbase.mapreduce.hfileoutputformat.blocksize"></a><code class="varname">hbase.mapreduce.hfileoutputformat.blocksize</code></dt>
          <dd>
            <p>MapReduce中HFileOutputFormat可以写 storefiles/hfiles.
              这个值是hfile的blocksize的最小值。通常在Hbase写Hfile的时候，bloocksize是由table schema(HColumnDescriptor)决定的，但是在mapreduce写的时候，我们无法获取schema中blocksize。这个值越小，你的索引就越大，你随机访问需要获取的数据就越小。如果你的cell都很小，而且你需要更快的随机访问，可以把这个值调低。 </p>
            <p>默认: <code class="varname">65536</code></p>
          </dd>
          <dt><a name="hfile.block.cache.size"></a><code class="varname">hfile.block.cache.size</code></dt>
          <dd>
            <p> 分配给HFile/StoreFile的block cache占最大堆(-Xmx setting)的比例。默认是20%，设置为0就是不分配。 </p>
            <p>默认: <code class="varname">0.2</code></p>
          </dd>
          <dt><a name="hbase.hash.type"></a><code class="varname">hbase.hash.type</code></dt>
          <dd>
            <p>哈希函数使用的哈希算法。可以选择两个值:: murmur (MurmurHash) 和 jenkins (JenkinsHash).
              这个哈希是给 bloom filters用的. </p>
            <p>默认: <code class="varname">murmur</code></p>
          </dd>
          <dt><a name="hbase.master.keytab.file"></a><code class="varname">hbase.master.keytab.file</code></dt>
          <dd>
            <p>HMaster server验证登录使用的kerberos keytab 文件路径。(译者注：Hbase使用Kerberos实现安全) </p>
            <p>默认: <code class="varname"></code></p>
          </dd>
          <dt><a name="hbase.master.kerberos.principal"></a><code class="varname">hbase.master.kerberos.principal</code></dt>
          <dd>
            <p>例如. "hbase/_HOST@EXAMPLE.COM".  HMaster运行需要使用 kerberos principal name.  principal name 可以在: user/hostname@DOMAIN 中获取. 如果 "_HOST" 被用做hostname
              portion，需要使用实际运行的hostname来替代它。 </p>
            <p>默认: <code class="varname"></code></p>
          </dd>
          <dt><a name="hbase.regionserver.keytab.file"></a><code class="varname">hbase.regionserver.keytab.file</code></dt>
          <dd>
            <p>HRegionServer验证登录使用的kerberos keytab 文件路径。 </p>
            <p>默认: <code class="varname"></code></p>
          </dd>
          <dt><a name="hbase.regionserver.kerberos.principal"></a><code class="varname">hbase.regionserver.kerberos.principal</code></dt>
          <dd>
            <p> 例如. "hbase/_HOST@EXAMPLE.COM".  HRegionServer运行需要使用 kerberos principal name.  principal name 可以在: user/hostname@DOMAIN 中获取. 如果 "_HOST" 被用做hostname
              portion，需要使用实际运行的hostname来替代它。在这个文件中必须要有一个entry来描述 hbase.regionserver.keytab.file </p>
            <p>默认: <code class="varname"></code></p>
          </dd>
          <dt><a name="zookeeper.session.timeout"></a><code class="varname">zookeeper.session.timeout</code></dt>
          <dd>
            <p>ZooKeeper 会话超时.Hbase把这个值传递改zk集群，向他推荐一个会话的最大超时时间。详见http://hadoop.apache.org/zookeeper/docs/current/zookeeperProgrammers.html#ch_zkSessions
              "The client sends a requested timeout, the server responds with the
              timeout that it can give the client. "。
              单位是毫秒 </p>
            <p>默认: <code class="varname">180000</code></p>
          </dd>
          <dt><a name="zookeeper.znode.parent"></a><code class="varname">zookeeper.znode.parent</code></dt>
          <dd>
            <p>ZooKeeper中的Hbase的根ZNode。所有的Hbase的ZooKeeper会用这个目录配置相对路径。默认情况下，所有的Hbase的ZooKeeper文件路径是用相对路径，所以他们会都去这个目录下面。 </p>
            <p>默认: <code class="varname">/hbase</code></p>
          </dd>
          <dt><a name="zookeeper.znode.rootserver"></a><code class="varname">zookeeper.znode.rootserver</code></dt>
          <dd>
            <p>ZNode 保存的 根region的路径. 这个值是由Master来写，client和regionserver 来读的。如果设为一个相对地址，父目录就是 ${zookeeper.znode.parent}.默认情形下，意味着根region的路径存储在/hbase/root-region-server. </p>
            <p>默认: <code class="varname">root-region-server</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.quorum"></a><code class="varname">hbase.zookeeper.quorum</code></dt>
          <dd>
            <p>Zookeeper集群的地址列表，用逗号分割。例如："host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".默认是localhost,是给伪分布式用的。要修改才能在完全分布式的情况下使用。如果在hbase-env.sh设置了HBASE_MANAGES_ZK，这些ZooKeeper节点就会和Hbase一起启动。 </p>
            <p>默认: <code class="varname">localhost</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.peerport"></a><code class="varname">hbase.zookeeper.peerport</code></dt>
          <dd>
            <p>ZooKeeper节点使用的端口。详细参见：http://hadoop.apache.org/zookeeper/docs/r3.1.1/zookeeperStarted.html#sc_RunningReplicatedZooKeeper </p>
            <p>默认: <code class="varname">2888</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.leaderport"></a><code class="varname">hbase.zookeeper.leaderport</code></dt>
          <dd>
            <p>ZooKeeper用来选择Leader的端口，详细参见：http://hadoop.apache.org/zookeeper/docs/r3.1.1/zookeeperStarted.html#sc_RunningReplicatedZooKeeper </p>
            <p>默认: <code class="varname">3888</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.property.initLimit"></a><code class="varname">hbase.zookeeper.property.initLimit</code></dt>
          <dd>
            <p>ZooKeeper的zoo.conf中的配置。
              初始化synchronization阶段的ticks数量限制 </p>
            <p>默认: <code class="varname">10</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.property.syncLimit"></a><code class="varname">hbase.zookeeper.property.syncLimit</code></dt>
          <dd>
            <p>ZooKeeper的zoo.conf中的配置。
              发送一个请求到获得承认之间的ticks的数量限制 </p>
            <p>默认: <code class="varname">5</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.property.dataDir"></a><code class="varname">hbase.zookeeper.property.dataDir</code></dt>
          <dd>
            <p>ZooKeeper的zoo.conf中的配置。
              快照的存储位置 </p>
            <p>默认: <code class="varname">${hbase.tmp.dir}/zookeeper</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.property.clientPort"></a><code class="varname">hbase.zookeeper.property.clientPort</code></dt>
          <dd>
            <p>ZooKeeper的zoo.conf中的配置。
              客户端连接的端口 </p>
            <p>默认: <code class="varname">2181</code></p>
          </dd>
          <dt><a name="hbase.zookeeper.property.maxClientCnxns"></a><code class="varname">hbase.zookeeper.property.maxClientCnxns</code></dt>
          <dd>
            <p>ZooKeeper的zoo.conf中的配置。
              ZooKeeper集群中的单个节点接受的单个Client(以IP区分)的请求的并发数。这个值可以调高一点，防止在单机和伪分布式模式中出问题。 </p>
            <p>默认: <code class="varname">2000</code></p>
          </dd>
          <dt><a name="hbase.rest.port"></a><code class="varname">hbase.rest.port</code></dt>
          <dd>
            <p>HBase REST server的端口</p>
            <p>默认: <code class="varname">8080</code></p>
          </dd>
          <dt><a name="hbase.rest.readonly"></a><code class="varname">hbase.rest.readonly</code></dt>
          <dd>
            <p> 定义REST server的运行模式。可以设置成如下的值：
              
              false: 所有的HTTP请求都是被允许的 - GET/PUT/POST/DELETE.
              true:只有GET请求是被允许的 </p>
            <p>默认: <code class="varname">false</code></p>
          </dd>
        </dl>
      </div>
    </div>
  </div>
  <div class="section" title="3.2.  hbase-env.sh">
    <div class="titlepage">
      <div>
        <div>
          <h2 class="title" style="clear: both"><a name="hbase.env.sh"></a>2.6.2.&nbsp;<code class="filename">hbase-env.sh</code></h2>
        </div>
      </div>
    </div>
    <p>在这个文件里面设置HBase环境变量。比如可以配置JVM启动的堆大小或者GC的参数。你还可在这里配置Hbase的参数，如Log位置，niceness(译者注:优先级)，ssh参数还有pid文件的位置等等。打开文件<code class="filename">conf/hbase-env.sh</code>细读其中的内容。每个选项都是有详尽的注释的。你可以在此添加自己的环境变量。 </p>
    <p> 这个文件的改动系统Hbase重启才能生效。 </p>
  </div>
  <div class="section" title="3.3. log4j.properties">
    <div class="titlepage">
      <div>
        <div>
          <h2 class="title" style="clear: both"><a name="log4j"></a>2.6.3.&nbsp;<code class="filename">log4j.properties</code></h2>
        </div>
      </div>
    </div>
    <p>编辑这个文件可以改变Hbase的日志的级别，轮滚策略等等。 </p>
    <p> 这个文件的改动系统Hbase重启才能生效。
      日志级别的更改会影响到HBase UI </p>
  </div>
  <div class="section" title="3.4. 重要的配置">
    <div class="titlepage">
      <div>
        <div>
          <div class="section" title="3.7. 连接Hbase集群的客户端配置和依赖">
            <div class="titlepage">
              <div>
                <div>
                  <h3 class="title" style="clear: both"><a name="client_dependencies"></a>2.6.4.&nbsp;连接Hbase集群的客户端配置和依赖</h3>
                </div>
              </div>
            </div>
            <p> 因为Hbase的Master有可能转移，所有客户端需要访问ZooKeeper来获得现在的位置。ZooKeeper会保存这些值。因此客户端必须知道Zookeeper集群的地址，否则做不了任何事情。通常这个地址存在 <code class="filename">hbase-site.xml</code> 里面，客户端可以从<code class="varname">CLASSPATH</code>取出这个文件.</p>
            <p>如果你是使用一个IDE来运行Hbase客户端，你需要将<code class="filename">conf/</code>放入你的 classpath,这样 <code class="filename">hbase-site.xml</code>就可以找到了，(或者把hbase-site.xml放到 <code class="filename">src/test/resources</code>，这样测试的时候可以使用). </p>
            <p> Hbase客户端最小化的依赖是 hbase, hadoop, log4j, commons-logging, commons-lang,
              和 ZooKeeper ，这些jars 需要能在 <code class="varname">CLASSPATH</code> 中找到。 </p>
            <p> 下面是一个基本的客户端 <code class="filename">hbase-site.xml</code> 例子： </p>
            <pre class="programlisting">&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;example1,example2,example3&lt;/value&gt;
    &lt;description&gt;The directory shared by region servers.
    &lt;/description&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
          </pre>
            <p></p>
            <div class="section" title="3.7.1. Java客户端配置">
              <div class="titlepage">
                <div>
                  <div>
                    <h4 class="title"><a name="d613e2045"></a>2.6.4.1.&nbsp;Java客户端配置</h4>
                  </div>
                  <div>
                    <h5 class="subtitle">Java是如何读到<code class="filename">hbase-site.xml</code> 的内容的</h5>
                  </div>
                </div>
              </div>
              <p>Java客户端使用的配置信息是被映射在一个<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HBaseConfiguration" target="_top">HBaseConfiguration</a> 实例中.
                
                HBaseConfiguration有一个工厂方法, <code class="code">HBaseConfiguration.create();</code>,运行这个方法的时候，他会去<code class="varname">CLASSPATH</code>,下找<code class="filename">hbase-site.xml</code>，读他发现的第一个配置文件的内容。
                
                (这个方法还会去找<code class="filename">hbase-default.xml</code> ; <code class="filename">hbase.X.X.X.jar</code>里面也会有一个an hbase-default.xml). 不使用任何<code class="filename">hbase-site.xml</code>文件直接通过Java代码注入配置信息也是可以的。例如，你可以用编程的方式设置ZooKeeper信息，只要这样做: </p>
              <pre class="programlisting">Configuration config = HBaseConfiguration.create();
config.set("hbase.zookeeper.quorum", "localhost");  // Here we are running zookeeper locally</pre>
              <p> 如果有多ZooKeeper实例，你可以使用逗号列表。(就像在<code class="filename">hbase-site.xml</code> 文件中做得一样).
                这个 <code class="classname">Configuration</code> 实例会被传递到 <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>,
                之类的实例里面去. </p>
            </div>
          </div>
          <div class="footnotes"><br>
            <hr width="100" align="left">
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e1048" href="book.htm#d613e1048" class="para">10</a>] </sup> Be careful editing XML.  Make sure you close all elements.
                Run your file through <span class="command"><strong>xmllint</strong></span> or similar
                to ensure well-formedness of your document after an edit session. </p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e1948" href="book.htm#d613e1948" class="para">11</a>] </sup>参见 <a class="xref" href="book.htm#hbase.regionserver.codecs" title="B.2.  hbase.regionserver.codecs">Section&nbsp;B.2, “ <code class="varname"> hbase.regionserver.codecs </code> ”</a> 可以看到关于LZO安装的具体信息，帮助你放在安装失败。</p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e1974" href="book.htm#d613e1974" class="para">12</a>] </sup>What follows is taken from the javadoc at the head of
                the <code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code> tool
                added to HBase post-0.90.0 release. </p>
            </div>
          </div>
          <div class="section" title="2.1. 需要的软件">
            <div class="section" title="2.3. 配置例子">
              <div class="section" title="2.3.1. 简单的分布式Hbase安装">
                <div class="titlepage">
                  <div>
                    <div>
                      <h2 class="title"><a name="example_config"></a>2.7. 配置示例</h2>
                      <h3 class="title"><a name="d613e896"></a>2.7.1.&nbsp;简单的分布式Hbase安装</h3>
                    </div>
                  </div>
                </div>
                <p>这里是一个10节点的Hbase的简单示例，这里的配置都是基本的，节点名为 <code class="varname">example0</code>, <code class="varname">example1</code>... 一直到 <code class="varname">example9</code> .  HBase Master 和
                  HDFS namenode 运作在同一个节点 <code class="varname">example0</code>上.
                  RegionServers 运行在节点 <code class="varname">example1</code>-<code class="varname">example9</code>. 一个 3-节点
                  ZooKeeper 集群运行在<code class="varname">example1</code>, <code class="varname">example2</code>, 和 <code class="varname">example3</code>，端口保持默认. ZooKeeper 的数据保存在目录 <code class="filename">/export/zookeeper</code>. 下面我们展示主要的配置文件-- <code class="filename">hbase-site.xml</code>, <code class="filename">regionservers</code>, 和 <code class="filename">hbase-env.sh</code> -- 这些文件可以在 <code class="filename">conf</code>目录找到.</p>
                <div class="section" title="2.3.1.1. hbase-site.xml">
                  <div class="titlepage">
                    <div>
                      <div>
                        <h4 class="title"><a name="hbase_site"></a>2.7.1.1.&nbsp;<code class="filename">hbase-site.xml</code></h4>
                      </div>
                    </div>
                  </div>
                  <pre class="programlisting">
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;
&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;
    &lt;value&gt;example1,example2,example3&lt;/value&gt;
    &lt;description&gt;The directory shared by RegionServers.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;
    &lt;value&gt;/export/zookeeper&lt;/value&gt;
    &lt;description&gt;Property from ZooKeeper's config zoo.cfg.
    The directory where the snapshot is stored.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.rootdir&lt;/name&gt;
    &lt;value&gt;hdfs://example0:9000/hbase&lt;/value&gt;
    &lt;description&gt;The directory shared by RegionServers.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;
    &lt;value&gt;true&lt;/value&gt;
    &lt;description&gt;The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    &lt;/description&gt;
  &lt;/property&gt;
&lt;/configuration&gt;

          </pre>
                </div>
                <div class="section" title="2.3.1.2. regionservers">
                  <div class="titlepage">
                    <div>
                      <div>
                        <h4 class="title"><a name="regionservers"></a>2.7.1.2.&nbsp;<code class="filename">regionservers</code></h4>
                      </div>
                    </div>
                  </div>
                  <p>这个文件把RegionServer的节点列了下来。在这个例子里面我们让所有的节点都运行RegionServer,除了第一个节点 <code class="varname">example1</code>，它要运行 HBase Master 和
                    HDFS namenode</p>
                  <pre class="programlisting">    example1
    example3
    example4
    example5
    example6
    example7
    example8
    example9
          </pre>
                </div>
                <div class="section" title="2.3.1.3. hbase-env.sh">
                  <div class="titlepage">
                    <div>
                      <div>
                        <h4 class="title"><a name="hbase_env"></a>2.7.1.3.&nbsp;<code class="filename">hbase-env.sh</code></h4>
                      </div>
                    </div>
                  </div>
                  <p>下面我们用<span class="command"><strong>diff</strong></span> 命令来展示 <code class="filename">hbase-env.sh</code> 文件相比默认变化的部分. 我们把Hbase的堆内存设置为4G而不是默认的1G.</p>
                  <pre class="programlisting">    
$ git diff hbase-env.sh
diff --git a/conf/hbase-env.sh b/conf/hbase-env.sh
index e70ebc6..96f8c27 100644
--- a/conf/hbase-env.sh
+++ b/conf/hbase-env.sh
@@ -31,7 +31,7 @@ export JAVA_HOME=/usr/lib//jvm/java-6-sun/
 # export HBASE_CLASSPATH=
 
 # The maximum amount of heap to use, in MB. Default is 1000.
-# export HBASE_HEAPSIZE=1000
+export HBASE_HEAPSIZE=4096
 
 # Extra Java runtime options.
 # Below are what we set by default.  May only work with SUN JVM.

          </pre>
                  <p>你可以使用 <span class="command"><strong>rsync</strong></span> 来同步 <code class="filename">conf</code> 文件夹到你的整个集群.</p>
                </div>
              </div>
            </div>
          </div>
          <div class="footnotes"><br>
            <hr width="100" align="left">
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e280" href="book.htm#d613e280" class="para">1</a>] </sup>See <a class="link" href="http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-append/CHANGES.txt" target="_top">CHANGES.txt</a> in branch-0.20-append to see list of patches involved adding
                append on the Hadoop 0.20 branch.</p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e389" href="book.htm#d613e389" class="para">2</a>] </sup>See Jack Levin's <a class="link" href="" target="_top">major hdfs issues</a> note up on the user list.</p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e396" href="book.htm#d613e396" class="para">3</a>] </sup>这样的需求对于数据库应用来说是很常见的，例如Oracle。 <span class="emphasis"><em>Setting Shell Limits for the Oracle User</em></span> in <a class="link" href="http://www.akadia.com/services/ora_linux_install_10g.html" target="_top"> Short Guide to install Oracle 10 on Linux</a>.</p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e408" href="book.htm#d613e408" class="para">4</a>] </sup>A useful read setting config on you hadoop cluster is Aaron
                Kimballs' Configuration
                Parameters: What can you just ignore?</p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e474" href="book.htm#d613e474" class="para">5</a>] </sup>参见 <a class="link" href="http://ccgtech.blogspot.com/2010/02/hadoop-hdfs-deceived-by-xciever.html" target="_top">Hadoop HDFS: Deceived by Xciever</a> for an informative rant on xceivering.</p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e543" href="book.htm#d613e543" class="para">6</a>] </sup>这两个命名法来自于Hadoop.</p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e606" href="book.htm#d613e606" class="para">7</a>] </sup>See <a class="link" href="http://hbase.apache.org/pseudo-distributed.html" target="_top">Pseudo-distributed
                mode extras</a> for notes on how to start extra Masters and
                RegionServers when running pseudo-distributed.</p>
            </div>
            <div class="footnote">
              <p><sup>[<a name="ftn.d613e690" href="book.htm#d613e690" class="para">8</a>] </sup>For the full list of ZooKeeper configurations, see
                ZooKeeper's <code class="filename">zoo.cfg</code>. HBase does not ship
                with a <code class="filename">zoo.cfg</code> so you will need to browse
                the <code class="filename">conf</code> directory in an appropriate
                ZooKeeper download.</p>
            </div>
          </div>
          <h2 class="title" style="clear: both"><a name="important_configurations"></a>2.8.&nbsp;重要的配置</h2>
        </div>
      </div>
    </div>
    <p>下面我们会列举<span class="emphasis"><em>重要</em></span> 的配置. 这个章节讲述必须的配置和那些值得一看的配置。(译者注:淘宝的博客也有本章节的内容，<a class="link" href="http://rdc.taobao.com/team/jm/archives/975" target="_top">HBase性能调优</a>，很详尽)。 </p>
  </div>
  <div class="section" title="3.5. 必须的配置">
    <div class="titlepage">
      <div>
        <div>
          <div title="2.8.1. Required Configurations">
            <div>
              <div>
                <div>
                  <h3>2.8.1. Required Configurations</h3>
                </div>
              </div>
            </div>
            <p>Review the <a href="book.htm#os" title="2.2. Operating System">Section 2.2, “Operating System”</a> and <a href="book.htm#hadoop" title="2.3. Hadoop">Section 2.3, “Hadoop”</a> sections.</p>
          </div>
          <div title="2.8.2. Recommended Configurations">
            <div>
              <div>
                <div>
                  <h3><a name="recommended_configurations"></a>2.8.2. Recommended Configurations</h3>
                </div>
              </div>
            </div>
            <div title="2.8.2.1. zookeeper.session.timeout">
              <div>
                <div>
                  <div>
                    <h4><a name="zookeeper.session.timeout"></a>2.8.2.1. zookeeper.session.timeout</h4>
                  </div>
                </div>
              </div>
              <p>这个默认值是3分钟。这意味着一旦一个server宕掉了，Master至少需要3分钟才能察觉到宕机，开始恢复。你可能希望将这个超时调短，这样Master就能更快的察觉到了。在你调这个值之前，你需要确认你的JVM的GC参数，否则一个长时间的GC操作就可能导致超时。（当一个RegionServer在运行一个长时间的GC的时候，你可能想要重启并恢复它）.</p>
              <p>要想改变这个配置，可以编辑 <code class="filename">hbase-site.xml</code>,
                将配置部署到全部集群，然后重启。</p>
              <p>我们之所以把这个值调的很高，是因为我们不想一天到晚在论坛里回答新手的问题。“为什么我在执行一个大规模数据导入的时候Region Server死掉啦”，通常这样的问题是因为长时间的GC操作引起的，他们的JVM没有调优。我们是这样想的，如果一个人对Hbase不很熟悉，不能期望他知道所有，打击他的自信心。等到他逐渐熟悉了，他就可以自己调这个参数了。</p>
            </div>
            <div title="2.8.2.2. Number of ZooKeeper Instances">
              <div>
                <div>
                  <div>
                    <h4><a name="zookeeper.instances"></a>2.8.2.2. Number of ZooKeeper Instances</h4>
                  </div>
                </div>
              </div>
              <p>See <a href="book.htm#zookeeper" title="2.5. ZooKeeper">Section 2.5, “ZooKeeper”</a>.</p>
            </div>
            <div title="2.8.2.3. hbase.regionserver.handler.count">
              <div>
                <div>
                  <div>
                    <h4><a name="hbase.regionserver.handler.count"></a>2.8.2.3. hbase.regionserver.handler.count</h4>
                  </div>
                </div>
              </div>
              <p>这个设置决定了处理用户请求的线程数量。默认是10，这个值设的比较小，主要是为了预防用户用一个比较大的写缓冲，然后还有很多客户端并发，这样region servers会垮掉。有经验的做法是，当请求内容很大(上MB，如大puts, 使用缓存的scans)的时候，把这个值放低。请求内容较小的时候(gets, 小puts, ICVs, deletes)，把这个值放大。 </p>
              <p> 当客户端的请求内容很小的时候，把这个值设置的和最大客户端数量一样是很安全的。一个典型的例子就是一个给网站服务的集群，put操作一般不会缓冲,绝大多数的操作是get操作。 </p>
              <p> 把这个值放大的危险之处在于，把所有的Put操作缓冲意味着对内存有很大的压力，甚至会导致OutOfMemory.一个运行在内存不足的机器的RegionServer会频繁的触发GC操作，渐渐就能感受到停顿。(因为所有请求内容所占用的内存不管GC执行几遍也是不能回收的)。一段时间后，集群也会受到影响，因为所有的指向这个region的请求都会变慢。这样就会拖累集群，加剧了这个问题。 </p>
<p>You can get a sense of whether you have too little or too many handlers by <a href="book.htm#rpc.logging" title="12.2.2.1. Enabling RPC-level logging">Section 12.2.2.1, “Enabling RPC-level logging”</a> on an individual RegionServer then tailing its logs (Queued requests consume memory).</p>
            </div>
            <div title="2.8.2.4. Configuration for large memory machines">
              <div>
                <div>
                  <div>
                    <h4><a name="big_memory"></a>2.8.2.4. <span class="title">大内存机器的配置</span></h4>
                  </div>
                </div>
              </div>
              <p>Hbase有一个合理的保守的配置，这样可以运作在所有的机器上。如果你有台大内存的集群-Hbase有8G或者更大的heap,接下来的配置可能会帮助你. TODO.</p>
            </div>
            <div title="2.8.2.5. Compression">
              <div>
                <div>
                  <div>
                    <h4><a name="config.compression"></a>2.8.2.5. 压缩</h4>
                  </div>
                </div>
              </div>
              <p>You should consider enabling ColumnFamily compression. There are several options that are near-frictionless and in most all cases boost performance by reducing the size of StoreFiles and thus reducing I/O.</p>
<p>See <a href="book.htm#compression" title="Appendix C. Compression In HBase">Appendix C, <em>Compression In HBase</em></a> for more information.</p>
            </div>
            <div title="2.8.2.6. Bigger Regions">
              <div>
                <div>
                  <div>
                    <h4><a name="bigger.regions"></a>2.8.2.6. 较大 Regions</h4>
                  </div>
                </div>
              </div>
              <p>更大的Region可以使你集群上的Region的总数量较少。
        一般来言，更少的Region可以使你的集群运行更加流畅。(你可以自己随时手工将大Region切割，这样单个热点Region就会被分布在集群的更多节点上)。</p>
              <p>较少的Region较好。一般每个RegionServer在20到小几百之间。 调整Region大小以适合该数字。</p>
              <p>&nbsp;</p>
              <p> 0.90.x 版本, 默认情况下单个Region是256MB。Region 大小的上界是 4Gb.  0.92.x 版本, 由于 HFile v2 已经将Region大小支持得大很多， (如, 20Gb).</p>
              <p>You may need to experiment with this setting based on your hardware configuration and application needs.</p>
              <p>可以调整<code class="filename">hbase-site.xml</code>中的 <code class="code">hbase.hregion.max.filesize</code>属性. RegionSize 也可以基于每个表设置：  <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>.</p>
            </div>
            <div title="2.8.2.7. Managed Splitting">
              <div>
                <div>
                  <div>
                    <h4><a name="disable.splitting"></a>2.8.2.7. <span class="title">管理</span> Splitting</h4>
                  </div>
                </div>
              </div>
              <p>除了让Hbase自动切割你的Region,你也可以手动切割。 <sup>[<a name="d613e1974" href="book.htm#ftn.d613e1974" class="footnote">12</a>]</sup> 随着数据量的增大，splite会被持续执行。如果你需要知道你现在有几个region,比如长时间的debug或者做调优，你需要手动切割。通过跟踪日志来了解region级的问题是很难的，因为他在不停的切割和重命名。data offlineing bug和未知量的region会让你没有办法。如果一个 <code class="classname">HLog</code> 或者 <code class="classname">StoreFile</code>由于一个奇怪的bug，Hbase没有执行它。等到一天之后，你才发现这个问题，你可以确保现在的regions和那个时候的一样，这样你就可以restore或者replay这些数据。你还可以调优你的合并算法。如果数据是均匀的，随着数据增长，很容易导致split / compaction疯狂的运行。因为所有的region都是差不多大的。用手的切割，你就可以交错执行定时的合并和切割操作，降低IO负载。 </p>
              <p> 为什么我关闭自动split呢？因为自动的splite是配置文件中的 <code class="code">hbase.hregion.max.filesize</code>决定的. 你把它设置成I<code class="varname">Long.MAX_VALUE</code>是不推荐的做法，要是你忘记了手工切割怎么办.推荐的做法是设置成100GB，一旦到达这样的值，至少需要一个小时执行 major compactions。 </p>
              <p>那什么是最佳的在pre-splite regions的数量呢。这个决定于你的应用程序了。你可以先从低的开始，比如每个server10个pre-splite regions.然后花时间观察数据增长。有太少的region至少比出错好，你可以之后再rolling split.一个更复杂的答案是这个值是取决于你的region中的最大的storefile。随着数据的增大，这个也会跟着增大。 你可以当这个文件足够大的时候，用一个定时的操作使用<code class="classname">Store</code>的合并选择算法(compact selection algorithm)来仅合并这一个HStore。如果你不这样做，这个算法会启动一个 major compactions，很多region会受到影响，你的集群会疯狂的运行。需要注意的是，这样的疯狂合并操作是数据增长造成的，而不是手动分割操作决定的。 </p>
              <p> 如果你 pre-split 导致 regions 很小,你可以通过配置<code class="varname">HConstants.MAJOR_COMPACTION_PERIOD</code>把你的major compaction参数调大 </p>
              <p>如果你的数据变得太大，可以使用<code class="classname">org.apache.hadoop.hbase.util.RegionSplitter</code> 脚本来执行针对全部集群的一个网络IO安全的rolling split操作。 </p>
<p>&nbsp;</p>
            </div>
            <div title="2.8.2.8. Managed Compactions">
              <div>
                <div>
                  <div>
                    <h4><a name="managed.compactions"></a>2.8.2.8. <span class="title">管理</span> Compactions</h4>
                  </div>
                </div>
              </div>
              <p>A common administrative technique is to manage major compactions manually, rather than letting HBase do it. By default, HConstants.MAJOR_COMPACTION_PERIOD is one day and major compactions may kick in when you least desire it - especially on a busy system. To turn off automatic major compactions set the value to0.</p>
              <p>It is important to stress that major compactions are absolutely necessary for StoreFile cleanup, the only variant is when they occur. They can be administered through the HBase shell, or via <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin</a>.</p>
              <p>For more information about compactions and the compaction file selection process, see <a href="book.htm#compaction" title="9.7.5.5. Compaction">Section 9.7.5.5, “Compaction”</a></p>
            </div>
            <div title="2.8.2.9. Speculative Execution">
              <div>
                <div>
                  <div>
                    <h4><a name="spec.ex"></a>2.8.2.9. Speculative Execution</h4>
                  </div>
                </div>
              </div>
              <p>Speculative Execution of MapReduce tasks is on by default, and for HBase clusters it is generally advised to turn off Speculative Execution at a system-level unless you need it for a specific case, where it can be configured per-job. Set the properties mapred.map.tasks.speculative.execution andmapred.reduce.tasks.speculative.execution to false.</p>
            </div>
          </div>
          <div title="2.8.3. Other Configurations">
            <div>
              <div>
                <div>
                  <h3><a name="other_configuration"></a>2.8.3. Other Configurations</h3>
                </div>
              </div>
            </div>
            <div title="2.8.3.1. Balancer">
              <div>
                <div>
                  <div>
                    <h4><a name="balancer_config"></a>2.8.3.1. Balancer</h4>
                  </div>
                </div>
              </div>
              <p>The balancer is periodic operation run on the master to redistribute regions on the cluster. It is configured via hbase.balancer.period and defaults to 300000 (5 minutes).</p>
              <p>See <a href="book.htm#master.processes.loadbalancer" title="9.5.4.1. LoadBalancer">Section 9.5.4.1, “LoadBalancer”</a> for more information on the LoadBalancer.</p>
            </div>
            <div title="2.8.3.2. Disabling Blockcache">
              <div>
                <div>
                  <div>
                    <h4><a name="disabling.blockcache"></a>2.8.3.2. Disabling Blockcache</h4>
                  </div>
                </div>
              </div>
              <p>Do not turn off block cache (You'd do it by setting hbase.block.cache.size to zero). Currently we do not do well if you do this because the regionserver will spend all its time loading hfile indices over and over again. If your working set it such that block cache does you no good, at least size the block cache such that hfile indices will stay up in the cache (you can get a rough idea on the size you need by surveying regionserver UIs; you'll see index block size accounted near the top of the webpage).</p>
            </div>
          </div>
          <div><br>
            <hr width="100" align="left">
            <div>
              <p>[<a id="ftn.d1934e2739" href="book.htm#d1934e2739">14</a>] What follows is taken from the javadoc at the head of the org.apache.hadoop.hbase.util.RegionSplitter tool added to HBase post-0.90.0 release.</p>
            </div>
          </div>
          <div>
            <div>
              <div>
                <h2><a name="config.bloom" id="config.bloom"></a>2.9. Bloom Filter Configuration</h2>
              </div>
            </div>
          </div>
          <div title="2.9.1. io.hfile.bloom.enabled global kill switch">
            <div>
              <div>
                <div>
                  <h3><a name="d1934e2830"></a>2.9.1. io.hfile.bloom.enabled global kill switch</h3>
                </div>
              </div>
            </div>
            <p>io.hfile.bloom.enabled in Configuration serves as the kill switch in case something goes wrong. Default = true.</p>
          </div>
          <div title="2.9.2. io.hfile.bloom.error.rate">
            <div>
              <div>
                <div>
                  <h3><a name="d1934e2845"></a>2.9.2. io.hfile.bloom.error.rate</h3>
                </div>
              </div>
            </div>
            <p>io.hfile.bloom.error.rate = average false positive rate. Default = 1%. Decrease rate by ½ (e.g. to .5%) == +1 bit per bloom entry.</p>
          </div>
          <div title="2.9.3. io.hfile.bloom.max.fold">
            <div>
              <div>
                <div>
                  <h3><a name="d1934e2853"></a>2.9.3. io.hfile.bloom.max.fold</h3>
                </div>
              </div>
            </div>
            <p>io.hfile.bloom.max.fold = guaranteed minimum fold rate. Most people should leave this alone. Default = 7, or can collapse to at least 1/128th of original size. See the <em>Development Process</em> section of the document <a href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters in HBase</a> for more on what this option means.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="section" title="3.6. 推荐的配置">
    <div class="titlepage">
      <div>
        <div></div>
      </div>
    </div>
    <div class="section" title="3.6.6. 管理 Splitting"> </div>
  </div>
  <div class="section" title="3.7. 连接Hbase集群的客户端配置和依赖">
    <div class="titlepage">
      <div>
        <div></div>
      </div>
    </div>
  </div>
  <div class="footnotes"> </div></div></div></div></div></div></div><div class="footnotes"></div></div><div class="chapter" title="Chapter 2. 升级"><div class="titlepage"><div><div>
  <h2 class="title"><a name="upgrading"></a>Chapter&nbsp;3.&nbsp;升级</h2></div></div></div><div class="toc"></div>
  <p>
        参见 <a class="xref" href="book.htm#requirements" title="2.1. 需要的软件">Section&nbsp;2, “配置”</a>, 需要特别注意有关Hadoop 版本的信息.
    </p><div class="section" title="2.1. 从HBase 0.20.x or 0.89.x 升级到 HBase 0.90.x"><div class="titlepage"><div><div>
      <h2 class="title" style="clear: both"><a name="upgrade0.90"></a>3.1.&nbsp;从HBase 0.20.x or 0.89.x 升级到 HBase 0.90.x </h2></div></div></div><p>0.90.x 版本的HBase可以在
              HBase 0.20.x 或者 HBase 0.89.x的数据上启动. 不需要转换数据文件， 
              HBase 0.89.x 和 0.90.x 的region目录名是不一样的 -- 老版本用md5 hash 而不是jenkins hash 来命名region-- 这就意味着，一旦启动，再也不能回退到 HBase 0.20.x.
          </p>
    <p>
             在升级的时候，一定要将<code class="filename">hbase-default.xml</code> 从你的
              <code class="filename">conf</code>目录删掉。
             0.20.x 版本的配置对于 0.90.x HBase不是最佳的. 
             <code class="filename">hbase-default.xml</code> 现在已经被打包在
             HBase jar 里面了.  如果你想看看这个文件内容，你可以在src目录下
             <code class="filename">src/main/resources/hbase-default.xml</code> 或者在
              <a class="xref" href="book.htm#hbase_default_configurations" title="3.1.1. HBase 默认配置">Section&nbsp;2.6.1.1, “HBase 默认配置”</a>看到.
          </p><p>
            最后，如果从0.20.x升级，需要在shell里检查
            <code class="varname">.META.</code> schema .  过去，我们推荐用户使用16KB的
            <code class="varname">MEMSTORE_FLUSHSIZE</code>.
            在shell中运行 <code class="code">hbase&gt; scan '-ROOT-'</code>. 会显示当前的<code class="varname">.META.</code> schema.  检查
            <code class="varname">MEMSTORE_FLUSHSIZE</code> 的大小. 看看是不是 16KB (16384)? 如果是的话，你需要修改它(默认的值是 64MB (67108864)) 
            运行脚本 <code class="filename">bin/set_meta_memstore_size.rb</code>.
            这个脚本会修改 <code class="varname">.META.</code> schema.
            如果不运行的话，集群会比较慢<sup>[<a name="d613e1033" href="book.htm#ftn.d613e1033" class="footnote">9</a>]</sup>
            .

          </p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a name="ftn.d613e1033" href="book.htm#d613e1033" class="para">9</a>] </sup>
            参见 <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3499" target="_top">HBASE-3499 Users upgrading to 0.90.0 need to have their .META. table updated with the right MEMSTORE_SIZE</a>
            </p></div></div></div><div class="chapter" title="Chapter 3. 配置"><div class="titlepage"><div><div>
              <div title="3.2. Upgrading from 0.90.x to 0.92.x">
                <div>
                  <div>
                    <div>
                      <h2>3.2. Upgrading from 0.90.x to 0.92.x</h2>
                    </div>
                    <div>
                      <h3>Upgrade Guide</h3>
                    </div>
                  </div>
                </div>
                <p>You will find that 0.92.0 runs a little differently to 0.90.x releases. Here are a few things to watch out for upgrading from 0.90.x to 0.92.0.</p>
                <div title="tl;dr">
                  <h3>&nbsp;</h3>
                  <p>If you've not patience, here are the important things to know upgrading.</p>
                  <div>
                    <ol type="1">
                      <li>Once you upgrade, you can’t go back.</li>
                      <li>MSLAB is on by default. Watch that heap usage if you have a lot of regions.</li>
                      <li>Distributed splitting is on by defaul. It should make region server failover faster.</li>
                      <li>There’s a separate tarball for security.</li>
                      <li>If -XX:MaxDirectMemorySize is set in your hbase-env.sh, it’s going to enable the experimental off-heap cache (You may not want this).</li>
                    </ol>
                  </div>
                </div>
                <div title="3.2.1. You can’t go back!">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e2951"></a>3.2.1. You can’t go back!</h3>
                      </div>
                    </div>
                  </div>
                  <p>To move to 0.92.0, all you need to do is shutdown your cluster, replace your hbase 0.90.x with hbase 0.92.0 binaries (be sure you clear out all 0.90.x instances) and restart (You cannot do a rolling restart from 0.90.x to 0.92.x -- you must restart). On startup, the .META. table content is rewritten removing the table schema from the info:regioninfo column. Also, any flushes done post first startup will write out data in the new 0.92.0 file format, <a href="http://hbase.apache.org/book.html#hfilev2" target="_top">HFile V2</a>. This means you cannot go back to 0.90.x once you’ve started HBase 0.92.0 over your HBase data directory.</p>
                </div>
                <div title="3.2.2. MSLAB is ON by default">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e2965"></a>3.2.2. MSLAB is ON by default</h3>
                      </div>
                    </div>
                  </div>
                  <p>In 0.92.0, the <a href="http://hbase.apache.org/book.html#hbase.hregion.memstore.mslab.enabled" target="_top">hbase.hregion.memstore.mslab.enabled</a> flag is set to true (See <a href="book.htm#mslab">Section 11.3.1.1, “Long GC pauses”</a>). In 0.90.x it was false. When it is enabled, memstores will step allocate memory in MSLAB 2MB chunks even if the memstore has zero or just a few small elements. This is fine usually but if you had lots of regions per regionserver in a 0.90.x cluster (and MSLAB was off), you may find yourself OOME'ing on upgrade because the thousands of regions * number of column families * 2MB MSLAB (at a minimum) puts your heap over the top. Set hbase.hregion.memstore.mslab.enabled to false or set the MSLAB size down from 2MB by setting hbase.hregion.memstore.mslab.chunksize to something less.</p>
                </div>
                <div title="3.2.3. Distributed splitting is on by default">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e2990"></a>3.2.3. Distributed splitting is on by default</h3>
                      </div>
                    </div>
                  </div>
                  <p>Previous, WAL logs on crash were split by the Master alone. In 0.92.0, log splitting is done by the cluster (See See “HBASE-1364 [performance] Distributed splitting of regionserver commit logs”). This should cut down significantly on the amount of time it takes splitting logs and getting regions back online again.</p>
                </div>
                <div title="3.2.4. Memory accounting is different now">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e2995"></a>3.2.4. Memory accounting is different now</h3>
                      </div>
                    </div>
                  </div>
                  <p>In 0.92.0, <a href="book.htm#hfilev2" title="Appendix E. HFile format version 2">Appendix E, <em>HFile format version 2</em></a> indices and bloom filters take up residence in the same LRU used caching blocks that come from the filesystem. In 0.90.x, the HFile v1 indices lived outside of the LRU so they took up space even if the index was on a ‘cold’ file, one that wasn’t being actively used. With the indices now in the LRU, you may find you have less space for block caching. Adjust your block cache accordingly. See the <a href="book.htm#regionserver.arch.html#block.cache" title="9.6.4. Block Cache">Section 9.6.4, “Block Cache”</a> for more detail. The block size default size has been changed in 0.92.0 from 0.2 (20 percent of heap) to 0.25.</p>
                </div>
                <div title="3.2.5. On the Hadoop version to use">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3004"></a>3.2.5. On the Hadoop version to use</h3>
                      </div>
                    </div>
                  </div>
                  <p>Run 0.92.0 on Hadoop 1.0.x (or CDH3u3 when it ships). The performance benefits are worth making the move. Otherwise, our Hadoop prescription is as it has been; you need an Hadoop that supports a working sync. See <a href="book.htm#hadoop" title="2.3. Hadoop">Section 2.3, “Hadoop”</a>.</p>
                  <p>If running on Hadoop 1.0.x (or CDH3u3), enable local read. See <a href="http://files.meetup.com/1350427/hug_ebay_jdcryans.pdf" target="_top">Practical Caching</a> presentation for ruminations on the performance benefits ‘going local’ (and for how to enable local reads).</p>
                </div>
                <div title="3.2.6. HBase 0.92.0 ships with ZooKeeper 3.4.2">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3016"></a>3.2.6. HBase 0.92.0 ships with ZooKeeper 3.4.2</h3>
                      </div>
                    </div>
                  </div>
                  <p>If you can, upgrade your zookeeper. If you can’t, 3.4.2 clients should work against 3.3.X ensembles (HBase makes use of 3.4.2 API).</p>
                </div>
                <div title="3.2.7. Online alter is off by default">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3021"></a>3.2.7. Online alter is off by default</h3>
                      </div>
                    </div>
                  </div>
                  <p>In 0.92.0, we’ve added an experimental online schema alter facility (See <a href="book.htm#hbase.online.schema.update.enable" title="hbase.online.schema.update.enable">hbase.online.schema.update.enable</a>). Its off by default. Enable it at your own risk. Online alter and splitting tables do not play well together so be sure your cluster quiescent using this feature (for now).</p>
                </div>
                <div title="3.2.8. WebUI">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3028"></a>3.2.8. WebUI</h3>
                      </div>
                    </div>
                  </div>
                  <p>The webui has had a few additions made in 0.92.0. It now shows a list of the regions currently transitioning, recent compactions/flushes, and a process list of running processes (usually empty if all is well and requests are being handled promptly). Other additions including requests by region, a debugging servlet dump, etc.</p>
                </div>
                <div title="3.2.9. Security tarball">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3033"></a>3.2.9. Security tarball</h3>
                      </div>
                    </div>
                  </div>
                  <p>We now ship with two tarballs; secure and insecure HBase. Documentation on how to setup a secure HBase is on the way.</p>
                </div>
                <div title="3.2.10. Experimental off-heap cache">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3038"></a>3.2.10. Experimental off-heap cache</h3>
                      </div>
                    </div>
                  </div>
                  <p>A new cache was contributed to 0.92.0 to act as a solution between using the “on-heap” cache which is the current LRU cache the region servers have and the operating system cache which is out of our control. To enable, set “-XX:MaxDirectMemorySize” in hbase-env.sh to the value for maximum direct memory size and specify hbase.offheapcache.percentage in hbase-site.xml with the percentage that you want to dedicate to off-heap cache. This should only be set for servers and not for clients. Use at your own risk. See this blog post for additional information on this new experimental feature: http://www.cloudera.com/blog/2012/01/caching-in-hbase-slabcache/</p>
                </div>
                <div title="3.2.11. Changes in HBase replication">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3043"></a>3.2.11. Changes in HBase replication</h3>
                      </div>
                    </div>
                  </div>
                  <p>0.92.0 adds two new features: multi-slave and multi-master replication. The way to enable this is the same as adding a new peer, so in order to have multi-master you would just run add_peer for each cluster that acts as a master to the other slave clusters. Collisions are handled at the timestamp level which may or may not be what you want, this needs to be evaluated on a per use case basis. Replication is still experimental in 0.92 and is disabled by default, run it at your own risk.</p>
                </div>
                <div title="3.2.12. RegionServer now aborts if OOME">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3048"></a>3.2.12. RegionServer now aborts if OOME</h3>
                      </div>
                    </div>
                  </div>
                  <p>If an OOME, we now have the JVM kill -9 the regionserver process so it goes down fast. Previous, a RegionServer might stick around after incurring an OOME limping along in some wounded state. To disable this facility, and recommend you leave it in place, you’d need to edit the bin/hbase file. Look for the addition of the -XX:OnOutOfMemoryError="kill -9 %p" arguments (See [HBASE-4769] - ‘Abort RegionServer Immediately on OOME’)</p>
                </div>
                <div title="3.2.13. HFile V2 and the “Bigger, Fewer” Tendency">
                  <div>
                    <div>
                      <div>
                        <h3><a name="d1934e3053"></a>3.2.13. HFile V2 and the “Bigger, Fewer” Tendency</h3>
                      </div>
                    </div>
                  </div>
                  <p>0.92.0 stores data in a new format, <a href="book.htm#hfilev2" title="Appendix E. HFile format version 2">Appendix E, <em>HFile format version 2</em></a>. As HBase runs, it will move all your data from HFile v1 to HFile v2 format. This auto-migration will run in the background as flushes and compactions run. HFile V2 allows HBase run with larger regions/files. In fact, we encourage that all HBasers going forward tend toward Facebook axiom #1, run with larger, fewer regions. If you have lots of regions now -- more than 100s per host -- you should look into setting your region size up after you move to 0.92.0 (In 0.92.0, default size is not 1G, up from 256M), and then running online merge tool (See “HBASE-1621 merge tool should work on online cluster, but disabled table”).</p>
                </div>
              </div>
              <br>
            </div></div></div><div class="footnotes"></div></div><div class="chapter" title="Chapter 4. The HBase Shell"><div class="titlepage"><div><div><h2 class="title"><a name="shell"></a>Chapter&nbsp;4.&nbsp;The HBase Shell</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="book.htm#scripting">4.1. 使用脚本</a></span></dt><dt><span class="section"><a href="book.htm#shell_tricks">4.2. Shell 技巧</a></span></dt><dd><dl><dt><span class="section"><a href="book.htm#d613e2127">4.2.1. <code class="filename">irbrc</code></a></span></dt><dt><span class="section"><a href="book.htm#d613e2145">4.2.2. LOG 时间转换</a></span></dt><dt><span class="section"><a href="book.htm#d613e2163">4.2.3. Debug</a></span></dt></dl></dd></dl></div><p>
        Hbase Shell is 在<a class="link" href="http://jruby.org/" target="_top">(J)Ruby</a>的IRB的基础上加上了HBase的命令。任何你可以在IRB里做的事情都可在在Hbase Shell中做。
      </p><p>你可以这样来运行HBase Shell:
        </p><pre class="programlisting">$ ./bin/hbase shell</pre><p>
        </p><p>输入 <span class="command"><strong>help</strong></span> 就会返回Shell的命令列表和选项。可以看看在Help文档尾部的关于如何输入变量和选项。尤其要注意的是表名，行，列名必须要加引号。</p><p>参见 <a class="xref" href="book.htm#shell_exercises" title="1.2.3. Shell 练习">Section&nbsp;1.2.3, “Shell 练习”</a>可以看到Shell的基本使用例子。
            </p><div class="section" title="4.1. 使用脚本"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="scripting"></a>4.1.&nbsp;使用脚本</h2></div></div></div><p>如果要使用脚本，可以看Hbase的<code class="filename">bin</code> 目录.在里面找到后缀为 <code class="filename">*.rb</code>的脚本.要想运行这个脚本，要这样
            </p><pre class="programlisting">$ ./bin/hbase org.jruby.Main PATH_TO_SCRIPT</pre><p>就可以了
        </p></div><div class="section" title="4.2. Shell 技巧"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="shell_tricks"></a>4.2.&nbsp;Shell 技巧</h2></div></div></div><div class="section" title="4.2.1. irbrc"><div class="titlepage"><div><div><h3 class="title"><a name="d613e2127"></a>4.2.1.&nbsp;<code class="filename">irbrc</code></h3></div></div></div><p>可以在你自己的Home目录下创建一个<code class="filename">.irbrc</code>文件. 在这个文件里加入自定义的命令。有一个有用的命令就是记录命令历史，这样你就可以把你的命令保存起来。
                    </p><pre class="programlisting">                        $ more .irbrc
                        require 'irb/ext/save-history'
                        IRB.conf[:SAVE_HISTORY] = 100
                        IRB.conf[:HISTORY_FILE] = "#{ENV['HOME']}/.irb-save-history"</pre><p>
                可以参见 <span class="application">ruby</span> 关于
                <code class="filename">.irbrc</code> 的文档来学习更多的关于IRB的配置方法。
                </p></div><div class="section" title="4.2.2. LOG 时间转换"><div class="titlepage"><div><div><h3 class="title"><a name="d613e2145"></a>4.2.2.&nbsp;LOG 时间转换</h3></div></div></div><p>
                可以将日期'08/08/16 20:56:29'从hbase log 转换成一个 timestamp, 操作如下:
                </p><pre class="programlisting">                    hbase(main):021:0&gt; import java.text.SimpleDateFormat
                    hbase(main):022:0&gt; import java.text.ParsePosition
                    hbase(main):023:0&gt; SimpleDateFormat.new("yy/MM/dd HH:mm:ss").parse("08/08/16 20:56:29", ParsePosition.new(0)).getTime() =&gt; 1218920189000</pre><p>
            </p><p>
                也可以逆过来操作。
                </p><pre class="programlisting">                    hbase(main):021:0&gt; import java.util.Date
                    hbase(main):022:0&gt; Date.new(1218920189000).toString() =&gt; "Sat Aug 16 20:56:29 UTC 2008"</pre><p>
            </p><p>
			    要想把日期格式和Hbase log格式完全相同，可以参见文档
                <a class="link" href="http://download.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html" target="_top">SimpleDateFormat</a>.
            </p></div><div class="section" title="4.2.3. Debug"><div class="titlepage"><div><div><h3 class="title"><a name="d613e2163"></a>4.2.3.&nbsp;Debug</h3></div></div></div><div class="section" title="4.2.3.1. Shell 切换成debug 模式"><div class="titlepage"><div><div><h4 class="title"><a name="d613e2166"></a>4.2.3.1.&nbsp;Shell 切换成debug 模式</h4></div></div></div><p>你可以将shell切换成debug模式。这样可以看到更多的信息。
                    -- 例如可以看到命令异常的stack trace:
                    </p><pre class="programlisting">hbase&gt; debug &lt;RETURN&gt;</pre><p>
                 </p></div><div class="section" title="4.2.3.2. DEBUG log level"><div class="titlepage"><div><div><h4 class="title"><a name="d613e2174"></a>4.2.3.2.&nbsp;DEBUG log level</h4></div></div></div><p>想要在shell中看到 DEBUG 级别的 logging ，可以在启动的时候加上  <span class="command"><strong>-d</strong></span> 参数.
                    </p><pre class="programlisting">$ ./bin/hbase shell -d</pre>
                    <div class="titlepage">
                      <div>
                        <div>
                          <h2 class="title"><a name="datamodel"></a>Chapter&nbsp;5.&nbsp;数据模型</h2>
                        </div>
                      </div>
                    </div>
                    <div class="toc">
                      <p><b>Table of Contents</b></p>
                      <dl>
                        <dt><a href="book.htm#conceptual.view">5.1. 概念视图</a></dt>
                        <dt><a href="book.htm#physical.view">5.2. 物理视图</a></dt>
                        <dt><a href="book.htm#table">5.3. 表</a></dt>
                        <dt><a href="book.htm#row">5.4. 行</a></dt>
                        <dt><a href="book.htm#columnfamily">5.5. Column Family</a></dt>
                        <dt><a href="book.htm#cells">5.6. Cells</a></dt>
                        <dt><a href="book.htm#versions">5.7. 版本</a></dt>
                        <dd>
                          <dl>
                            <dt><a href="book.htm#versions.ops">5.7.1. Hbase的操作(包含版本操作)</a></dt>
                            <dt><a href="book.htm#d613e2965">5.7.2. 现有的限制</a></dt>
                          </dl>
                        </dd>
                      </dl>
                    </div>
                    <p>简单来说，应用程序是以表的方式在Hbase存储数据的。表是由行和列构成的，所以的列是从属于某一个column family的。行和列的交叉点称之为cell,cell是版本化的。cell的内容是不可分割的字节数组。 </p>
                    <p>表的row key也是一段字节数组，所以任何东西都可以保存进去，不论是字符串或者数字。Hbase的表是按key排序的，排序方式之针对字节的。所以的表都必须要有主键-key. </p>
                    <div class="section" title="11.1. 概念视图">
                      <div class="titlepage">
                        <div>
                          <div>
                            <h2 class="title" style="clear: both"><a name="conceptual.view"></a>5.1.&nbsp;概念视图</h2>
                          </div>
                        </div>
                      </div>
                      <p> 下面
                        是根据<a class="link" href="http://labs.google.com/papers/bigtable.html" target="_top">BigTable</a> 论文稍加修改的例子。
                        有一个名为<code class="varname">webtable</code>的表，包含两个column family<code class="varname">：contents</code>和<code class="varname">anchor</code>.在这个例子里面，<code class="varname">anchor</code>有两个列 (<code class="varname">anchor:cssnsi.com</code>, <code class="varname">anchor:my.look.ca</code>)，<code class="varname">contents</code>仅有一列(<code class="varname">contents:html</code>) </p>
                      <div class="note" title="列名" style="margin-left: 0.5in; margin-right: 0.5in;">
                        <h3 class="title">列名</h3>
                        <p> 一个列名是有它的column family前缀和<span class="emphasis"><em>qualifier</em></span>连接而成。例如列<span class="emphasis"><em>contents:html</em></span>是column family <code class="varname">contents</code>加冒号(<code class="literal">:</code>)加 <span class="emphasis"><em>qualifier</em></span> <code class="varname">html</code>组成的。 </p>
                      </div>
                      <p></p>
                      <div class="table"><a name="d1934e3221" id="d1934e3221"></a>
                        <p class="title"><b>Table&nbsp;5.1.&nbsp;表 <code class="varname">webtable</code></b></p>
                        <div class="table-contents">
                          <table summary="表 webtable" border="1">
                            <colgroup>
                              <col align="left">
                              <col align="left">
                              <col align="left">
                              <col align="left">
                            </colgroup>
                            <thead>
                              <tr>
                                <th align="left">Row Key</th>
                                <th align="left">Time Stamp</th>
                                <th align="left">ColumnFamily <code class="varname">contents</code></th>
                                <th align="left">ColumnFamily <code class="varname">anchor</code></th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t9</td>
                                <td align="left">&nbsp;</td>
                                <td align="left"><code class="varname">anchor:cnnsi.com</code> = "CNN"</td>
                              </tr>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t8</td>
                                <td align="left">&nbsp;</td>
                                <td align="left"><code class="varname">anchor:my.look.ca</code> = "CNN.com"</td>
                              </tr>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t6</td>
                                <td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td>
                                <td align="left">&nbsp;</td>
                              </tr>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t5</td>
                                <td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td>
                                <td align="left">&nbsp;</td>
                              </tr>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t3</td>
                                <td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td>
                                <td align="left">&nbsp;</td>
                              </tr>
                            </tbody>
                          </table>
                        </div>
                      </div>
                      <p><br class="table-break">
                      </p>
                    </div>
                   <div class="section" title="11.2. 物理视图">
                      <div class="titlepage">
                        <div>
                          <div>
                            <h2 class="title" style="clear: both"><a name="physical.view"></a>5.2.&nbsp;物理视图</h2>
                          </div>
                        </div>
                      </div>
                      <p> 尽管在概念视图里，表可以被看成是一个稀疏的行的集合。但在物理上，它的是区分column family 存储的。新的columns可以不经过声明直接加入一个column family. </p>
                      <div class="table"><a name="d1934e3305" id="d1934e3305"></a>
                        <p class="title"><b>Table&nbsp;5.2.&nbsp;ColumnFamily <code class="varname">anchor</code></b></p>
                        <div class="table-contents">
                          <table summary="ColumnFamily anchor" border="1">
                            <colgroup>
                              <col align="left">
                              <col align="left">
                              <col align="left">
                            </colgroup>
                            <thead>
                              <tr>
                                <th align="left">Row Key</th>
                                <th align="left">Time Stamp</th>
                                <th align="left">Column Family <code class="varname">anchor</code></th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t9</td>
                                <td align="left"><code class="varname">anchor:cnnsi.com</code> = "CNN"</td>
                              </tr>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t8</td>
                                <td align="left"><code class="varname">anchor:my.look.ca</code> = "CNN.com"</td>
                              </tr>
                            </tbody>
                          </table>
                        </div>
                      </div>
                      <p><br class="table-break">
                      </p>
                     <div class="table"><a name="d1934e3344" id="d1934e3344"></a>
                       <p class="title"><b>Table&nbsp;5.3.&nbsp;ColumnFamily <code class="varname">contents</code></b></p>
                        <div class="table-contents">
                          <table summary="ColumnFamily contents" border="1">
                            <colgroup>
                              <col align="left">
                              <col align="left">
                              <col align="left">
                            </colgroup>
                            <thead>
                              <tr>
                                <th align="left">Row Key</th>
                                <th align="left">Time Stamp</th>
                                <th align="left">ColumnFamily "contents:"</th>
                              </tr>
                            </thead>
                            <tbody>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t6</td>
                                <td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td>
                              </tr>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t5</td>
                                <td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td>
                              </tr>
                              <tr>
                                <td align="left">"com.cnn.www"</td>
                                <td align="left">t3</td>
                                <td align="left"><code class="varname">contents:html</code> = "&lt;html&gt;..."</td>
                              </tr>
                            </tbody>
                          </table>
                        </div>
                     </div>
                     <p><br class="table-break">
                       值得注意的是在上面的概念视图中空白cell在物理上是不存储的，因为根本没有必要存储。因此若一个请求为要获取<code class="literal">t8</code>时间的<code class="varname">contents:html</code>，他的结果就是空。相似的，若请求为获取<code class="literal">t9</code>时间的<code class="varname">anchor:my.look.ca</code>，结果也是空。但是，如果不指明时间，将会返回最新时间的行，每个最新的都会返回。例如，如果请求为获取row key为"com.cnn.www"，没有指明时间戳的话，活动的结果是<code class="literal">t6</code>下的contents:html，<code class="literal">t9</code>下的<code class="varname">anchor:cnnsi.com</code>和<code class="literal">t8</code>下<code class="varname">anchor:my.look.ca</code>。 </p>
                      <p> For more information about the internals of how HBase stores data, see <a href="book.htm#regions.arch" title="9.7. Regions">Section 9.7, “Regions”</a>. </p>
                    </div>
                    <div class="section" title="11.3. 表">
                      <div class="titlepage">
                        <div>
                          <div>
                            <h2 class="title" style="clear: both"><a name="table"></a>5.3.&nbsp;表</h2>
                          </div>
                        </div>
                      </div>
                      <p> 表是在schema声明的时候定义的。 </p>
                    </div>
                    <div class="section" title="11.4. 行">
                      <div class="titlepage">
                        <div>
                          <div>
                            <h2 class="title" style="clear: both"><a name="row"></a>5.4.&nbsp;行</h2>
                          </div>
                        </div>
                      </div>
                      <p>row key是不可分割的字节数组。行是按字典排序由低到高存储在表中的。一个空的数组是用来标识表空间的起始或者结尾。</p>
                    </div>
                    <div class="section" title="11.5. Column Family">
                      <div class="titlepage">
                        <div>
                          <div>
                            <h2 class="title" style="clear: both"><a name="columnfamily"></a>5.5.&nbsp;Column Family<a class="indexterm" name="d613e2767"></a></h2>
                          </div>
                        </div>
                      </div>
                      <p> 在Hbase是<span class="emphasis"><em>column family</em></span>一些列的集合。一个column family所有列成员是有着相同的前缀。比如，列<span class="emphasis"><em>courses:history</em></span> 和 <span class="emphasis"><em>courses:math</em></span>都是 column family <span class="emphasis"><em>courses</em></span>的成员.冒号(:)是column family的分隔符，用来区分前缀和列名。column 前缀必须是可打印的字符，剩下的部分(称为qualify),可以又任意字节数组组成。column family必须在表建立的时候声明。column就不需要了，随时可以新建。 </p>
                      <p>在物理上，一个的column family成员在文件系统上都是存储在一起。因为存储优化都是针对column family级别的，这就意味着，一个colimn family的所有成员的是用相同的方式访问的。</p>
                      <p></p>
                    </div>
                    <div class="section" title="11.6. Cells">
                      <div class="titlepage">
                        <div>
                          <div>
                            <h2 class="title" style="clear: both"><a name="cells"></a>5.6.&nbsp;Cells<a class="indexterm" name="d613e2790"></a></h2>
                          </div>
                        </div>
                      </div>
                      <p>A <span class="emphasis"><em>{row, column, version} </em></span>元组就是一个Hbase中的一个 <code class="literal">cell</code>。Cell的内容是不可分割的字节数组。</p>
                    </div>
                    <div class="section" title="11.7. 版本">
                      <div class="titlepage">
                        <div>
                          <div></div>
                        </div>
                      </div>
                      <div class="section" title="5.7.&nbsp;Data Model Operations"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="data_model_operations"></a>5.7.&nbsp;Data Model Operations</h2></div></div></div><p>The four primary data model operations are Get, Put, Scan, and Delete.  Operations are applied via 
                        <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a> instances.
                        </p><div class="section" title="5.7.1.&nbsp;Get"><div class="titlepage"><div><div><h3 class="title"><a name="get"></a>5.7.1.&nbsp;Get</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html" target="_top">Get</a> returns
                          attributes for a specified row.  Gets are executed via 
                          <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#get%28org.apache.hadoop.hbase.client.Get%29" target="_top">
                            HTable.get</a>.
                          </p></div><div class="section" title="5.7.2.&nbsp;Put"><div class="titlepage"><div><div><h3 class="title"><a name="put"></a>5.7.2.&nbsp;Put</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Put.html" target="_top">Put</a> either 
                            adds new rows to a table (if the key is new) or can update existing rows (if the key already exists).  Puts are executed via
                            <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#put%28org.apache.hadoop.hbase.client.Put%29" target="_top">
                              HTable.put</a> (writeBuffer) or <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#batch%28java.util.List%29" target="_top">
                                HTable.batch</a> (non-writeBuffer).  
                            </p></div><div class="section" title="5.7.3.&nbsp;Scans"><div class="titlepage"><div><div><h3 class="title"><a name="scan"></a>5.7.3.&nbsp;Scans</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a> allow
                              iteration over multiple rows for specified attributes.
                              </p><p>The following is an example of a 
                                on an HTable table instance.  Assume that a table is populated with rows with keys "row1", "row2", "row3", 
                                and then another set of rows with the keys "abc1", "abc2", and "abc3".  The following example shows how startRow and stopRow 
                                can be applied to a Scan instance to return the rows beginning with "row".        
  </p><pre class="programlisting">HTable htable = ...      // instantiate HTable
    
Scan scan = new Scan();
scan.addColumn(Bytes.toBytes("cf"),Bytes.toBytes("attr"));
scan.setStartRow( Bytes.toBytes("row"));                   // start key is inclusive
scan.setStopRow( Bytes.toBytes("row" +  (char)0));  // stop key is exclusive
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
</pre><p>
  </p></div><div class="section" title="5.7.4.&nbsp;Delete"><div class="titlepage"><div><div><h3 class="title"><a name="delete"></a>5.7.4.&nbsp;Delete</h3></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Delete.html" target="_top">Delete</a> removes
    a row from a table.  Deletes are executed via 
    <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29" target="_top">
      HTable.delete</a>.
    </p><p>HBase does not modify data in place, and so deletes are handled by creating new markers called <span class="emphasis"><em>tombstones</em></span>.
      These tombstones, along with the dead values, are cleaned up on major compactions.
      </p><p>See <a class="xref" href="versions.html#version.delete" title="5.8.1.5.&nbsp;Delete">Section&nbsp;5.8.1.5, “Delete”</a> for more information on deleting versions of columns, and see 
        <a class="xref" href="regions.arch.html#compaction" title="9.7.5.5.&nbsp;Compaction">Section&nbsp;9.7.5.5, “Compaction”</a> for more information on compactions.         
      </p></div></div>
        
                      <div class="section" title="11.7.1. Hbase的操作(包含版本操作)">
                        <div class="titlepage">
                          <div>
                            <div>
                              <div class="titlepage">
                                <div>
                                  <div>
                                    <h2 class="title" style="clear: both"><a name="versions"></a>5.8.&nbsp;版本<a class="indexterm" name="d613e2804"></a></h2>
                                  </div>
                                </div>
                              </div>
                              <p>一个 <span class="emphasis"><em>{row, column, version} </em></span>元组是Hbase中的一个<code class="literal">cell</code> .但是有可能会有很多的cell的row和column是相同的，可以使用version来区分不同的cell.</p>
                              <p>rows和column key是用字节数组表示的，version则是用一个长整型表示。这个long的值使用 <code class="code">java.util.Date.getTime()</code> 或者 <code class="code">System.currentTimeMillis()</code>产生的。这就意味着他的含义是<span class="quote">“当前时间和1970-01-01 UTC的时间差，单位毫秒。”</span></p>
                              <p>在Hbase中，版本是按倒序排列的，因此当读取这个文件的时候，最先找到的是最近的版本。</p>
                              <p>有些人不是很理解Hbase的 <code class="literal">cell</code> 意思。一个常见的问题是:</p>
                              <div class="itemizedlist">
                                <ul class="itemizedlist" type="disc">
                                  <li class="listitem">
                                    <p>如果有多个包含版本写操作同时发起，Hbase会保存全部还是会保持最新的一个？<sup>[<a name="d613e2836" href="book.htm#ftn.d613e2836" class="footnote">13</a>]</sup></p>
                                  </li>
                                  <li class="listitem">
                                    <p>可以发起包含版本的写操作，但是他们的版本顺序和操作顺序相反吗?<sup>[<a name="d613e2842" href="book.htm#ftn.d613e2842" class="footnote">14</a>]</sup></p>
                                  </li>
                                </ul>
                              </div>
                              <p>下面我们介绍下在Hbase中版本是如何工作的。<sup>[<a name="d613e2847" href="book.htm#ftn.d613e2847" class="footnote">15</a>]</sup>.</p>
                              <h3 class="title"><a name="versions.ops"></a>5.8.1.&nbsp;Hbase的操作(包含版本操作)</h3>
                            </div>
                          </div>
                        </div>
                        <p>在这一章我们来仔细看看在Hbase的各个主要操作中版本起到了什么作用。</p>
                        <div class="section" title="11.7.1.1. Get/Scan">
                          <div class="titlepage">
                            <div>
                              <div>
                                <h4 class="title"><a name="d613e2865"></a>5.8.1.1.&nbsp;Get/Scan</h4>
                              </div>
                            </div>
                          </div>
                          <p>Gets实在Scan的基础上实现的。可以详细参见下面的讨论 <a class="link" href="http://hbase.apache.org/docs/current/api/org/apache/hadoop/hbase/client/Get.html" target="_top">Get</a> 同样可以用 <a class="link" href="http://hbase.apache.org/docs/current/api/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>来描述.</p>
                          <p>默认情况下，如果你没有指定版本，当你使用<code class="literal">Get</code>操作的时候，会返回最近版本的Cell(该Cell可能是最新写入的，但不能保证)。默认的操作可以这样修改:</p>
                          <div class="itemizedlist">
                            <ul class="itemizedlist" type="disc">
                              <li class="listitem">
                                <p>如果想要返回返回两个以上的把版本,参见<a class="link" href="http://hbase.apache.org/docs/current/api/org/apache/hadoop/hbase/client/Get.html#setMaxVersions()" target="_top">Get.setMaxVersions()</a></p>
                              </li>
                              <li class="listitem">
                                <p>如果想要返回的版本不只是最近的，参见 <a class="link" href="book.htm???" target="_top">Get.setTimeRange()</a></p>
                                <p>要向查询的最新版本要小于或等于给定的这个值，这就意味着给定的'最近'的值可以是某一个时间点。可以使用0到你想要的时间来设置，还要把max versions设置为1.</p>
                              </li>
                            </ul>
                          </div>
                        </div>
                        <div class="section" title="11.7.1.2. 默认 Get 例子">
                          <div class="titlepage">
                            <div>
                              <div>
                                <h4 class="title"><a name="default_get_example"></a>5.8.1.2.&nbsp;默认 Get 例子</h4>
                              </div>
                            </div>
                          </div>
                          <p>下面的Get操作会只获得最新的一个版本。 </p>
                          <pre class="programlisting">        Get get = new Get(Bytes.toBytes("row1"));
        Result r = htable.get(get);
        byte[] b = r.getValue(Bytes.toBytes("cf"), Bytes.toBytes("attr"));  // returns current version of value          </pre>
                          <p></p>
                        </div>
                        <div class="section" title="11.7.1.3. 含有的版本的Get例子">
                          <div class="titlepage">
                            <div>
                              <div>
                                <h4 class="title"><a name="versioned_get_example"></a>5.8.1.3.&nbsp;含有的版本的Get例子</h4>
                              </div>
                            </div>
                          </div>
                          <p>下面的Get操作会获得最近的3个版本。 </p>
                          <pre class="programlisting">        Get get = new Get(Bytes.toBytes("row1"));
        get.setMaxVersions(3);  // will return last 3 versions of row
        Result r = htable.get(get);
        byte[] b = r.getValue(Bytes.toBytes("cf"), Bytes.toBytes("attr"));  // returns current version of value
        List&lt;KeyValue&gt; kv = r.getColumn(Bytes.toBytes("cf"), Bytes.toBytes("attr"));  // returns all versions of this column       
                    </pre>
                          <p></p>
                        </div>
                        <div class="section" title="11.7.1.4. Put">
                          <div class="titlepage">
                            <div>
                              <div>
                                <h4 class="title"><a name="put_operation"></a>5.8.1.4.&nbsp;Put</h4>
                              </div>
                            </div>
                          </div>
                          <p>一个Put操作会给一个<code class="literal">cell</code>,创建一个版本，默认使用当前时间戳，当然你也可以自己设置时间戳。这就意味着你可以把时间设置在过去或者未来，或者随意使用一个Long值。 </p>
                          <p>要想覆盖一个现有的值，就意味着你的row,column和版本必须完全相等。</p>
                          <div class="section" title="11.7.1.4.1. 不指明版本的例子">
                            <div class="titlepage">
                              <div>
                                <div>
                                  <h5 class="title"><a name="implicit_version_example"></a>5.8.1.4.1.&nbsp;不指明版本的例子</h5>
                                </div>
                              </div>
                            </div>
                            <p>下面的Put操作不指明版本，所以Hbase会用当前时间作为版本。 </p>
                            <pre class="programlisting">          Put put = new Put(Bytes.toBytes(row));
          put.add(Bytes.toBytes("cf"), Bytes.toBytes("attr1"), Bytes.toBytes( data));
          htable.put(put);
                    </pre>
                            <p></p>
                          </div>
                          <div class="section" title="11.7.1.4.2. 指明版本的例子">
                            <div class="titlepage">
                              <div>
                                <div>
                                  <h5 class="title"><a name="explicit_version_example"></a>5.8.1.4.2.&nbsp;指明版本的例子</h5>
                                </div>
                              </div>
                            </div>
                            <p>下面的Put操作，指明了版本。 </p>
                            <pre class="programlisting">          Put put = new Put( Bytes.toBytes(row ));
          long explicitTimeInMs = 555;  // just an example
          put.add(Bytes.toBytes("cf"), Bytes.toBytes("attr1"), explicitTimeInMs, Bytes.toBytes(data));
          htable.put(put);
                    </pre>
                            <p></p>
                          </div>
                        </div>
                        <div class="section" title="11.7.1.5. Delete">
                          <div class="titlepage">
                            <div>
                              <div>
                                <h4 class="title"><a name="d613e2936"></a>5.8.1.5.&nbsp;Delete</h4>
                              </div>
                            </div>
                          </div>
                          <p>当你进行delete操作的是，有两种方式来确定要删除的版本。</p>
                          <div class="itemizedlist">
                            <ul class="itemizedlist" type="disc">
                              <li class="listitem">
                                <p>删除所有比当前早的版本。</p>
                              </li>
                              <li class="listitem">
                                <p>删除指定的版本。</p>
                              </li>
                            </ul>
                          </div>
                          <p>一个删除操作可以删除一行，也可以是一个column family，或者仅仅删除一个column。你也可以删除指明的一个版本。若你没有指明，默认情况下是删除比当前时间早的版本。</p>
                          <p>删除操作的实现是创建一个<span class="emphasis"><em>删除标记</em></span>。例如，我们想要删除一个版本，或者默认是<code class="literal">currentTimeMillis</code>。就意味着<span class="quote">“删除比这个版本更早的所有版本”</span>.Hbase不会去改那些数据，数据不会立即从文件中删除。他使用删除标记来屏蔽掉这些值。<sup>[<a name="d613e2961" href="book.htm#ftn.d613e2961" class="footnote">16</a>]</sup>若你知道的版本比数据中的版本晚，就意味着这一行中的所有数据都会被删除。</p>
                        </div>
                      </div>
                      <div class="section" title="11.7.2. 现有的限制">
                        <div class="titlepage">
                          <div>
                            <div>
                              <h3 class="title"><a name="d613e2965"></a>5.8.2.&nbsp;现有的限制</h3>
                            </div>
                          </div>
                        </div>
                        <p>关于版本还有一些bug(或者称之为未实现的功能)，计划在下个版本实现。</p>
                        <div class="section" title="11.7.2.1. 删除标记误删Puts">
                          <div class="titlepage">
                            <div>
                              <div>
                                <h4 class="title"><a name="d613e2970"></a>5.8.2.1.&nbsp;删除标记误删Puts</h4>
                              </div>
                            </div>
                          </div>
                          <p>删除标记操作可能会标记之后put的数据。<sup>[<a name="d613e2975" href="book.htm#ftn.d613e2975" class="footnote">17</a>]</sup>.需要值得注意的是，当写下一个删除标记后，只有下一个major compaction操作发起之后，这个删除标记才会消失。设想一下，当你写下一个删除标记-“删除所有&lt;= 时间T的数据”。但之后，你又执行了一个Put操作，版本&lt;= T。这样就算这个Put发生在删除之后，他的数据也算是打上了删除标记。这个Put并不会失败，但是你需要注意的是这个操作没有任何作用。只有一个major compaction执行只有，一切才会恢复正常。如果你的Put操作一直使用升序的版本，这个错误就不会发生。但是也有可能出现这样的情况，你删除之后，</p>
                        </div>
                        <div class="section" title="11.7.2.2. Major compactions 改变查询的结果">
                          <div class="titlepage">
                            <div>
                              <div>
                                <h4 class="title"><a name="d613e2980"></a>5.8.2.2.&nbsp;Major compactions 改变查询的结果</h4>
                              </div>
                            </div>
                          </div>
                          <p><span class="quote">“设想一下，你一个cell有三个版本t1,t2和t3。你的maximun-version设置是2.当你请求获取全部版本的时候，只会返回两个，t2和t3。如果你将t2和t3删除，就会返回t1。但是如果在删除之前，发生了major compaction操作，那么什么值都不好返回了。<sup>[<a name="d613e2986" href="book.htm#ftn.d613e2986" class="footnote">18</a>]</sup>”</span></p>
                        </div>
                      </div>
                    </div>
                    <div class="footnotes"><br>
                      <hr width="100" align="left">
                      <div class="footnote">
                        <p><sup>[<a name="ftn.d613e2836" href="book.htm#d613e2836" class="para">13</a>] </sup>目前，只有最新的那个是可以获取到的。.</p>
                      </div>
                      <div class="footnote">
                        <p><sup>[<a name="ftn.d613e2842" href="book.htm#d613e2842" class="para">14</a>] </sup>可以</p>
                      </div>
                      <div class="footnote">
                        <p><sup>[<a name="ftn.d613e2847" href="book.htm#d613e2847" class="para">15</a>] </sup>See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-2406" target="_top">HBASE-2406</a> for discussion of HBase versions. <a class="link" href="http://outerthought.org/blog/417-ot.html" target="_top">Bending time
                          in HBase</a> makes for a good read on the version, or time,
                          dimension in HBase. It has more detail on versioning than is
                          provided here. As of this writing, the limiitation <span class="emphasis"><em>Overwriting values at existing timestamps</em></span> mentioned in the article no longer holds in HBase. This section is
                          basically a synopsis of this article by Bruno Dumon.</p>
                      </div>
                      <div class="footnote">
                        <p><sup>[<a name="ftn.d613e2961" href="book.htm#d613e2961" class="para">16</a>] </sup>当Hbase执行一次major compaction,标记删除的数据会被实际的删除，删除标记也会被删除。</p>
                      </div>
                      <div class="footnote">
                        <p><sup>[<a name="ftn.d613e2975" href="book.htm#d613e2975" class="para">17</a>] </sup><a class="link" href="https://issues.apache.org/jira/browse/HBASE-2256" target="_top">HBASE-2256</a></p>
                      </div>
                      <div class="footnote">
                        <p><sup>[<a name="ftn.d613e2986" href="book.htm#d613e2986" class="para">18</a>] </sup>See <span class="emphasis"><em>Garbage Collection</em></span> in <a class="link" href="http://outerthought.org/blog/417-ot.html" target="_top">Bending
                          time in HBase</a></p>
                      </div>
                    </div>
                    <div>
                      <div>
                        <div>
                          <h2><a name="dm.sort" id="dm.sort"></a>5.9. Sort Order</h2>
                        </div>
                      </div>
                    </div>
                   <p>All data model operations HBase return data in sorted order. First by row, then by ColumnFamily, followed by column qualifier, and finally timestamp (sorted in reverse, so newest records are returned first).</p>
                   <div>
                     <div>
                       <div>
                         <h2><a name="dm.column.metadata" id="dm.column.metadata"></a>5.10. Column Metadata</h2>
                       </div>
                     </div>
                   </div>
                   <p>There is no store of column metadata outside of the internal KeyValue instances for a ColumnFamily. Thus, while HBase can support not only a wide number of columns per row, but a heterogenous set of columns between rows as well, it is your responsibility to keep track of the column names.</p>
                   <p>The only way to get a complete set of columns that exist for a ColumnFamily is to process all the rows. For more information about how HBase stores data internally, see <a href="book.htm#regions.arch.html#keyvalue" title="9.7.5.4. KeyValue">Section 9.7.5.4, “KeyValue”</a>.</p>
                   <div title="5.11. Joins">
                     <div>
                       <div>
                         <div>
                           <h2><a name="joins"></a>5.11. Joins</h2>
                         </div>
                       </div>
                     </div>
                     <p>Whether HBase supports joins is a common question on the dist-list, and there is a simple answer: it doesn't, at not least in the way that RDBMS' support them (e.g., with equi-joins or outer-joins in SQL). As has been illustrated in this chapter, the read data model operations in HBase are Get and Scan.</p>
                     <p>However, that doesn't mean that equivalent join functionality can't be supported in your application, but you have to do it yourself. The two primary strategies are either denormalizing the data upon writing to HBase, or to have lookup tables and do the join between HBase tables in your application or MapReduce code (and as RDBMS' demonstrate, there are several strategies for this depending on the size of the tables, e.g., nested loops vs. hash-joins). So which is the best approach? It depends on what you are trying to do, and as such there isn't a single answer that works for every use case.</p>
                   </div>
                   <br>
<p>&nbsp;</p>
                   <p>
            </p></div></div></div></div><div class="chapter" title="Chapter 5. 构建 HBase"><div class="titlepage"><div><div></div></div></div></div><div class="chapter" title="Chapter 6. Developers"><div class="titlepage"><div><div>
          <div class="section" title="8.1.  Schema 创建">
            <div class="titlepage">
              <div>
                <div>
                  <div class="titlepage">
                    <div>
                      <div>
                        <h2 class="title"><a name="schema"></a>Chapter&nbsp;6.&nbsp;HBase 的 Schema 设计</h2>
                      </div>
                    </div>
                  </div>
                  <div class="toc">
                    <p><b>Table of Contents</b></p>
                    <dl>
                      <dt><a href="book.htm#schema.creation">6.1. 
                        Schema 创建 </a></dt>
                      <dt><a href="book.htm#number.of.cfs">6.2. 
                        column families的数量 </a></dt>
                      <dt><a href="book.htm#timeseries">6.3. 
                        单调递增Row Keys/时序数据(log) </a></dt>
                      <dt><a href="book.htm#keysize">6.4. 尽量最小化row和column的大小</a></dt>
                      <dt><a href="book.htm#schema.versions">6.5. 
                        版本的时间 </a></dt>
                    </dl>
                  </div>
                  <p>有一个关于NSQL数据库的优点和确定的介绍， <a class="link" href="http://ianvarley.com/UT/MR/Varley_MastersReport_Full_2009-08-07.pdf" target="_top">No Relation: The Mixed Blessings of Non-Relational Databases</a>.
                    推荐看一看. </p>
                  <h2 class="title" style="clear: both"><a name="schema.creation"></a>6.1.&nbsp;
                    Schema 创建 </h2>
                </div>
              </div>
            </div>
            <p>可以使用<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html" target="_top">HBaseAdmin</a>或者<a class="xref" href="book.htm#shell" title="Chapter 4. The HBase Shell">Chapter&nbsp;4, <i>The HBase Shell</i></a> 来创建和编辑Hbase的schemas </p>
            <p>Tables must be disabled when making ColumnFamily modifications, for example..</p>
            <pre>Configuration config = HBaseConfiguration.create();    HBaseAdmin admin = new HBaseAdmin(conf);      String table = "myTable";    admin.disableTable(table);               HColumnDescriptor cf1 = ...;  admin.addColumn(table, cf1);      // adding new ColumnFamily  HColumnDescriptor cf2 = ...;  admin.modifyColumn(table, cf2);    // modifying existing ColumnFamily    admin.enableTable(table);                        </pre>
            See <a href="book.htm#client_dependencies" title="2.6.4. Client configuration and dependencies connecting to an HBase cluster">Section 2.6.4, “Client configuration and dependencies connecting to an HBase cluster”</a> for more information about configuring client connections.
            <p>Note: online schema changes are supported in the 0.92.x codebase, but the 0.90.x codebase requires the table to be disabled.</p>
            <div title="6.1.1. Schema Updates">
            <div>
              <div>
                <div>
                  <h3>6.1.1. Schema Updates</h3>
                </div>
              </div>
            </div>
            <p>When changes are made to either Tables or ColumnFamilies (e.g., region size, block size), these changes take effect the next time there is a major compaction and the StoreFiles get re-written.</p>
            <p>See <a href="book.htm#store" title="9.7.5. Store">Section 9.7.5, “Store”</a> for more information on StoreFiles.</p>
<div>
            <div>
            <div>
            <br>
          </div>
          <div class="section" title="8.2.  column families的数量">
            <div class="titlepage">
              <div>
                <div>
                  <h2 class="title" style="clear: both"><a name="number.of.cfs"></a>6.2.&nbsp;
                    column families的数量 </h2>
                </div>
              </div>
            </div>
            <p> 现在Hbase并不能很好的处理两个或者三个以上的column families，所以尽量让你的column families数量少一些。目前，flush和compaction操作是针对一个Region。所以当一个column family操作大量数据的时候会引发一个flush。那些不相关的column families也有进行flush操作，尽管他们没有操作多少数据。Compaction操作现在是根据一个column family下的全部文件的数量触发的，而不是根据文件大小触发的。当很多的column families在flush和compaction时,会造成很多没用的I/O负载(要想解决这个问题，需要将flush和compaction操作只针对一个column family) 。  For more information on compactions, see<a href="book.htm#compaction" title="9.7.5.5. Compaction">Section 9.7.5.5, “Compaction”</a>. </p>
            <p>尽量在你的应用中使用一个Column family。只有你的所有查询操作只访问一个column family的时候，可以引入第二个和第三个column family.例如，你有两个column family,但你查询的时候总是访问其中的一个，从来不会两个一起访问。</p>
            <div title="6.2.  On the number of column families">
              <div title="6.2.1. Cardinality of ColumnFamilies">
                <div>
                  <div>
                    <div>
                      <h3>6.2.1. Cardinality of ColumnFamilies</h3>
                    </div>
                  </div>
                </div>
                <p>Where multiple ColumnFamilies exist in a single table, be aware of the cardinality (i.e., number of rows). If ColumnFamilyA has 1 million rows and ColumnFamilyB has 1 billion rows, ColumnFamilyA's data will likely be spread across many, many regions (and RegionServers). This makes mass scans for ColumnFamilyA less efficient.</p>
              </div>
            </div>
            <p>&nbsp; </p>
          </div>
          <div class="section" title="8.3.  单调递增Row Keys/时序数据(log)">
            <div class="titlepage">
              <div>
                <div>
                  <h2 class="title" style="clear: both"><a name="rowkey.design" id="rowkey.design"></a>6.3.&nbsp;                    Rowkey设计</h2>
                  <h3 class="title" style="clear: both">6.3.1. 单调递增Row Keys/时序数据</h3>
                </div>
              </div>
            </div>
            <p> 在Tom White的Hadoop: The Definitive Guide一书中，有一个章节描述了一个值得注意的问题：在一个集群中，一个导入数据的进程一动不动，所以的client都在等待一个region(就是一个节点)，过了一会后，变成了下一个region...如果使用了单调递增或者时序的key就会造成这样的问题。详情可以参见IKai画的漫画<a class="link" href="http://ikaisays.com/2011/01/25/app-engine-datastore-tip-monotonically-increasing-values-are-bad/" target="_top">monotonically increasing values are bad</a>。使用了顺序的key会将本没有顺序的数据变得有顺序，把负载压在一台机器上。所以要尽量避免时间戳或者(e.g. 1, 2, 3)这样的key。 </p>
            <p>如果你需要导入时间顺序的文件(如log)到Hbase中，可以学习<a class="link" href="http://opentsdb.net/" target="_top">OpenTSDB</a>的做法。他有一个页面来描述他的<a class="link" href="http://opentsdb.net/schema.html" target="_top">schema</a>.OpenTSDB的Key的格式是[metric_type][event_timestamp]，乍一看，似乎违背了不将timestamp做key的建议，但是他并没有将timestamp作为key的一个关键位置，有成百上千的metric_type就足够将压力分散到各个region了。 </p>
          </div>
          <div class="section" title="8.4. 尽量最小化row和column的大小">
            <div class="titlepage">
              <div>
                <div>
                  <h3 class="title" style="clear: both"><a name="keysize"></a>6.3.2.&nbsp;尽量最小化row和column的大小(为何我的StoreFile 指示很大？)</h3>
                </div>
              </div>
            </div>
            <p>在Hbase中，值是作为一个cell保存在系统的中的，要定位一个cell,需要row,column name和timestamp.通常情况下，如果你的row和column的名字要是太大(甚至比value的大小还要大)的话，你可能会遇到一些有趣的情况。例如Marc Limotte 在 <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3551?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13005272#comment-13005272" target="_top">HBASE-3551</a>(recommended!)尾部提到的现象。在Hbase的存储文件<a class="xref" href="book.htm#hfile" title="9.7.5.2. StoreFile (HFile)">Section&nbsp;9.7.5.2, “StoreFile (HFile)”</a>中，有一个索引用来方便value的随机访问，但是访问一个cell的坐标要是太大的话，会占用很大的内存，这个索引会被用尽。所以要想解决，可以设置一个更大的block size，当然也可以使用更小的column name
              。</p>
            <div title="6.3.2. Try to minimize row and column sizes">
              <p>Compression will also make for larger indices. See the thread <a href="http://search-hadoop.com/m/hemBv1LiN4Q1/a+question+storefileIndexSize&subj=a+question+storefileIndexSize" target="_top">a question storefileIndexSize</a> up on the user mailing list.</p>
              <p>Most of the time small inefficiencies don't matter all that much. Unfortunately, this is a case where they do. Whatever patterns are selected for ColumnFamilies, attributes, and rowkeys they could be repeated several billion times in your data.</p>
              <p>See <a href="book.htm#keyvalue" title="9.7.5.4. KeyValue">Section 9.7.5.4, “KeyValue”</a> for more information on HBase stores data internally to see why this is important.</p>
              <div title="6.3.2.1. Column Families">
                <div>
                  <div>
                    <div>
                      <h4><a name="keysize.cf"></a>6.3.2.1. Column Families</h4>
                    </div>
                  </div>
                </div>
                <p>Try to keep the ColumnFamily names as small as possible, preferably one character (e.g. "d" for data/default).</p>
                <p>See <a href="book.htm#keyvalue" title="9.7.5.4. KeyValue">Section 9.7.5.4, “KeyValue”</a> for more information on HBase stores data internally to see why this is important.</p>
              </div>
              <div title="6.3.2.2. Attributes">
                <div>
                  <div>
                    <div>
                      <h4><a name="keysize.atttributes"></a>6.3.2.2. Attributes</h4>
                    </div>
                  </div>
                </div>
                <p>Although verbose attribute names (e.g., "myVeryImportantAttribute") are easier to read, prefer shorter attribute names (e.g., "via") to store in HBase.</p>
                <p>See <a href="book.htm#keyvalue" title="9.7.5.4. KeyValue">Section 9.7.5.4, “KeyValue”</a> for more information on HBase stores data internally to see why this is important.</p>
              </div>
              <div title="6.3.2.3. Rowkey Length">
                <div>
                  <div>
                    <div>
                      <h4><a name="keysize.row"></a>6.3.2.3. Rowkey Length</h4>
                    </div>
                  </div>
                </div>
                <p>Keep them as short as is reasonable such that they can still be useful for required data access (e.g., Get vs. Scan). A short key that is useless for data access is not better than a longer key with better get/scan properties. Expect tradeoffs when designing rowkeys.</p>
              </div>
              <div title="6.3.2.4. Byte Patterns">
                <div>
                  <div>
                    <div>
                      <h4><a name="keysize.patterns"></a>6.3.2.4. Byte Patterns</h4>
                    </div>
                  </div>
                </div>
                <p>A long is 8 bytes. You can store an unsigned number up to 18,446,744,073,709,551,615 in those eight bytes. If you stored this number as a String -- presuming a byte per character -- you need nearly 3x the bytes.</p>
                <p>Not convinced? Below is some sample code that you can run on your own.</p>
                <pre>// long  //  long l = 1234567890L;  byte[] lb = Bytes.toBytes(l);  System.out.println("long bytes length: " + lb.length);   // returns 8  		  String s = "" + l;  byte[] sb = Bytes.toBytes(s);  System.out.println("long as string length: " + sb.length);    // returns 10  			  // hash   //  MessageDigest md = MessageDigest.getInstance("MD5");  byte[] digest = md.digest(Bytes.toBytes(s));  System.out.println("md5 digest bytes length: " + digest.length);    // returns 16  		  String sDigest = new String(digest);  byte[] sbDigest = Bytes.toBytes(sDigest);  System.out.println("md5 digest as string length: " + sbDigest.length);    // returns 26		  </pre>
              </div>
            </div>
            <div title="6.3.3. Reverse Timestamps">
              <div>
                <div>
                  <div>
                    <h3><a name="reverse.timestamp"></a>6.3.3. Reverse Timestamps</h3>
                  </div>
                </div>
              </div>
              <p>A common problem in database processing is quickly finding the most recent version of a value. A technique using reverse timestamps as a part of the key can help greatly with a special case of this problem. Also found in the HBase chapter of Tom White's book Hadoop: The Definitive Guide (O'Reilly), the technique involves appending (Long.MAX_VALUE - timestamp) to the end of any key, e.g., [key][reverse_timestamp].</p>
              <p>The most recent value for [key] in a table can be found by performing a Scan for [key] and obtaining the first record. Since HBase keys are in sorted order, this key sorts before any older row-keys for [key] and thus is first.</p>
              <p>This technique would be used instead of using <a href="book.htm#schema.versions" title="6.4.  Number of Versions">Section 6.4, “ Number of Versions ”</a> where the intent is to hold onto all versions "forever" (or a very long time) and at the same time quickly obtain access to any other version by using the same Scan technique.</p>
            </div>
            <div title="6.3.4. Rowkeys and ColumnFamilies">
              <div>
                <div>
                  <div>
                    <h3><a name="rowkey.scope"></a>6.3.4. Rowkeys and ColumnFamilies</h3>
                  </div>
                </div>
              </div>
              <p>Rowkeys are scoped to ColumnFamilies. Thus, the same rowkey could exist in each ColumnFamily that exists in a table without collision.</p>
            </div>
            <div title="6.3.5. Immutability of Rowkeys">
              <div>
                <div>
                  <div>
                    <h3><a name="changing.rowkeys"></a>6.3.5. Immutability of Rowkeys</h3>
                  </div>
                </div>
              </div>
              <p>Rowkeys cannot be changed. The only way they can be "changed" in a table is if the row is deleted and then re-inserted. This is a fairly common question on the HBase dist-list so it pays to get the rowkeys right the first time (and/or before you've inserted a lot of data).</p>
              <div>
                <div>
                  <div>
                    <h2><a name="schema.versions" id="schema.versions"></a>6.4.  版本数量</h2>
</div>
                </div>
              </div>
              <div title="6.4.1. Maximum Number of Versions">
                <div>
                  <div>
                    <div>
                      <h3><a name="schema.versions.max"></a>6.4.1. Maximum Number of Versions</h3>
                    </div>
                  </div>
                </div>
                <p> 行的版本的数量是<a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a>设置的，每个column family可以单独设置，默认是3.这个设置是很重要的，在<a href="book.htm#datamodel" title="Chapter 11. 数据模型">Chapter 5, <em>数据模型</em></a>有描述，因为Hbase是<em>不会</em>去覆盖一个值的，他只会在后面在追加写，用timestamp来区分、过早的版本会在执行major compaction的时候删除。这个版本的值可以根据具体的应用增加减少。 </p>
                <p>It is not recommended setting the number of max versions to an exceedingly high level (e.g., hundreds or more) unless those old values are very dear to you because this will greatly increase StoreFile size.</p>
              </div>
              <div title="6.4.2.  Minimum Number of Versions">
                <div>
                  <div>
                    <div>
                      <h3><a name="schema.minversions"></a>6.4.2.  Minimum Number of Versions</h3>
                    </div>
                  </div>
                </div>
                <p>Like maximum number of row versions, the minimum number of row versions to keep is configured per column family via <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a>. The default for min versions is 0, which means the feature is disabled. The minimum number of row versions parameter is used together with the time-to-live parameter and can be combined with the number of row versions parameter to allow configurations such as "keep the last T minutes worth of data, at most N versions, <em>but keep at least M versions around</em>" (where M is the value for minimum number of row versions, M&lt;N). This parameter should only be set when time-to-live is enabled for a column family and must be less than the number of row versions.</p>
              </div>
              <div>
                <div>
                  <div>
                    <h2><a name="supported.datatypes" id="supported.datatypes"></a>6.5.  Supported Datatypes</h2>
                  </div>
                </div>
              </div>
              <p>HBase supports a "bytes-in/bytes-out" interface via <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Put.html" target="_top">Put</a> and <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Result.html" target="_top">Result</a>, so anything that can be converted to an array of bytes can be stored as a value. Input could be strings, numbers, complex objects, or even images as long as they can rendered as bytes.</p>
              <p>There are practical limits to the size of values (e.g., storing 10-50MB objects in HBase would probably be too much to ask); search the mailling list for conversations on this topic. All rows in HBase conform to the <a href="book.htm#datamodel" title="Chapter 5. Data Model">Chapter 5, <em>Data Model</em></a>, and that includes versioning. Take that into consideration when making your design, as well as block size for the ColumnFamily.</p>
              <div title="6.5.1. Counters">
                <div>
                  <div>
                    <div>
                      <h3><a name="counters"></a>6.5.1. Counters</h3>
                    </div>
                  </div>
                </div>
                <p>One supported datatype that deserves special mention are "counters" (i.e., the ability to do atomic increments of numbers). See <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#increment%28org.apache.hadoop.hbase.client.Increment%29" target="_top">Increment</a> in HTable.</p>
                <p>Synchronization on counters are done on the RegionServer, not in the client.</p>
              </div>
              <div>
                <div>
                  <div>
                    <h2><a name="schema.joins" id="schema.joins"></a>6.6. Joins</h2>
                  </div>
                </div>
              </div>
              <p>If you have multiple tables, don't forget to factor in the potential for <a href="book.htm#joins" title="5.11. Joins">Section 5.11, “Joins”</a> into the schema design.</p>
              <div>
                <div>
                  <div>
                    <h2><a name="ttl"></a>6.7. Time To Live (TTL)</h2>
                  </div>
                </div>
              </div>
              <p>ColumnFamilies can set a TTL length in seconds, and HBase will automatically delete rows once the expiration time is reached. This applies to <em>all</em> versions of a row - even the current one. The TTL time encoded in the HBase for the row is specified in UTC.</p>
              <p>See <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor" target="_top">HColumnDescriptor</a> for more information.</p>
              <div>
                <div>
                  <div>
                    <h2><a name="cf.keep.deleted" id="cf.keep.deleted"></a>6.8.  Keeping Deleted Cells</h2>
                  </div>
                </div>
              </div>
              <p>ColumnFamilies can optionally keep deleted cells. That means deleted cells can still be retrieved with <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html" target="_top">Get</a> or <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a> operations, as long these operations have a time range specified that ends before the timestamp of any delete that would affect the cells. This allows for point in time queries even in the presence of deletes.</p>
              <p>Deleted cells are still subject to TTL and there will never be more than "maximum number of versions" deleted cells. A new "raw" scan options returns all deleted rows and the delete markers.</p>
              <p>See <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> for more information.</p>
              <div>
                <div>
                  <div>
                    <h2><a name="secondary.indexes" id="secondary.indexes"></a>6.9.  Secondary Indexes and Alternate Query Paths</h2>
                  </div>
                </div>
              </div>
              <p>This section could also be titled "what if my table rowkey looks like <em>this</em> but I also want to query my table like <em>that</em>." A common example on the dist-list is where a row-key is of the format "user-timestamp" but there are are reporting requirements on activity across users for certain time ranges. Thus, selecting by user is easy because it is in the lead position of the key, but time is not.</p>
              <p>There is no single answer on the best way to handle this because it depends on...</p>
              <div>
                <ul type="disc">
                  <li>Number of users</li>
                  <li>Data size and data arrival rate</li>
                  <li>Flexibility of reporting requirements (e.g., completely ad-hoc date selection vs. pre-configured ranges)</li>
                  <li>Desired execution speed of query (e.g., 90 seconds may be reasonable to some for an ad-hoc report, whereas it may be too long for others)</li>
                </ul>
              </div>
              <p>... and solutions are also influenced by the size of the cluster and how much processing power you have to throw at the solution. Common techniques are in sub-sections below. This is a comprehensive, but not exhaustive, list of approaches.</p>
              <p>It should not be a surprise that secondary indexes require additional cluster space and processing. This is precisely what happens in an RDBMS because the act of creating an alternate index requires both space and processing cycles to update. RBDMS products are more advanced in this regard to handle alternative index management out of the box. However, HBase scales better at larger data volumes, so this is a feature trade-off.</p>
              <p>Pay attention to <a href="book.htm#performance" title="Chapter 11. Performance Tuning">Chapter 11, <em>Performance Tuning</em></a> when implementing any of these approaches.</p>
              <p>Additionally, see the David Butler response in this dist-list thread <a href="http://search-hadoop.com/m/nvbiBp2TDP/Stargate%252Bhbase&subj=Stargate+hbase" target="_top">HBase, mail # user - Stargate+hbase</a></p>
              <div title="6.9.1.  Filter Query">
                <div>
                  <div>
                    <div>
                      <h3><a name="secondary.indexes.filter"></a>6.9.1.  Filter Query</h3>
                    </div>
                  </div>
                </div>
                <p>Depending on the case, it may be appropriate to use <a href="book.htm#client.filter" title="9.4. Client Request Filters">Section 9.4, “Client Request Filters”</a>. In this case, no secondary index is created. However, don't try a full-scan on a large table like this from an application (i.e., single-threaded client).</p>
              </div>
              <div title="6.9.2.  Periodic-Update Secondary Index">
                <div>
                  <div>
                    <div>
                      <h3><a name="secondary.indexes.periodic"></a>6.9.2.  Periodic-Update Secondary Index</h3>
                    </div>
                  </div>
                </div>
                <p>A secondary index could be created in an other table which is periodically updated via a MapReduce job. The job could be executed intra-day, but depending on load-strategy it could still potentially be out of sync with the main data table.</p>
                <p>See <a href="book.htm#mapreduce.example.readwrite" title="7.2.2. HBase MapReduce Read/Write Example">Section 7.2.2, “HBase MapReduce Read/Write Example”</a> for more information.</p>
              </div>
              <div title="6.9.3.  Dual-Write Secondary Index">
                <div>
                  <div>
                    <div>
                      <h3><a name="secondary.indexes.dualwrite"></a>6.9.3.  Dual-Write Secondary Index</h3>
                    </div>
                  </div>
                </div>
                <p>Another strategy is to build the secondary index while publishing data to the cluster (e.g., write to data table, write to index table). If this is approach is taken after a data table already exists, then bootstrapping will be needed for the secondary index with a MapReduce job (see <a href="book.htm#secondary.indexes.periodic" title="6.9.2.  Periodic-Update Secondary Index">Section 6.9.2, “ Periodic-Update Secondary Index ”</a>).</p>
              </div>
              <div title="6.9.4.  Summary Tables">
                <div>
                  <div>
                    <div>
                      <h3><a name="secondary.indexes.summary"></a>6.9.4.  Summary Tables</h3>
                    </div>
                  </div>
                </div>
                <p>Where time-ranges are very wide (e.g., year-long report) and where the data is voluminous, summary tables are a common approach. These would be generated with MapReduce jobs into another table.</p>
                <p>See <a href="book.htm#mapreduce.example.summary" title="7.2.4. HBase MapReduce Summary to HBase Example">Section 7.2.4, “HBase MapReduce Summary to HBase Example”</a> for more information.</p>
              </div>
              <div title="6.9.5.  Coprocessor Secondary Index">
                <div>
                  <div>
                    <div>
                      <h3><a name="secondary.indexes.coproc"></a>6.9.5.  Coprocessor Secondary Index</h3>
                    </div>
                  </div>
                </div>
                <p>Coprocessors act like RDBMS triggers. These were added in 0.92. For more information, see <a href="book.htm#coprocessors" title="9.6.3. Coprocessors">Section 9.6.3, “Coprocessors”</a></p>
              </div>
              <div>
                <div>
                  <div>
                    <h2><a name="schema.smackdown" id="schema.smackdown"></a>6.10. Schema Design Smackdown</h2>
                  </div>
                </div>
              </div>
              <p>This section will describe common schema design questions that appear on the dist-list. These are general guidelines and not laws - each application must consider it's own needs.</p>
              <div title="6.10.1. Rows vs. Versions">
                <div>
                  <div>
                    <div>
                      <h3><a name="schema.smackdown.rowsversions"></a>6.10.1. Rows vs. Versions</h3>
                    </div>
                  </div>
                </div>
                <p>A common question is whether one should prefer rows or HBase's built-in-versioning. The context is typically where there are "a lot" of versions of a row to be retained (e.g., where it is significantly above the HBase default of 3 max versions). The rows-approach would require storing a timstamp in some portion of the rowkey so that they would not overwite with each successive update.</p>
                <p>Preference: Rows (generally speaking).</p>
              </div>
              <div title="6.10.2. Rows vs. Columns">
                <div>
                  <div>
                    <div>
                      <h3><a name="schema.smackdown.rowscols"></a>6.10.2. Rows vs. Columns</h3>
                    </div>
                  </div>
                </div>
                <p>Another common question is whether one should prefer rows or columns. The context is typically in extreme cases of wide tables, such as having 1 row with 1 million attributes, or 1 million rows with 1 columns apiece.</p>
                <p>Preference: Rows (generally speaking). To be clear, this guideline is in the context is in extremely wide cases, not in the standard use-case where one needs to store a few dozen or hundred columns.</p>
              </div>
              <div>
                <div>
                  <div>
                    <h2><a name="schema.ops" id="schema.ops"></a>6.11. Operational and Performance Configuration Options</h2>
                  </div>
                </div>
              </div>
              <p>See the Performance section <a href="book.htm#perf.schema" title="11.6. Schema Design">Section 11.6, “Schema Design”</a> for more information operational and performance schema design options, such as Bloom Filters, Table-configured regionsizes, compression, and blocksizes.</p>
              <div>
                <div>
                  <div>
                    <h2><a name="constraints" id="constraints"></a>6.12. Constraints</h2>
                  </div>
                </div>
              </div>
              <p>HBase currently supports 'constraints' in traditional (SQL) database parlance. The advised usage for Constraints is in enforcing business rules for attributes in the table (eg. make sure values are in the range 1-10). Constraints could also be used to enforce referential integrity, but this is strongly discouraged as it will dramatically decrease the write throughput of the tables where integrity checking is enabled. Extensive documentation on using Constraints can be found at: <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/constraint" target="_top">Constraint</a> since version 0.94.</p>
<p>&nbsp;</p>
              <p>&nbsp;</p>
            </div>
          </div>
          <h2 class="title">&nbsp;</h2></div></div></div></div><div class="chapter" title="Chapter 7. HBase 和 MapReduce"><div class="titlepage"><div><div><h2 class="title"><a name="mapreduce"></a>Chapter&nbsp;7.&nbsp;HBase 和 MapReduce</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="book.htm#splitter">7.1. 默认 HBase MapReduce 分割器(Splitter)</a></span></dt><dt><span class="section"><a href="book.htm#mapreduce.example">7.2. HBase Input MapReduce 例子</a></span></dt><dt><span class="section"><a href="book.htm#mapreduce.htable.access">7.3. 在一个MapReduce Job中访问其他的HBase Tables</a></span></dt><dt><span class="section"><a href="book.htm#mapreduce.specex">7.4. 预测执行</a></span></dt></dl></div><p>关于 <a class="link" href="http://hbase.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#package_description" target="_top">HBase 和 MapReduce</a>详见 javadocs.
  下面是一些附加的帮助文档.  For more information about MapReduce (i.e., the framework in general), see the <a href="http://hadoop.apache.org/common/docs/current/mapred_tutorial.html" target="_top">Hadoop MapReduce Tutorial</a>. </p>
           <div class="section" title="7.1. 默认 HBase MapReduce 分割器(Splitter)"><div class="titlepage"><div>
             <div><h2 class="title" style="clear: both"><a name="splitter"></a>7.1.&nbsp;Map-Task 分割</h2>
<h3 class="title" style="clear: both">7.1.1 默认 HBase MapReduce 分割器(Splitter)</h3>
             </div></div></div><p>当 MapReduce job的HBase table 使用<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html" target="_top">TableInputFormat</a>为数据源格式的时候,他的splitter会给这个table的每个region一个map。因此，如果一个table有100个region，就有100个map-tasks，不论需要scan多少个column families 。
 </p>
             <div>
               <div>
                 <div>
                   <h3>7.1.2. Custom Splitters</h3>
                 </div>
               </div>
             </div>
             <p>For those interested in implementing custom splitters, see the method getSplits in <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormatBase.html" target="_top">TableInputFormatBase</a>. That is where the logic for map-task assignment resides.</p>
<p>&nbsp;</p>
           </div><div class="section" title="7.2. HBase Input MapReduce 例子"><div class="titlepage"><div>
             <div>
               <h2 class="title" style="clear: both"><a name="mapreduce.example"></a>7.2.&nbsp;HBase MapReduce 例子</h2>
<h2 class="title" style="clear: both">7.2.1 HBase  MapReduce 读取例子</h2>
             </div></div></div><p>&nbsp;</p>
             <p> The following is an example of using HBase as a MapReduce source in read-only manner. Specifically, there is a Mapper instance but no Reducer, and nothing is being emitted from the Mapper. 如下所示...
	         </p>
             <pre class="programlisting">Configuration config = HBaseConfiguration.create();<br>Job job = new Job(config, &quot;ExampleRead&quot;);<br>job.setJarByClass(MyReadJob.class);     // class that contains mapper<br>	<br>Scan scan = new Scan();<br>scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs<br>scan.setCacheBlocks(false);  // don't set to true for MR jobs<br>// set other scan attrs<br>...<br>  <br>TableMapReduceUtil.initTableMapperJob(<br>  tableName,        // input HBase table name<br>  scan,             // Scan instance to control CF and attribute selection<br>  MyMapper.class,   // mapper<br>  null,             // mapper output key <br>  null,             // mapper output value<br>  job);<br>job.setOutputFormatClass(NullOutputFormat.class);   // because we aren't emitting anything from mapper<br>	    <br>boolean b = job.waitForCompletion(true);<br>if (!b) {<br>  throw new IOException(&quot;error with job!&quot;);<br>}</pre>
             <p>
  ...mapper需要继承于<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableMapper.html" target="_top">TableMapper</a>...
	</p><pre class="programlisting">public class MyMapper extends TableMapper&lt;Text, LongWritable&gt; {
public void map(ImmutableBytesWritable row, Result value, Context context) 
throws InterruptedException, IOException {
// process data for the row from the Result instance.</pre><p>
  	</p></div>
  	<div class="section" title="7.2.&nbsp;HBase MapReduce Examples"><div class="titlepage"><div><div></div></div></div><div class="section" title="7.2.2.&nbsp;HBase MapReduce Read/Write Example"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.readwrite"></a>7.2.2.&nbsp;HBase MapReduce Read/Write Example</h3></div></div></div><p>The following is an example of using HBase both as a source and as a sink with MapReduce. 
    This example will simply copy data from one table to another.
    </p><pre class="programlisting">Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleReadWrite");
job.setJarByClass(MyReadWriteJob.class);    // class that contains mapper
	        	        
Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs
	        
TableMapReduceUtil.initTableMapperJob(
	sourceTable,      // input table
	scan,	          // Scan instance to control CF and attribute selection
	MyMapper.class,   // mapper class
	null,	          // mapper output key
	null,	          // mapper output value
	job);
TableMapReduceUtil.initTableReducerJob(
	targetTable,      // output table
	null,             // reducer class
	job);
job.setNumReduceTasks(0);
	        
boolean b = job.waitForCompletion(true);
if (!b) {
    throw new IOException("error with job!");
}
    </pre><p>
	An explanation is required of what <code class="classname">TableMapReduceUtil</code> is doing, especially with the reducer.  
	<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html" target="_top">TableOutputFormat</a> is being used
	as the outputFormat class, and several parameters are being set on the config (e.g., TableOutputFormat.OUTPUT_TABLE), as
	well as setting the reducer output key to <code class="classname">ImmutableBytesWritable</code> and reducer value to <code class="classname">Writable</code>.
	These could be set by the programmer on the job and conf, but <code class="classname">TableMapReduceUtil</code> tries to make things easier.    
	</p><p>The following is the example mapper, which will create a <code class="classname">Put</code> and matching the input <code class="classname">Result</code>
	and emit it.  Note:  this is what the CopyTable utility does.
	</p><p>
    </p><pre class="programlisting">public static class MyMapper extends TableMapper&lt;ImmutableBytesWritable, Put&gt;  {

	public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {
		// this example is just copying the data from the source table...
   		context.write(row, resultToPut(row,value));
   	}
        
  	private static Put resultToPut(ImmutableBytesWritable key, Result result) throws IOException {
  		Put put = new Put(key.get());
 		for (KeyValue kv : result.raw()) {
			put.add(kv);
		}
		return put;
   	}
}
    </pre><p>
    </p><p>There isn't actually a reducer step, so <code class="classname">TableOutputFormat</code> takes care of sending the <code class="classname">Put</code>
    to the target table. 
    </p><p>
    </p><p>This is just an example, developers could choose not to use <code class="classname">TableOutputFormat</code> and connect to the 
    target table themselves.
    </p><p>
    </p></div><div class="section" title="7.2.3.&nbsp;HBase MapReduce Read/Write Example With Multi-Table Output"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.readwrite.multi"></a>7.2.3.&nbsp;HBase MapReduce Read/Write Example With Multi-Table Output</h3></div></div></div><p>TODO:  example for <code class="classname">MultiTableOutputFormat</code>.
    </p></div><div class="section" title="7.2.4.&nbsp;HBase MapReduce Summary to HBase Example"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.summary"></a>7.2.4.&nbsp;HBase MapReduce Summary to HBase Example</h3></div></div></div><p>The following example uses HBase as a MapReduce source and sink with a summarization step.  This example will 
    count the number of distinct instances of a value in a table and write those summarized counts in another table.
    </p><pre class="programlisting">Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleSummary");
job.setJarByClass(MySummaryJob.class);     // class that contains mapper and reducer
	        
Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs
	        
TableMapReduceUtil.initTableMapperJob(
	sourceTable,        // input table
	scan,               // Scan instance to control CF and attribute selection
	MyMapper.class,     // mapper class
	Text.class,         // mapper output key
	IntWritable.class,  // mapper output value
	job);
TableMapReduceUtil.initTableReducerJob(
	targetTable,        // output table
	MyTableReducer.class,    // reducer class
	job);
job.setNumReduceTasks(1);   // at least one, adjust as required
	    
boolean b = job.waitForCompletion(true);
if (!b) {
	throw new IOException("error with job!");
}    
    </pre><p>
    In this example mapper a column with a String-value is chosen as the value to summarize upon.  
    This value is used as the key to emit from the mapper, and an <code class="classname">IntWritable</code> represents an instance counter.
    </p><pre class="programlisting">public static class MyMapper extends TableMapper&lt;Text, IntWritable&gt;  {

	private final IntWritable ONE = new IntWritable(1);
   	private Text text = new Text();
    	
   	public void map(ImmutableBytesWritable row, Result value, Context context) throws IOException, InterruptedException {
        	String val = new String(value.getValue(Bytes.toBytes("cf"), Bytes.toBytes("attr1")));
          	text.set(val);     // we can only emit Writables...

        	context.write(text, ONE);
   	}
}
    </pre><p>
    In the reducer, the "ones" are counted (just like any other MR example that does this), and then emits a <code class="classname">Put</code>.
    </p><pre class="programlisting">public static class MyTableReducer extends TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt;  {
        
 	public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
    		int i = 0;
    		for (IntWritable val : values) {
    			i += val.get();
    		}
    		Put put = new Put(Bytes.toBytes(key.toString()));
    		put.add(Bytes.toBytes("cf"), Bytes.toBytes("count"), Bytes.toBytes(i));

    		context.write(null, put);
   	}
}
    </pre><p>
    </p></div><div class="section" title="7.2.5.&nbsp;HBase MapReduce Summary to File Example"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.summary.file"></a>7.2.5.&nbsp;HBase MapReduce Summary to File Example</h3></div></div></div><p>This very similar to the summary example above, with exception that this is using HBase as a MapReduce source
       but HDFS as the sink.  The differences are in the job setup and in the reducer.  The mapper remains the same.
       </p><pre class="programlisting">Configuration config = HBaseConfiguration.create();
Job job = new Job(config,"ExampleSummaryToFile");
job.setJarByClass(MySummaryFileJob.class);     // class that contains mapper and reducer
	        
Scan scan = new Scan();
scan.setCaching(500);        // 1 is the default in Scan, which will be bad for MapReduce jobs
scan.setCacheBlocks(false);  // don't set to true for MR jobs
// set other scan attrs
	        
TableMapReduceUtil.initTableMapperJob(
	sourceTable,        // input table
	scan,               // Scan instance to control CF and attribute selection
	MyMapper.class,     // mapper class
	Text.class,         // mapper output key
	IntWritable.class,  // mapper output value
	job);
job.setReducerClass(MyReducer.class);    // reducer class
job.setNumReduceTasks(1);    // at least one, adjust as required
FileOutputFormat.setOutputPath(job, new Path("/tmp/mr/mySummaryFile"));  // adjust directories as required
	    
boolean b = job.waitForCompletion(true);
if (!b) {
	throw new IOException("error with job!");
}    
    </pre>
    As stated above, the previous Mapper can run unchanged with this example.  
    As for the Reducer, it is a "generic" Reducer instead of extending TableMapper and emitting Puts.
    <pre class="programlisting"> public static class MyReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;  {
        
	public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
		int i = 0;
		for (IntWritable val : values) {
			i += val.get();
		}	
		context.write(key, new IntWritable(i));
	}
}
    </pre></div><div class="section" title="7.2.6.&nbsp;HBase MapReduce Summary to HBase Without Reducer"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.summary.noreducer"></a>7.2.6.&nbsp;HBase MapReduce Summary to HBase Without Reducer</h3></div></div></div><p>It is also possible to perform summaries without a reducer - if you use HBase as the reducer.
       </p><p>An HBase target table would need to exist for the job summary.  The HTable method <code class="code">incrementColumnValue</code>
       would be used to atomically increment values.  From a performance perspective, it might make sense to keep a Map 
       of values with their values to be incremeneted for each map-task, and make one update per key at during the <code class="code">
       cleanup</code> method of the mapper.  However, your milage may vary depending on the number of rows to be processed and 
       unique keys.
       </p><p>In the end, the summary results are in HBase.
       </p></div><div class="section" title="7.2.7.&nbsp;HBase MapReduce Summary to RDBMS"><div class="titlepage"><div><div><h3 class="title"><a name="mapreduce.example.summary.rdbms"></a>7.2.7.&nbsp;HBase MapReduce Summary to RDBMS</h3></div></div></div><p>Sometimes it is more appropriate to generate summaries to an RDBMS.  For these cases, it is possible
       to generate summaries directly to an RDBMS via a custom reducer.  The <code class="code">setup</code> method
       can connect to an RDBMS (the connection information can be passed via custom parameters in the context) and the
       cleanup method can close the connection.
       </p><p>It is critical to understand that number of reducers for the job affects the summarization implementation, and
       you'll have to design this into your reducer.  Specifically, whether it is designed to run as a singleton (one reducer)
       or multiple reducers.  Neither is right or wrong, it depends on your use-case.  Recognize that the more reducers that
       are assigned to the job, the more simultaneous connections to the RDBMS will be created - this will scale, but only to a point. 
       </p><pre class="programlisting"> public static class MyRdbmsReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;  {

	private Connection c = null;
	
	public void setup(Context context) {
  		// create DB connection...
  	}
        
	public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {
		// do summarization
		// in this example the keys are Text, but this is just an example
	}
	
	public void cleanup(Context context) {
  		// close db connection
  	}
	
}
    </pre><p>In the end, the summary results are written to your RDBMS table/s.
       </p></div></div>
  	<div class="section" title="7.3. 在一个MapReduce Job中访问其他的HBase Tables"><div class="titlepage"><div>
  	  <div>
  	    <div>
  	      <div>
  	        <div></div>
  	      </div>
  	    </div>
  	    <h2 class="title" style="clear: both"><a name="mapreduce.htable.access"></a>7.3.&nbsp;在一个MapReduce Job中访问其他的HBase Tables</h2>
  	  </div></div></div><p>尽管现有的框架允许一个HBase table作为一个MapReduce job的输入，其他的Hbase table可以同时作为普通的表被访问。例如在一个MapReduce的job中，可以在Mapper的setup方法中创建HTable实例。
	</p><pre class="programlisting">public class MyMapper extends TableMapper&lt;Text, LongWritable&gt; {
  private HTable myOtherTable;

  @Override
  public void setup(Context context) {
    myOtherTable = new HTable("myOtherTable");
  }</pre><p>
   </p></div><div class="section" title="7.4. 预测执行"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="mapreduce.specex"></a>7.4.&nbsp;预测执行</h2></div></div></div><p>通常建议关掉针对HBase的MapReduce job的预测执行(speculative execution)功能。这个功能也可以用每个Job的配置来完成。对于整个集群，使用预测执行意味着双倍的运算量。这可不是你所希望的。</p>
     <p> See <a href="book.htm#spec.ex" title="2.8.2.9. Speculative Execution">Section 2.8.2.9, “Speculative Execution”</a> for more information. </p>
   </div></div>
   
   <div class="chapter" title="Chapter&nbsp;8.&nbsp;Secure HBase"><div class="titlepage"><div><div><h2 class="title"><a name="security"></a>Chapter&nbsp;8.&nbsp;Secure HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="security.html#hbase.secure.configuration">8.1. Secure Client Access to HBase</a></span></dt><dd><dl><dt><span class="section"><a href="security.html#d1934e4347">8.1.1. Prerequisites</a></span></dt><dt><span class="section"><a href="security.html#d1934e4393">8.1.2. Server-side Configuration for Secure Operation</a></span></dt><dt><span class="section"><a href="security.html#d1934e4405">8.1.3. Client-side Configuration for Secure Operation</a></span></dt><dt><span class="section"><a href="security.html#d1934e4447">8.1.4. Client-side Configuration for Secure Operation - Thrift Gateway</a></span></dt><dt><span class="section"><a href="security.html#d1934e4462">8.1.5. Client-side Configuration for Secure Operation - REST Gateway</a></span></dt></dl></dd><dt><span class="section"><a href="hbase.accesscontrol.configuration.html">8.2. Access Control</a></span></dt><dd><dl><dt><span class="section"><a href="hbase.accesscontrol.configuration.html#d1934e4487">8.2.1. Prerequisites</a></span></dt><dt><span class="section"><a href="hbase.accesscontrol.configuration.html#d1934e4494">8.2.2. Overview</a></span></dt><dt><span class="section"><a href="hbase.accesscontrol.configuration.html#d1934e4651">8.2.3. Server-side Configuration for Access Control</a></span></dt><dt><span class="section"><a href="hbase.accesscontrol.configuration.html#d1934e4663">8.2.4. Shell Enhancements for Access Control</a></span></dt></dl></dd></dl></div><div class="section" title="8.1.&nbsp;Secure Client Access to HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.secure.configuration"></a>8.1.&nbsp;Secure Client Access to HBase</h2></div></div></div><p>Newer releases of HBase (&gt;= 0.92) support optional SASL authentication of clients.</p><p>This describes how to set up HBase and HBase clients for connection to secure HBase resources. </p><div class="section" title="8.1.1.&nbsp;Prerequisites"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4347"></a>8.1.1.&nbsp;Prerequisites</h3></div></div></div><p>
       HBase must have been built using the new maven profile for secure Hadoop/HBase: <code class="code">-P security</code>. Secure Hadoop dependent classes are separated under a pseudo-module in the security/ directory and are only included if built with the secure Hadoop profile. 
    </p><p>
        You need to have a working Kerberos KDC.
    </p><p>
        A HBase configured for secure client access is expected to be running
        on top of a secured HDFS cluster. HBase must be able to authenticate
        to HDFS services. HBase needs Kerberos credentials to interact with
        the Kerberos-enabled HDFS daemons. Authenticating a service should be
        done using a keytab file. The procedure for creating keytabs for HBase
        service is the same as for creating keytabs for Hadoop. Those steps
        are omitted here. Copy the resulting keytab files to wherever HBase
        Master and RegionServer processes are deployed and make them readable
        only to the user account under which the HBase daemons will run.
    </p><p>
        A Kerberos principal has three parts, with the form 
        <code class="code">username/fully.qualified.domain.name@YOUR-REALM.COM</code>. We
        recommend using <code class="code">hbase</code> as the username portion.
    </p><p>
        The following is an example of the configuration properties for
        Kerberos operation that must be added to the 
        <code class="code">hbase-site.xml</code> file on every server machine in the
        cluster. Required for even the most basic interactions with a
        secure Hadoop configuration, independent of HBase security.
    </p><pre class="programlisting">      &lt;property&gt;
        &lt;name&gt;hbase.regionserver.kerberos.principal&lt;/name&gt;
        &lt;value&gt;hbase/_HOST@YOUR-REALM.COM&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.regionserver.keytab.file&lt;/name&gt;
        &lt;value&gt;/etc/hbase/conf/keytab.krb5&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.master.kerberos.principal&lt;/name&gt;
        &lt;value&gt;hbase/_HOST@YOUR-REALM.COM&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.master.keytab.file&lt;/name&gt;
        &lt;value&gt;/etc/hbase/conf/keytab.krb5&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        Each HBase client user should also be given a Kerberos principal. This
        principal should have a password assigned to it (as opposed to a
        keytab file). The client principal's <code class="code">maxrenewlife</code> should
        be set so that it can be renewed enough times for the HBase client
        process to complete. For example, if a user runs a long-running HBase
        client process that takes at most 3 days, we might create this user's
        principal within <code class="code">kadmin</code> with: <code class="code">addprinc -maxrenewlife
        3days</code>
    </p><p>
        Long running daemons with indefinite lifetimes that require client
        access to HBase can instead be configured to log in from a keytab. For
        each host running such daemons, create a keytab with 
        <code class="code">kadmin</code> or <code class="code">kadmin.local</code>. The procedure for
        creating keytabs for HBase service is the same as for creating
        keytabs for Hadoop. Those steps are omitted here. Copy the resulting
        keytab files to where the client daemon will execute and make them
        readable only to the user account under which the daemon will run.
    </p></div><div class="section" title="8.1.2.&nbsp;Server-side Configuration for Secure Operation"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4393"></a>8.1.2.&nbsp;Server-side Configuration for Secure Operation</h3></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file on every server machine in the cluster:
    </p><pre class="programlisting">      &lt;property&gt;
        &lt;name&gt;hbase.security.authentication&lt;/name&gt;
        &lt;value&gt;kerberos&lt;/value&gt; 
      &lt;/property&gt; 
      &lt;property&gt;
        &lt;name&gt;hbase.security.authorization&lt;/name&gt;
        &lt;value&gt;true&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
        &lt;name&gt;hbase.rpc.engine&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.ipc.SecureRpcEngine&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
      &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.token.TokenProvider&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
       A full shutdown and restart of HBase service is required when deploying
       these configuration changes.
    </p></div><div class="section" title="8.1.3.&nbsp;Client-side Configuration for Secure Operation"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4405"></a>8.1.3.&nbsp;Client-side Configuration for Secure Operation</h3></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file on every client:
    </p><pre class="programlisting">      &lt;property&gt;
        &lt;name&gt;hbase.security.authentication&lt;/name&gt;
        &lt;value&gt;kerberos&lt;/value&gt;
      &lt;/property&gt; 
      &lt;property&gt;
        &lt;name&gt;hbase.rpc.engine&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.ipc.SecureRpcEngine&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        The client environment must be logged in to Kerberos from KDC or
        keytab via the <code class="code">kinit</code> command before communication with
        the HBase cluster will be possible.
    </p><p>
        Be advised that if the <code class="code">hbase.security.authentication</code>
        and <code class="code">hbase.rpc.engine</code> properties in the client- and
        server-side site files do not match, the client will not be able to
        communicate with the cluster.
    </p><p>
        Once HBase is configured for secure RPC it is possible to optionally
        configure encrypted communication. To do so, add the following to the
        <code class="code">hbase-site.xml</code> file on every client:
    </p><pre class="programlisting">      &lt;property&gt;
        &lt;name&gt;hbase.rpc.protection&lt;/name&gt;
        &lt;value&gt;privacy&lt;/value&gt;
      &lt;/property&gt;
    </pre><p>
        This configuration property can also be set on a per connection basis.
        Set it in the <code class="code">Configuration</code> supplied to
        <code class="code">HTable</code>:
    </p><pre class="programlisting">      Configuration conf = HBaseConfiguration.create();
      conf.set("hbase.rpc.protection", "privacy");
      HTable table = new HTable(conf, tablename);
    </pre><p>
        Expect a ~10% performance penalty for encrypted communication.
    </p></div><div class="section" title="8.1.4.&nbsp;Client-side Configuration for Secure Operation - Thrift Gateway"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4447"></a>8.1.4.&nbsp;Client-side Configuration for Secure Operation - Thrift Gateway</h3></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file for every Thrift gateway:
    </p><pre class="programlisting">    &lt;property&gt;
      &lt;name&gt;hbase.thrift.keytab.file&lt;/name&gt;
      &lt;value&gt;/etc/hbase/conf/hbase.keytab&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.thrift.kerberos.principal&lt;/name&gt;
      &lt;value&gt;$USER/_HOST@HADOOP.LOCALDOMAIN&lt;/value&gt;
    &lt;/property&gt;
    </pre><p>
    </p><p>
        Substitute the appropriate credential and keytab for $USER and $KEYTAB
        respectively.
    </p><p>
        The Thrift gateway will authenticate with HBase using the supplied
        credential. No authentication will be performed by the Thrift gateway
        itself. All client access via the Thrift gateway will use the Thrift
        gateway's credential and have its privilege.
    </p></div><div class="section" title="8.1.5.&nbsp;Client-side Configuration for Secure Operation - REST Gateway"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4462"></a>8.1.5.&nbsp;Client-side Configuration for Secure Operation - REST Gateway</h3></div></div></div><p>
        Add the following to the <code class="code">hbase-site.xml</code> file for every REST gateway:
    </p><pre class="programlisting">    &lt;property&gt;
      &lt;name&gt;hbase.rest.keytab.file&lt;/name&gt;
      &lt;value&gt;$KEYTAB&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
      &lt;name&gt;hbase.rest.kerberos.principal&lt;/name&gt;
      &lt;value&gt;$USER/_HOST@HADOOP.LOCALDOMAIN&lt;/value&gt;
    &lt;/property&gt;
    </pre><p>
    </p><p>
        Substitute the appropriate credential and keytab for $USER and $KEYTAB
        respectively.
    </p><p>
        The REST gateway will authenticate with HBase using the supplied
        credential. No authentication will be performed by the REST gateway
        itself. All client access via the REST gateway will use the REST
        gateway's credential and have its privilege.
    </p><p>
        It should be possible for clients to authenticate with the HBase
        cluster through the REST gateway in a pass-through manner via SPEGNO
        HTTP authentication. This is future work.
    </p></div></div></div>
   <div class="section" title="8.2.&nbsp;Access Control"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.accesscontrol.configuration"></a>8.2.&nbsp;Access Control</h2></div></div></div><p>
        Newer releases of HBase (&gt;= 0.92) support optional access control
        list (ACL-) based protection of resources on a column family and/or
        table basis.
    </p><p>
        This describes how to set up Secure HBase for access control, with an
        example of granting and revoking user permission on table resources
        provided.
    </p><div class="section" title="8.2.1.&nbsp;Prerequisites"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4487"></a>8.2.1.&nbsp;Prerequisites</h3></div></div></div><p>
       You must configure HBase for secure operation. Refer to the section
       "Secure Client Access to HBase" and complete all of the steps described
       there.
    </p><p>
       You must also configure ZooKeeper for secure operation. Changes to ACLs
       are synchronized throughout the cluster using ZooKeeper. Secure 
       authentication to ZooKeeper must be enabled or otherwise it will be
       possible to subvert HBase access control via direct client access to
       ZooKeeper. Refer to the section on secure ZooKeeper configuration and
       complete all of the steps described there.
    </p></div><div class="section" title="8.2.2.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4494"></a>8.2.2.&nbsp;Overview</h3></div></div></div><p>
        With Secure RPC and Access Control enabled, client access to HBase is
        authenticated and user data is private unless access has been
        explicitly granted. Access to data can be granted at a table or per
        column family basis. 
    </p><p>
        However, the following items have been left out of the initial
        implementation for simplicity:
    </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Row-level or per value (cell): This would require broader changes for storing the ACLs inline with rows. It is a future goal.</p></li><li class="listitem"><p>Push down of file ownership to HDFS: HBase is not designed for the case where files may have different permissions than the HBase system principal. Pushing file ownership down into HDFS would necessitate changes to core code. Also, while HDFS file ownership would make applying quotas easy, and possibly make bulk imports more straightforward, it is not clear that it would offer a more secure setup.</p></li><li class="listitem"><p>HBase managed "roles" as collections of permissions: We will not model "roles" internally in HBase to begin with. We instead allow group names to be granted permissions, which allows external modeling of roles via group membership. Groups are created and manipulated externally to HBase, via the Hadoop group mapping service.</p></li></ol></div><p>
Access control mechanisms are mature and fairly standardized in the relational database world. The HBase implementation approximates current convention, but HBase has a simpler feature set than relational databases, especially in terms of client operations. We don't distinguish between an insert (new record) and update (of existing record), for example, as both collapse down into a Put. Accordingly, the important operations condense to four permissions: READ, WRITE, CREATE, and ADMIN.
    </p>Operation To Permission MappingPermissionOperationReadGetExistsScanWritePutDeleteLock/UnlockRowIncrementColumnValueCheckAndDelete/PutFlushCompactCreateCreateAlterDropAdminEnable/DisableSplitMajor CompactGrantRevokeShutdown<table id="d1934e4513"><thead></thead><tbody></tbody></table><p>
        Permissions can be granted in any of the following scopes, though
        CREATE and ADMIN permissions are effective only at table scope.
    </p><p>
    </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p>Table</p><p>
        </p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>Read: User can read from any column family in table</p></li><li class="listitem"><p>Write: User can write to any column family in table</p></li><li class="listitem"><p>Create: User can alter table attributes; add, alter, or drop column families; and drop the table.</p></li><li class="listitem"><p>Admin: User can alter table attributes; add, alter, or drop column families; and enable, disable, or drop the table. User can also trigger region (re)assignments or relocation.</p></li></ul></div><p>
        </p></li><li class="listitem"><p>Column Family</p><p>
        </p><div class="itemizedlist"><ul class="itemizedlist" type="circle"><li class="listitem"><p>Read: User can read from the column family</p></li><li class="listitem"><p>Write: User can write to the column family</p></li></ul></div><p>
        </p></li></ul></div><p>
    </p><p>
       There is also an implicit global scope for the superuser.
    </p><p>
       The superuser is a principal, specified in the HBase site configuration
       file, that has equivalent access to HBase as the 'root' user would on a
       UNIX derived system. Normally this is the principal that the HBase
       processes themselves authenticate as. Although future versions of HBase
       Access Control may support multiple superusers, the superuser privilege
       will always include the principal used to run the HMaster process. Only
       the superuser is allowed to create tables, switch the balancer on or
       off, or take other actions with global consequence. Furthermore, the
       superuser has an implicit grant of all permissions to all resources.
    </p><p>
       Tables have a new metadata attribute: OWNER, the user principal who owns
       the table. By default this will be set to the user principal who creates
       the table, though it may be changed at table creation time or during an
       alter operation by setting or changing the OWNER table attribute. Only a
       single user principal can own a table at a given time. A table owner will
       have all permissions over a given table. 
    </p></div><div class="section" title="8.2.3.&nbsp;Server-side Configuration for Access Control"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4651"></a>8.2.3.&nbsp;Server-side Configuration for Access Control</h3></div></div></div><p>
        Enable the AccessController coprocessor in the cluster configuration
        and restart HBase. The restart can be a rolling one. Complete the
        restart of all Master and RegionServer processes before setting up
        ACLs.
    </p><p>
        To enable the AccessController, modify the <code class="code">hbase-site.xml</code> file on every server machine in the cluster to look like:
    </p><pre class="programlisting">      &lt;property&gt;
        &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;
      &lt;/property&gt;
      &lt;property&gt;
      &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt;
        &lt;value&gt;org.apache.hadoop.hbase.security.token.TokenProvider,
        org.apache.hadoop.hbase.security.access.AccessController&lt;/value&gt;
      &lt;/property&gt;
    </pre></div><div class="section" title="8.2.4.&nbsp;Shell Enhancements for Access Control"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e4663"></a>8.2.4.&nbsp;Shell Enhancements for Access Control</h3></div></div></div><p>
The HBase shell has been extended to provide simple commands for editing and updating user permissions. The following commands have been added for access control list management: 
    </p>
    Grant
    <p>
    </p><pre class="programlisting">    grant &lt;user&gt; &lt;permissions&gt; &lt;table&gt; [ &lt;column family&gt; [ &lt;column qualifier&gt; ] ]
    </pre><p>
    </p><p>
    <code class="code">&lt;permissions&gt;</code> is zero or more letters from the set "RWCA": READ('R'), WRITE('W'), CREATE('C'), ADMIN('A'). 
    </p><p>
    Note: Grants and revocations of individual permissions on a resource are both accomplished using the <code class="code">grant</code> command. A separate <code class="code">revoke</code> command is also provided by the shell, but this is for fast revocation of all of a user's access rights to a given resource only.
    </p><p>
    Revoke
    </p><p>
    </p><pre class="programlisting">    revoke &lt;user&gt; &lt;table&gt; [ &lt;column family&gt; [ &lt;column qualifier&gt; ] ]
    </pre><p>
    </p><p>
    Alter
    </p><p>
    The <code class="code">alter</code> command has been extended to allow ownership assignment:
    </p><pre class="programlisting">      alter 'tablename', {OWNER =&gt; 'username'}
    </pre><p>
    </p><p>
    User Permission
    </p><p>
    The <code class="code">user_permission</code> command shows all access permissions for the current user for a given table:
    </p><pre class="programlisting">      user_permission &lt;table&gt;
    </pre><p>
    </p></div></div>
    
   <div class="chapter" title="Chapter 8. HBase 的 Schema 设计"><div class="titlepage"><div><div></div></div></div><div class="section" title="8.1.  Schema 创建"><div class="titlepage"><div><div></div></div></div></div></div><div class="chapter" title="Chapter 9. Metrics"><div class="titlepage"><div><div>
     <div class="titlepage">
       <div>
         <div>
           <h2 class="title"><a name="architecture"></a>Chapter&nbsp;9.&nbsp;架构</h2>
         </div>
       </div>
     </div>
     <div class="toc">
       <p><b>Table of Contents</b></p>
       <dl>
         <dt><span class="section"><a href="book.htm#client">9.1. 客户端</a></span></dt>
         <dd>
           <dl>
             <dt><span class="section"><a href="book.htm#client.connections">9.1.1. 连接</a></span></dt>
             <dt><span class="section"><a href="book.htm#client.writebuffer">9.1.2. 写缓冲和批量操作 </a></span></dt>
             <dt><span class="section"><a href="book.htm#client.filter">9.1.3. Filters</a></span></dt>
           </dl>
         </dd>
         <dt><span class="section"><a href="book.htm#daemons">9.2. Daemons</a></span></dt>
         <dd>
           <dl>
             <dt><span class="section"><a href="book.htm#master">9.2.1. Master</a></span></dt>
             <dt><span class="section"><a href="book.htm#regionserver.arch">9.2.2. RegionServer</a></span></dt>
           </dl>
         </dd>
         <dt><span class="section"><a href="book.htm#regions.arch">9.3. Regions</a></span></dt>
         <dd>
           <dl>
             <dt><span class="section"><a href="book.htm#arch.regions.size">9.3.1. Region大小</a></span></dt>
             <dt><span class="section"><a href="book.htm#d613e3126">9.3.2. Region Splits</a></span></dt>
             <dt><span class="section"><a href="book.htm#d613e3133">9.3.3. Region负载均衡</a></span></dt>
             <dt><span class="section"><a href="book.htm#store">9.3.4. Store</a></span></dt>
           </dl>
         </dd>
         <dt><span class="section"><a href="book.htm#wal">9.4. Write Ahead Log (WAL)</a></span></dt>
         <dd>
           <dl>
             <dt><span class="section"><a href="book.htm#purpose.wal">9.4.1. 目的</a></span></dt>
             <dt><span class="section"><a href="book.htm#wal_flush">9.4.2. WAL Flushing</a></span></dt>
             <dt><span class="section"><a href="book.htm#wal_splitting">9.4.3. WAL Splitting</a></span></dt>
           </dl>
         </dd>
       </dl>
     </div>
     <div class="section" title="9.1.&nbsp;Overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.overview"></a>9.1.&nbsp;Overview</h2></div></div></div><div class="section" title="9.1.1.&nbsp;NoSQL?"><div class="titlepage"><div><div><h3 class="title"><a name="arch.overview.nosql"></a>9.1.1.&nbsp;NoSQL?</h3></div></div></div><p>HBase is a type of "NoSQL" database.  "NoSQL" is a general term meaning that the database isn't an RDBMS which
	  supports SQL as it's primary access language, but there are many types of NoSQL databases:  BerkeleyDB is an 
	  example of a local NoSQL database, whereas HBase is very much a distributed database.  Technically speaking,
	  HBase is really more a "Data Store" than "Data Base" because it lacks many of the features you find in an RDBMS,
	  such as typed columns, secondary indexes, triggers, and advanced query languages, etc.
	  </p><p>However, HBase has many features which supports both linear and modular scaling.  HBase clusters expand
	  by adding RegionServers that are hosted on commodity class servers. If a cluster expands from 10 to 20 
	  RegionServers, for example, it doubles both in terms of storage and as well as processing capacity.
	  RDBMS can scale well, but only up to a point - specifically, the size of a single database server - and for the best
	  performance requires specialized hardware and storage devices.  HBase features of note are:
	        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Strongly consistent reads/writes:  HBase is not an "eventually consistent" DataStore.  This 
              makes it very suitable for tasks such as high-speed counter aggregation.  </li><li class="listitem">Automatic sharding:  HBase tables are distributed on the cluster via regions, and regions are
              automatically split and re-distributed as your data grows.</li><li class="listitem">Automatic RegionServer failover</li><li class="listitem">Hadoop/HDFS Integration:  HBase supports HDFS out of the box as it's distributed file system.</li><li class="listitem">MapReduce:  HBase supports massively parallelized processing via MapReduce for using HBase as both 
              source and sink.</li><li class="listitem">Java Client API:  HBase supports an easy to use Java API for programmatic access.</li><li class="listitem">Thrift/REST API:  HBase also supports Thrift and REST for non-Java front-ends.</li><li class="listitem">Block Cache and Bloom Filters:  HBase supports a Block Cache and Bloom Filters for high volume query optimization.</li><li class="listitem">Operational Management:  HBase provides build-in web-pages for operational insight as well as JMX metrics.</li></ul></div><p>
	  </p></div><div class="section" title="9.1.2.&nbsp;When Should I Use HBase?"><div class="titlepage"><div><div><h3 class="title"><a name="arch.overview.when"></a>9.1.2.&nbsp;When Should I Use HBase?</h3></div></div></div><p>HBase isn't suitable for every problem.</p><p>First, make sure you have enough data.  If you have hundreds of millions or billions of rows, then 
	            HBase is a good candidate.  If you only have a few thousand/million rows, then using a traditional RDBMS
	            might be a better choice due to the fact that all of your data might wind up on a single node (or two) and
	            the rest of the cluster may be sitting idle.
	          </p><p>Second, make sure you can live without all the extra features that an RDBMS provides (e.g., typed columns,
	          secondary indexes, transactions, advanced query languages, etc.)  An application built against an RDBMS cannot be
	          "ported" to HBase by simply changing a JDBC driver, for example.  Consider moving from an RDBMS to HBase as a
	          complete redesign as opposed to a port.	          
              </p><p>Third, make sure you have enough hardware.  Even HDFS doesn't do well with anything less than
                5 DataNodes (due to things such as HDFS block replication which has a default of 3), plus a NameNode.
                </p><p>HBase can run quite well stand-alone on a laptop - but this should be considered a development
                configuration only.
                </p></div><div class="section" title="9.1.3.&nbsp;What Is The Difference Between HBase and Hadoop/HDFS?"><div class="titlepage"><div><div><h3 class="title"><a name="arch.overview.hbasehdfs"></a>9.1.3.&nbsp;What Is The Difference Between HBase and Hadoop/HDFS?</h3></div></div></div><p><a class="link" href="http://hadoop.apache.org/hdfs/" target="_top">HDFS</a> is a distributed file system that is well suited for the storage of large files. 
          It's documentation states that it is not, however, a general purpose file system, and does not provide fast individual record lookups in files. 
          HBase, on the other hand, is built on top of HDFS and provides fast record lookups (and updates) for large tables. 
          This can sometimes be a point of conceptual confusion.  HBase internally puts your data in indexed "StoreFiles" that exist
          on HDFS for high-speed lookups.  See the <a class="xref" href="datamodel.html" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a> and the rest of this chapter for more information on how HBase achieves its goals.
         </p></div></div>
        
     <div class="section" title="9.2.&nbsp;Catalog Tables"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.catalog"></a>9.2.&nbsp;Catalog Tables</h2></div></div></div><p>The catalog tables -ROOT- and .META. exist as HBase tables.  They are are filtered out 
	  of the HBase shell's <code class="code">list</code> command, but they are in fact tables just like any other.
     </p><div class="section" title="9.2.1.&nbsp;ROOT"><div class="titlepage"><div><div><h3 class="title"><a name="arch.catalog.root"></a>9.2.1.&nbsp;ROOT</h3></div></div></div><p>-ROOT- keeps track of where the .META. table is.  The -ROOT- table structure is as follows: 
       </p><p>Key:   
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">.META. region key (<code class="code">.META.,,1</code>)</li></ul></div><p>
       </p><p>Values:   
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">info:regioninfo</code> (serialized <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HRegionInfo.html" target="_top">HRegionInfo</a>
               instance of .META.)</li><li class="listitem"><code class="code">info:server</code> (server:port of the RegionServer holding .META.)</li><li class="listitem"><code class="code">info:serverstartcode</code> (start-time of the RegionServer process holding .META.)</li></ul></div><p>
       </p></div><div class="section" title="9.2.2.&nbsp;META"><div class="titlepage"><div><div><h3 class="title"><a name="arch.catalog.meta"></a>9.2.2.&nbsp;META</h3></div></div></div><p>The .META. table keeps a list of all regions in the system. The .META. table structure is as follows: 
       </p><p>Key:   
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Region key of the format (<code class="code">[table],[region start key],[region id]</code>)</li></ul></div><p>
       </p><p>Values:   
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">info:regioninfo</code> (serialized <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HRegionInfo.html" target="_top">
              HRegionInfo</a> instance for this region)
              </li><li class="listitem"><code class="code">info:server</code> (server:port of the RegionServer containing this region)</li><li class="listitem"><code class="code">info:serverstartcode</code> (start-time of the RegionServer process containing this region)</li></ul></div><p>
       </p><p>When a table is in the process of splitting two other columns will be created, <code class="code">info:splitA</code> and <code class="code">info:splitB</code> 
       which represent the two daughter regions.  The values for these columns are also serialized HRegionInfo instances.
       After the region has been split eventually this row will be deleted.
       </p><p>Notes on HRegionInfo:  the empty key is used to denote table start and table end.  A region with an empty start key
       is the first region in a table.  If region has both an empty start and an empty end key, its the only region in the table
       </p><p>In the (hopefully unlikely) event that programmatic processing of catalog metadata is required, see the
         <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/util/Writables.html#getHRegionInfo%28byte[]%29" target="_top">Writables</a> utility.
       </p></div><div class="section" title="9.2.3.&nbsp;Startup Sequencing"><div class="titlepage"><div><div><h3 class="title"><a name="arch.catalog.startup"></a>9.2.3.&nbsp;Startup Sequencing</h3></div></div></div><p>The META location is set in ROOT first.  Then META is updated with server and startcode values.
	    </p><p>For information on region-RegionServer assignment, see <a class="xref" href="regions.arch.html#regions.arch.assignment" title="9.7.2.&nbsp;Region-RegionServer Assignment">Section&nbsp;9.7.2, “Region-RegionServer Assignment”</a>.
	    </p></div></div>
	    
     <div class="section" title="12.1. 客户端">
       <div class="titlepage">
         <div>
           <div>
             <h2 class="title" style="clear: both"><a name="client"></a>9.3.&nbsp;客户端</h2>
           </div>
         </div>
       </div>
       <p>Hbase客户端的 <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>类负责寻找相应的RegionServers来处理行。他是先查询 <code class="code">.META.</code> 和 <code class="code">-ROOT</code> 目录表。然后再确定region的位置。定位到所需要的区域后，客户端会<span class="emphasis"><em>直接</em></span> 去访问相应的region(不经过master)，发起读写请求。这些信息会缓存在客户端，这样就不用每发起一个请求就去查一下。如果一个region已经废弃(原因可能是master load balance或者RegionServer死了)，客户端就会重新进行这个步骤，决定要去访问的新的地址。 </p>
       <p> See <a href="book.htm#master.runtime" title="9.5.2. Runtime Impact">Section 9.5.2, “Runtime Impact”</a> for more information about the impact of the Master on HBase Client communication. </p>
       <p>管理集群操作是经由<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html" target="_top">HBaseAdmin</a>发起的 </p>
       <div class="section" title="12.1.1. 连接">
         <div class="titlepage">
           <div>
             <div>
               <h3 class="title"><a name="client.connections"></a>9.3.1.&nbsp;连接</h3>
             </div>
           </div>
         </div>
         <p>关于连接的配置信息，参见<a class="xref" href="book.htm#client_dependencies" title="3.7. 连接Hbase集群的客户端配置和依赖">Section&nbsp;3.7, “连接Hbase集群的客户端配置和依赖”</a>. </p>
         <p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>不是线程安全的。建议使用同一个<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HBaseConfiguration" target="_top">HBaseConfiguration</a>实例来创建HTable实例。这样可以共享ZooKeeper和socket实例。例如，最好这样做： </p>
         <pre class="programlisting">HBaseConfiguration conf = HBaseConfiguration.create();
HTable table1 = new HTable(conf, "myTable");
HTable table2 = new HTable(conf, "myTable");</pre>
         <p> 而不是这样： </p>
         <pre class="programlisting">HBaseConfiguration conf1 = HBaseConfiguration.create();
HTable table1 = new HTable(conf1, "myTable");
HBaseConfiguration conf2 = HBaseConfiguration.create();
HTable table2 = new HTable(conf2, "myTable");</pre>
         <p> 如果你想知道的更多的关于Hbase客户端connection的知识，可以参照： <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HConnectionManager.html" target="_top">HConnectionManager</a>. </p>
         <div>
           <div>
             <div>
               <h4>9.3.1.1. Connection Pooling</h4>
             </div>
           </div>
         </div>
         <p>For applications which require high-end multithreaded access (e.g., web-servers or application servers that may serve many application threads in a single JVM), see <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTablePool.html" target="_top">HTablePool</a>.</p>
<p>&nbsp;</p>
       </div>
       <div class="section" title="12.1.2. 写缓冲和批量操作">
         <div class="titlepage">
           <div>
             <div>
               <h3 class="title"><a name="client.writebuffer"></a>9.3.2.&nbsp;写缓冲和批量操作 </h3>
             </div>
           </div>
         </div>
         <p>若关闭了<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>中的 <a class="xref" href="book.htm#perf.hbase.client.autoflush" title="13.6.1. AutoFlush">Section&nbsp;11.7.4, “AutoFlush”</a>，<code class="classname">Put</code>操作会在写缓冲填满的时候向RegionServer发起请求。默认情况下，写缓冲是2MB.在Htable被废弃之前，要调用<code class="methodname">close()</code>, <code class="methodname">flushCommits()</code>操作，这样写缓冲就不会丢失。 </p>
         <p>要想更好的细粒度控制 <code class="classname">Put</code>或<code class="classname">Delete</code>的批量操作，可以参考Htable中的<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#batch%28java.util.List%29" target="_top">batch</a> 方法. </p>
         <div title="9.3.3. External Clients">
           <div>
             <div>
               <div>
                 <h3>9.3.3. External Clients</h3>
               </div>
             </div>
           </div>
           <p>Information on non-Java clients and custom protocols is covered in <a href="book.htm#external_apis" title="Chapter 10. External APIs">Chapter 10, <em>External APIs</em></a></p>
         </div>
         <div title="9.3.4. RowLocks">
           <div>
             <div>
               <div>
                 <h3><a name="client.rowlocks"></a>9.3.4. RowLocks</h3>
               </div>
             </div>
           </div>
           <p><a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#lockRow%28byte[]%29" target="_top">RowLocks</a> are still in the client API <em>however</em> they are discouraged because if not managed properly these can lock up the RegionServers.</p>
           <p>There is an oustanding ticket <a href="https://issues.apache.org/jira/browse/HBASE-2332" target="_top">HBASE-2332</a> to remove this feature from the client.</p>
         </div>
<p>&nbsp;</p>
       </div>
       
       <div class="section" title="9.4.&nbsp;Client Request Filters"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="client.filter"></a>9.4.&nbsp;Client Request Filters</h2></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Get.html" target="_top">Get</a> and <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a> instances can be
       optionally configured with <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/Filter.html" target="_top">filters</a> which are applied on the RegionServer. 
      </p><p>Filters can be confusing because there are many different types, and it is best to approach them by understanding the groups
      of Filter functionality.
      </p><div class="section" title="9.4.1.&nbsp;Structural"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.structural"></a>9.4.1.&nbsp;Structural</h3></div></div></div><p>Structural Filters contain other Filters.</p><div class="section" title="9.4.1.1.&nbsp;FilterList"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.structural.fl"></a>9.4.1.1.&nbsp;FilterList</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FilterList.html" target="_top">FilterList</a>
          represents a list of Filters with a relationship of <code class="code">FilterList.Operator.MUST_PASS_ALL</code> or 
          <code class="code">FilterList.Operator.MUST_PASS_ONE</code> between the Filters.  The following example shows an 'or' between two 
          Filters (checking for either 'my value' or 'my other value' on the same attribute).
</p><pre class="programlisting">FilterList list = new FilterList(FilterList.Operator.MUST_PASS_ONE);
SingleColumnValueFilter filter1 = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	Bytes.toBytes("my value")
	);
list.add(filter1);
SingleColumnValueFilter filter2 = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	Bytes.toBytes("my other value")
	);
list.add(filter2);
scan.setFilter(list);
</pre><p>
          </p></div></div><div class="section" title="9.4.2.&nbsp;Column Value"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.cv"></a>9.4.2.&nbsp;Column Value</h3></div></div></div><div class="section" title="9.4.2.1.&nbsp;SingleColumnValueFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cv.scvf"></a>9.4.2.1.&nbsp;SingleColumnValueFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SingleColumnValueFilter.html" target="_top">SingleColumnValueFilter</a>
          can be used to test column values for equivalence (<code class="code"><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/CompareFilter.CompareOp.html" target="_top">CompareOp.EQUAL</a>
          </code>), inequality (<code class="code">CompareOp.NOT_EQUAL</code>), or ranges
          (e.g., <code class="code">CompareOp.GREATER</code>).  The folowing is example of testing equivalence a column to a String value "my value"...
</p><pre class="programlisting">SingleColumnValueFilter filter = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	Bytes.toBytes("my value")
	);
scan.setFilter(filter);
</pre><p>
          </p></div></div><div class="section" title="9.4.3.&nbsp;Column Value Comparators"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.cvp"></a>9.4.3.&nbsp;Column Value Comparators</h3></div></div></div><p>There are several Comparator classes in the Filter package that deserve special mention.
        These Comparators are used in concert with other Filters, such as  <a class="xref" href="client.filter.html#client.filter.cv.scvf" title="9.4.2.1.&nbsp;SingleColumnValueFilter">Section&nbsp;9.4.2.1, “SingleColumnValueFilter”</a>.
        </p><div class="section" title="9.4.3.1.&nbsp;RegexStringComparator"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cvp.rcs"></a>9.4.3.1.&nbsp;RegexStringComparator</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/RegexStringComparator.html" target="_top">RegexStringComparator</a>
          supports regular expressions for value comparisons. 
</p><pre class="programlisting">RegexStringComparator comp = new RegexStringComparator("my.");   // any value that starts with 'my'
SingleColumnValueFilter filter = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	comp
	);
scan.setFilter(filter);
</pre><p>
          See the Oracle JavaDoc for <a class="link" href="http://download.oracle.com/javase/6/docs/api/java/util/regex/Pattern.html" target="_top">supported RegEx patterns in Java</a>. 
          </p></div><div class="section" title="9.4.3.2.&nbsp;SubstringComparator"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cvp.rcs"></a>9.4.3.2.&nbsp;SubstringComparator</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SubstringComparator.html" target="_top">SubstringComparator</a>
          can be used to determine if a given substring exists in a value.  The comparison is case-insensitive.
          </p><pre class="programlisting">SubstringComparator comp = new SubstringComparator("y val");   // looking for 'my value'
SingleColumnValueFilter filter = new SingleColumnValueFilter(
	cf,
	column,
	CompareOp.EQUAL,
	comp
	);
scan.setFilter(filter);
</pre></div><div class="section" title="9.4.3.3.&nbsp;BinaryPrefixComparator"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cvp.bfp"></a>9.4.3.3.&nbsp;BinaryPrefixComparator</h4></div></div></div><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryPrefixComparator.html" target="_top">BinaryPrefixComparator</a>.</p></div><div class="section" title="9.4.3.4.&nbsp;BinaryComparator"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.cvp.bc"></a>9.4.3.4.&nbsp;BinaryComparator</h4></div></div></div><p>See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/BinaryComparator.html" target="_top">BinaryComparator</a>.</p></div></div><div class="section" title="9.4.4.&nbsp;KeyValue Metadata"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.kvm"></a>9.4.4.&nbsp;KeyValue Metadata</h3></div></div></div><p>As HBase stores data internally as KeyValue pairs, KeyValue Metadata Filters evaluate the existence of keys (i.e., ColumnFamily:Column qualifiers)
        for a row, as opposed to values the previous section.
        </p><div class="section" title="9.4.4.1.&nbsp;FamilyFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.ff"></a>9.4.4.1.&nbsp;FamilyFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FamilyFilter.html" target="_top">FamilyFilter</a> can be used
          to filter on the ColumnFamily.  It is generally a better idea to select ColumnFamilies in the Scan than to do it with a Filter.</p></div><div class="section" title="9.4.4.2.&nbsp;QualifierFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.qf"></a>9.4.4.2.&nbsp;QualifierFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/QualifierFilter.html" target="_top">QualifierFilter</a> can be used
          to filter based on Column (aka Qualifier) name.
          </p></div><div class="section" title="9.4.4.3.&nbsp;ColumnPrefixFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.cpf"></a>9.4.4.3.&nbsp;ColumnPrefixFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnPrefixFilter.html" target="_top">ColumnPrefixFilter</a> can be used
          to filter based on the lead portion of Column (aka Qualifier) names.
          </p><p>A ColumnPrefixFilter seeks ahead to the first column matching the prefix in each row and for each involved column family. It can be used to efficiently
	 	  get a subset of the columns in very wide rows.
	      </p><p>Note: The same column qualifier can be used in different column families. This filter returns all matching columns.
          </p><p>Example: Find all columns in a row and family that start with "abc"
</p><pre class="programlisting">HTableInterface t = ...;
byte[] row = ...;
byte[] family = ...;
byte[] prefix = Bytes.toBytes("abc");
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new ColumnPrefixFilter(prefix);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
</pre><p>
</p></div><div class="section" title="9.4.4.4.&nbsp;MultipleColumnPrefixFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.mcpf"></a>9.4.4.4.&nbsp;MultipleColumnPrefixFilter</h4></div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/MultipleColumnPrefixFilter.html" target="_top">MultipleColumnPrefixFilter</a> behaves like ColumnPrefixFilter
          but allows specifying multiple prefixes.
          </p><p>Like ColumnPrefixFilter, MultipleColumnPrefixFilter efficiently seeks ahead to the first column matching the lowest prefix and also seeks past ranges of columns between prefixes.
	      It can be used to efficiently get discontinuous sets of columns from very wide rows.
		  </p><p>Example: Find all columns in a row and family that start with "abc" or "xyz"
</p><pre class="programlisting">HTableInterface t = ...;
byte[] row = ...;
byte[] family = ...;
byte[][] prefixes = new byte[][] {Bytes.toBytes("abc"), Bytes.toBytes("xyz")};
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new MultipleColumnPrefixFilter(prefixes);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
</pre><p>
</p></div><div class="section" title="9.4.4.5.&nbsp;ColumnRangeFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.kvm.crf "></a>9.4.4.5.&nbsp;ColumnRangeFilter</h4></div></div></div><p>A <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/ColumnRangeFilter.html" target="_top">ColumnRangeFilter</a> allows efficient intra row scanning.
            </p><p>A ColumnRangeFilter can seek ahead to the first matching column for each involved column family. It can be used to efficiently
			get a 'slice' of the columns of a very wide row.
			 i.e. you have a million columns in a row but you only want to look at columns bbbb-bbdd.
            </p><p>Note: The same column qualifier can be used in different column families. This filter returns all matching columns.
            </p><p>Example: Find all columns in a row and family between "bbbb" (inclusive) and "bbdd" (inclusive)
</p><pre class="programlisting">HTableInterface t = ...;
byte[] row = ...;
byte[] family = ...;
byte[] startColumn = Bytes.toBytes("bbbb");
byte[] endColumn = Bytes.toBytes("bbdd");
Scan scan = new Scan(row, row); // (optional) limit to one row
scan.addFamily(family); // (optional) limit to one family
Filter f = new ColumnRangeFilter(startColumn, true, endColumn, true);
scan.setFilter(f);
scan.setBatch(10); // set this if there could be many columns returned
ResultScanner rs = t.getScanner(scan);
for (Result r = rs.next(); r != null; r = rs.next()) {
  for (KeyValue kv : r.raw()) {
    // each kv represents a column
  }
}
rs.close();
</pre><p>
</p><p>Note:  Introduced in HBase 0.92</p></div></div><div class="section" title="9.4.5.&nbsp;RowKey"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.row"></a>9.4.5.&nbsp;RowKey</h3></div></div></div><div class="section" title="9.4.5.1.&nbsp;RowFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.row.rf"></a>9.4.5.1.&nbsp;RowFilter</h4></div></div></div><p>It is generally a better idea to use the startRow/stopRow methods on Scan for row selection, however 
          <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/RowFilter.html" target="_top">RowFilter</a> can also be used.</p></div></div><div class="section" title="9.4.6.&nbsp;Utility"><div class="titlepage"><div><div><h3 class="title"><a name="client.filter.utility"></a>9.4.6.&nbsp;Utility</h3></div></div></div><div class="section" title="9.4.6.1.&nbsp;FirstKeyOnlyFilter"><div class="titlepage"><div><div><h4 class="title"><a name="client.filter.utility.fkof"></a>9.4.6.1.&nbsp;FirstKeyOnlyFilter</h4></div></div></div><p>This is primarily used for rowcount jobs.  
          See <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html" target="_top">FirstKeyOnlyFilter</a>.</p></div></div></div>
       
       <div class="section" title="9.5.&nbsp;Master"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="master"></a>9.5.&nbsp;Master</h2></div></div></div><p><code class="code">HMaster</code> is the implementation of the Master Server.  The Master server
       is responsible for monitoring all RegionServer instances in the cluster, and is
       the interface for all metadata changes.  In a distributed cluster, the Master typically runs on the <a class="xref" href="arch.hdfs.html#arch.hdfs.nn" title="9.9.1.&nbsp;NameNode">Section&nbsp;9.9.1, “NameNode”</a>.
       </p><div class="section" title="9.5.1.&nbsp;Startup Behavior"><div class="titlepage"><div><div><h3 class="title"><a name="master.startup"></a>9.5.1.&nbsp;Startup Behavior</h3></div></div></div><p>If run in a multi-Master environment, all Masters compete to run the cluster.  If the active
         Master loses it's lease in ZooKeeper (or the Master shuts down), then then the remaining Masters jostle to 
         take over the Master role.
         </p></div><div class="section" title="9.5.2.&nbsp;Runtime Impact"><div class="titlepage"><div><div><h3 class="title"><a name="master.runtime"></a>9.5.2.&nbsp;Runtime Impact</h3></div></div></div><p>A common dist-list question is what happens to an HBase cluster when the Master goes down.  Because the
         HBase client talks directly to the RegionServers, the cluster can still function in a "steady 
         state."  Additionally, per <a class="xref" href="arch.catalog.html" title="9.2.&nbsp;Catalog Tables">Section&nbsp;9.2, “Catalog Tables”</a> ROOT and META exist as HBase tables (i.e., are
         not resident in the Master).  However, the Master controls critical functions such as RegionServer failover and 
         completing region splits.  So while the cluster can still run <span class="emphasis"><em>for a time</em></span> without the Master, 
         the Master should be restarted as soon as possible.     
         </p></div><div class="section" title="9.5.3.&nbsp;Interface"><div class="titlepage"><div><div><h3 class="title"><a name="master.api"></a>9.5.3.&nbsp;Interface</h3></div></div></div><p>The methods exposed by <code class="code">HMasterInterface</code> are primarily metadata-oriented methods:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Table (createTable, modifyTable, removeTable, enable, disable)
            </li><li class="listitem">ColumnFamily (addColumn, modifyColumn, removeColumn) 
            </li><li class="listitem">Region (move, assign, unassign)
            </li></ul></div><p>
         For example, when the <code class="code">HBaseAdmin</code> method <code class="code">disableTable</code> is invoked, it is serviced by the Master server. 
         </p></div><div class="section" title="9.5.4.&nbsp;Processes"><div class="titlepage"><div><div><h3 class="title"><a name="master.processes"></a>9.5.4.&nbsp;Processes</h3></div></div></div><p>The Master runs several background threads:
         </p><div class="section" title="9.5.4.1.&nbsp;LoadBalancer"><div class="titlepage"><div><div><h4 class="title"><a name="master.processes.loadbalancer"></a>9.5.4.1.&nbsp;LoadBalancer</h4></div></div></div><p>Periodically, and when there are not any regions in transition,
             a load balancer will run and move regions around to balance cluster load.
             See <a class="xref" href="important_configurations.html#balancer_config" title="2.8.3.1.&nbsp;Balancer">Section&nbsp;2.8.3.1, “Balancer”</a> for configuring this property.</p><p>See <a class="xref" href="regions.arch.html#regions.arch.assignment" title="9.7.2.&nbsp;Region-RegionServer Assignment">Section&nbsp;9.7.2, “Region-RegionServer Assignment”</a> for more information on region assignment.
             </p></div><div class="section" title="9.5.4.2.&nbsp;CatalogJanitor"><div class="titlepage"><div><div><h4 class="title"><a name="master.processes.catalog"></a>9.5.4.2.&nbsp;CatalogJanitor</h4></div></div></div><p>Periodically checks and cleans up the .META. table.  See <a class="xref" href="arch.catalog.html#arch.catalog.meta" title="9.2.2.&nbsp;META">Section&nbsp;9.2.2, “META”</a> for more information on META.</p></div></div></div>
             
        
        <div class="section" title="9.6.&nbsp;RegionServer"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="regionserver.arch"></a>9.6.&nbsp;RegionServer</h2></div></div></div><p><code class="code">HRegionServer</code> is the RegionServer implementation.  It is responsible for serving and managing regions.
       In a distributed cluster, a RegionServer runs on a <a class="xref" href="arch.hdfs.html#arch.hdfs.dn" title="9.9.2.&nbsp;DataNode">Section&nbsp;9.9.2, “DataNode”</a>.  
       </p><div class="section" title="9.6.1.&nbsp;Interface"><div class="titlepage"><div><div><h3 class="title"><a name="regionserver.arch.api"></a>9.6.1.&nbsp;Interface</h3></div></div></div><p>The methods exposed by <code class="code">HRegionRegionInterface</code> contain both data-oriented and region-maintenance methods:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Data (get, put, delete, next, etc.)
            </li><li class="listitem">Region (splitRegion, compactRegion, etc.)  
            </li></ul></div><p>
         For example, when the <code class="code">HBaseAdmin</code> method <code class="code">majorCompact</code> is invoked on a table, the client is actually iterating through
         all regions for the specified table and requesting a major compaction directly to each region. 
         </p></div><div class="section" title="9.6.2.&nbsp;Processes"><div class="titlepage"><div><div><h3 class="title"><a name="regionserver.arch.processes"></a>9.6.2.&nbsp;Processes</h3></div></div></div><p>The RegionServer runs a variety of background threads:</p><div class="section" title="9.6.2.1.&nbsp;CompactSplitThread"><div class="titlepage"><div><div><h4 class="title"><a name="regionserver.arch.processes.compactsplit"></a>9.6.2.1.&nbsp;CompactSplitThread</h4></div></div></div><p>Checks for splits and handle minor compactions.</p></div><div class="section" title="9.6.2.2.&nbsp;MajorCompactionChecker"><div class="titlepage"><div><div><h4 class="title"><a name="regionserver.arch.processes.majorcompact"></a>9.6.2.2.&nbsp;MajorCompactionChecker</h4></div></div></div><p>Checks for major compactions.</p></div><div class="section" title="9.6.2.3.&nbsp;MemStoreFlusher"><div class="titlepage"><div><div><h4 class="title"><a name="regionserver.arch.processes.memstore"></a>9.6.2.3.&nbsp;MemStoreFlusher</h4></div></div></div><p>Periodically flushes in-memory writes in the MemStore to StoreFiles.</p></div><div class="section" title="9.6.2.4.&nbsp;LogRoller"><div class="titlepage"><div><div><h4 class="title"><a name="regionserver.arch.processes.log"></a>9.6.2.4.&nbsp;LogRoller</h4></div></div></div><p>Periodically checks the RegionServer's HLog.</p></div></div><div class="section" title="9.6.3.&nbsp;Coprocessors"><div class="titlepage"><div><div><h3 class="title"><a name="coprocessors"></a>9.6.3.&nbsp;Coprocessors</h3></div></div></div><p>Coprocessors were added in 0.92.  There is a thorough <a class="link" href="https://blogs.apache.org/hbase/entry/coprocessor_introduction" target="_top">Blog Overview of CoProcessors</a>
         posted.  Documentation will eventually move to this reference guide, but the blog is the most current information available at this time.
         </p></div><div class="section" title="9.6.4.&nbsp;Block Cache"><div class="titlepage"><div><div><h3 class="title"><a name="block.cache"></a>9.6.4.&nbsp;Block Cache</h3></div></div></div><div class="section" title="9.6.4.1.&nbsp;Design"><div class="titlepage"><div><div><h4 class="title"><a name="block.cache.design"></a>9.6.4.1.&nbsp;Design</h4></div></div></div><p>The Block Cache is an LRU cache that contains three levels of block priority to allow for scan-resistance and in-memory ColumnFamilies:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Single access priority: The first time a block is loaded from HDFS it normally has this priority and it will be part of the first group to be considered
            during evictions. The advantage is that scanned blocks are more likely to get evicted than blocks that are getting more usage.
            </li><li class="listitem">Mutli access priority: If a block in the previous priority group is accessed again, it upgrades to this priority. It is thus part of the second group
            considered during evictions.
            </li><li class="listitem">In-memory access priority: If the block's family was configured to be "in-memory", it will be part of this priority disregarding the number of times it
            was accessed. Catalog tables are configured like this. This group is the last one considered during evictions.
            </li></ul></div><p>
        For more information, see the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/io/hfile/LruBlockCache.html" target="_top">LruBlockCache source</a>
        </p></div><div class="section" title="9.6.4.2.&nbsp;Usage"><div class="titlepage"><div><div><h4 class="title"><a name="block.cache.usage"></a>9.6.4.2.&nbsp;Usage</h4></div></div></div><p>Block caching is enabled by default for all the user tables which means that any read operation will load the LRU cache. This might be good for a large number of use cases,
        but further tunings are usually required in order to achieve better performance. An important concept is the
        <a class="link" href="http://en.wikipedia.org/wiki/Working_set_size" target="_top">working set size</a>, or WSS, which is: "the amount of memory needed to compute the answer to a problem".
        For a website, this would be the data that's needed to answer the queries over a short amount of time.
        </p><p>The way to calculate how much memory is available in HBase for caching is:
        </p><pre class="programlisting">            number of region servers * heap size * hfile.block.cache.size * 0.85
        </pre><p>The default value for the block cache is 0.25 which represents 25% of the available heap. The last value (85%) is the default acceptable loading factor in the LRU cache after
        which eviction is started. The reason it is included in this equation is that it would be unrealistic to say that it is possible to use 100% of the available memory since this would
        make the process blocking from the point where it loads new blocks. Here are some examples:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">One region server with the default heap size (1GB) and the default block cache size will have 217MB of block cache available.
            </li><li class="listitem">20 region servers with the heap size set to 8GB and a default block cache size will have 34GB of block cache.
            </li><li class="listitem">100 region servers with the heap size set to 24GB and a block cache size of 0.5 will have about 1TB of block cache.
            </li></ul></div><p>Your data isn't the only resident of the block cache, here are others that you may have to take into account:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Catalog tables: The -ROOT- and .META. tables are forced into the block cache and have the in-memory priority which means that they are harder to evict. The former never uses
            more than a few hundreds of bytes while the latter can occupy a few MBs (depending on the number of regions).
            </li><li class="listitem">HFiles indexes: HFile is the file format that HBase uses to store data in HDFS and it contains a multi-layered index in order seek to the data without having to read the whole file.
            The size of those indexes is a factor of the block size (64KB by default), the size of your keys and the amount of data you are storing. For big data sets it's not unusual to see numbers around
            1GB per region server, although not all of it will be in cache because the LRU will evict indexes that aren't used.
            </li><li class="listitem">Keys: Taking into account only the values that are being stored is missing half the picture since every value is stored along with its keys
            (row key, family, qualifier, and timestamp). See <a class="xref" href="rowkey.design.html#keysize" title="6.3.2.&nbsp;Try to minimize row and column sizes">Section&nbsp;6.3.2, “Try to minimize row and column sizes”</a>.
            </li><li class="listitem">Bloom filters: Just like the HFile indexes, those data structures (when enabled) are stored in the LRU.
            </li></ul></div><p>Currently the recommended way to measure HFile indexes and bloom filters sizes is to look at the region server web UI and checkout the relevant metrics. For keys,
        sampling can be done by using the HFile command line tool and look for the average key size metric.
        </p><p>It's generally bad to use block caching when the WSS doesn't fit in memory. This is the case when you have for example 40GB available across all your region servers' block caches
        but you need to process 1TB of data. One of the reasons is that the churn generated by the evictions will trigger more garbage collections unnecessarily. Here are two use cases:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Fully random reading pattern: This is a case where you almost never access the same row twice within a short amount of time such that the chance of hitting a cached block is close
            to 0. Setting block caching on such a table is a waste of memory and CPU cycles, more so that it will generate more garbage to pick up by the JVM. For more information on monitoring GC,
            see <a class="xref" href="trouble.log.html#trouble.log.gc" title="12.2.3.&nbsp;JVM Garbage Collection Logs">Section&nbsp;12.2.3, “JVM Garbage Collection Logs”</a>.
            </li><li class="listitem">Mapping a table: In a typical MapReduce job that takes a table in input, every row will be read only once so there's no need to put them into the block cache. The Scan object has
            the option of turning this off via the setCaching method (set it to false). You can still keep block caching turned on on this table if you need fast random read access. An example would be
            counting the number of rows in a table that serves live traffic, caching every block of that table would create massive churn and would surely evict data that's currently in use.
            </li></ul></div></div></div><div class="section" title="9.6.5.&nbsp;Write Ahead Log (WAL)"><div class="titlepage"><div><div><h3 class="title"><a name="wal"></a>9.6.5.&nbsp;Write Ahead Log (WAL)</h3></div></div></div><div class="section" title="9.6.5.1.&nbsp;Purpose"><div class="titlepage"><div><div><h4 class="title"><a name="purpose.wal"></a>9.6.5.1.&nbsp;Purpose</h4></div></div></div>
            <p>每个RegionServer会将更新(Puts, Deletes) 先记录到Write Ahead Log中(WAL)，然后将其更新在<a class="xref" href="book.htm#store" title="12.3.4. Store">Section&nbsp;9.7.5, “Store”</a>的<a class="xref" href="book.htm#store.memstore" title="12.3.4.1. MemStore">Section&nbsp;9.7.5.1, “MemStore”</a>里面。这样就保证了Hbase的写的可靠性。如果没有WAL,当RegionServer宕掉的时候，MemStore还没有flush，StoreFile还没有保存，数据就会丢失。<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/wal/HLog.html" target="_top">HLog</a> 是Hbase的一个WAL实现，一个RegionServer有一个HLog实例。 </p>
WAL 保存在HDFS 的 <code class="filename">/hbase/.logs/</code> 里面，每个region一个文件。
<p> 要想知道更多的信息，可以访问维基百科 <a class="link" href="http://en.wikipedia.org/wiki/Write-ahead_logging" target="_top">Write-Ahead Log</a> 的文章. </p>
<p>&nbsp;</p></div><div class="section" title="9.6.5.2.&nbsp;WAL Flushing"><div class="titlepage"><div><div><h4 class="title"><a name="wal_flush"></a>9.6.5.2.&nbsp;WAL Flushing</h4></div></div></div><p>TODO (describe).
          </p></div><div class="section" title="9.6.5.3.&nbsp;WAL Splitting"><div class="titlepage"><div><div><h4 class="title"><a name="wal_splitting"></a>9.6.5.3.&nbsp;WAL Splitting</h4></div></div></div><div class="section" title="9.6.5.3.1.&nbsp;How edits are recovered from a crashed RegionServer"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e5412"></a>9.6.5.3.1.&nbsp;当RegionServer宕掉的时候，如何恢复</h5>
          </div></div></div><p>When a RegionServer crashes, it will lose its ephemeral lease in
         ZooKeeper...TODO</p></div><div class="section" title="9.6.5.3.2.&nbsp;hbase.hlog.split.skip.errors"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e5417"></a>9.6.5.3.2.&nbsp;<code class="varname">hbase.hlog.split.skip.errors</code></h5></div></div></div>
           <p>默认设置为 <code class="constant">true</code>,在split执行中发生的任何错误会被记录，有问题的WAL会被移动到Hbase <code class="varname">rootdir</code>目录下的<code class="filename">.corrupt</code>目录，接着进行处理。如果设置为 <code class="constant">false</code>，异常会被抛出，split会记录错误。<sup>[<a name="d613e3262" href="book.htm#ftn.d1934e5435" class="footnote">23</a>]</sup></p>
         </div><div class="section" title="9.6.5.3.3.&nbsp;How EOFExceptions are treated when splitting a crashed RegionServers' WALs"><div class="titlepage"><div>
           <div>
             <h5 class="title"><a name="d1934e5441"></a>9.6.5.3.3.&nbsp;如何处理一个发生在当RegionServers' WALs 分割时候的EOFExceptions异常</h5>
           </div></div></div><p>如果我们在分割日志的时候发生EOF,就是<code class="varname">hbase.hlog.split.skip.errors</code>设置为 <code class="constant">false</code>，我们也会进行处理。一个EOF会发生在一行一行读取Log，但是Log中最后一行似乎只写了一半就停止了。如果在处理过程中发生了EOF，我们还会继续处理，除非这个文件是要处理的最后一个文件。<sup>[<a name="d1934e5452" href="#ftn.d1934e5452" class="footnote">24</a>]</sup></p></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d1934e5435" href="#d1934e5435" class="para">23</a>] </sup>See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-2958" target="_top">HBASE-2958
            When hbase.hlog.split.skip.errors is set to false, we fail the
            split but thats it</a>. We need to do more than just fail split
            if this flag is set.</p></div><div class="footnote"><p><sup>[<a id="ftn.d1934e5452" href="#d1934e5452" class="para">24</a>] </sup>要想知道背景知识, 参见<a class="link" href="https://issues.apache.org/jira/browse/HBASE-2643" target="_top">HBASE-2643
            Figure how to deal with eof splitting logs</a></p></div></div></div>
            
        <div class="section" title="9.7.&nbsp;Regions"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="regions.arch"></a>9.7.&nbsp;Regions</h2></div></div></div><p>Regions are the basic element of availability and
     distribution for tables, and are comprised of a Store per Column Family. The heirarchy of objects 
     is as follows:
</p><pre class="programlisting"><code class="filename">Table</code>       (HBase table)      
    <code class="filename">Region</code>       (Regions for the table)
         <code class="filename">Store</code>          (Store per ColumnFamily for each Region for the table)
              <code class="filename">MemStore</code>           (MemStore for each Store for each Region for the table)
              <code class="filename">StoreFile</code>          (StoreFiles for each Store for each Region for the table)
                    <code class="filename">Block</code>             (Blocks within a StoreFile within a Store for each Region for the table)
 </pre><p>
     For a description of what HBase files look like when written to HDFS, see <a class="xref" href="trouble.namenode.html#trouble.namenode.hbase.objects" title="12.7.2.&nbsp;Browsing HDFS for HBase Objects">Section&nbsp;12.7.2, “Browsing HDFS for HBase Objects”</a>.
            </p><div class="section" title="9.7.1.&nbsp;Region Size"><div class="titlepage"><div><div>
              <h3 class="title"><a name="arch.regions.size"></a>9.7.1.&nbsp;Region 大小</h3></div></div></div><p>Region的大小是一个棘手的问题，需要考量如下几个因素。 </p>
              <div class="itemizedlist">
                <ul class="itemizedlist" type="disc">
                  <li class="listitem">
                    <p>Regions是可用性和分布式的最基本单位</p>
                  </li>
                  <li class="listitem">
                    <p>HBase通过将region切分在许多机器上实现分布式。也就是说，你如果有16GB的数据，只分了2个region， 你却有20台机器，有18台就浪费了。</p>
                  </li>
                  <li class="listitem">
                    <p>region数目太多就会造成性能下降，现在比以前好多了。但是对于同样大小的数据，700个region比3000个要好。</p>
                  </li>
                  <li class="listitem">
                    <p>region数目太少就会妨碍可扩展性，降低并行能力。有的时候导致压力不够分散。这就是为什么，你向一个10节点的Hbase集群导入200MB的数据，大部分的节点是idle的。</p>
                  </li>
                  <li class="listitem">
                    <p>RegionServer中1个region和10个region索引需要的内存量没有太多的差别。</p>
                  </li>
                </ul>
              </div>
              <p>最好是使用默认的配置，可以把热的表配小一点(或者受到split热点的region把压力分散到集群中)。如果你的cell的大小比较大(100KB或更大)，就可以把region的大小调到1GB。</p>
              <p>See <a class="xref" href="important_configurations.html#bigger.regions" title="2.8.2.6.&nbsp;Bigger Regions">Section&nbsp;2.8.2.6, “Bigger Regions”</a> for more information on configuration.
      </p></div><div class="section" title="9.7.2.&nbsp;Region-RegionServer Assignment"><div class="titlepage"><div><div><h3 class="title"><a name="regions.arch.assignment"></a>9.7.2.&nbsp;Region-RegionServer Assignment</h3></div></div></div><p>This section describes how Regions are assigned to RegionServers.
         </p><div class="section" title="9.7.2.1.&nbsp;Startup"><div class="titlepage"><div><div><h4 class="title"><a name="regions.arch.assignment.startup"></a>9.7.2.1.&nbsp;Startup</h4></div></div></div><p>When HBase starts regions are assigned as follows (short version):
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">The Master invokes the <code class="code">AssignmentManager</code> upon startup.
              </li><li class="listitem">The <code class="code">AssignmentManager</code> looks at the existing region assignments in META.
              </li><li class="listitem">If the region assignment is still valid (i.e., if the RegionServer is still online)
                then the assignment is kept.
              </li><li class="listitem">If the assignment is invalid, then the <code class="code">LoadBalancerFactory</code> is invoked to assign the 
                region.  The <code class="code">DefaultLoadBalancer</code> will randomly assign the region to a RegionServer.
              </li><li class="listitem">META is updated with the RegionServer assignment (if needed) and the RegionServer start codes 
              (start time of the RegionServer process) upon region opening by the RegionServer.
              </li></ol></div><p>
          </p></div><div class="section" title="9.7.2.2.&nbsp;Failover"><div class="titlepage"><div><div><h4 class="title"><a name="regions.arch.assignment.failover"></a>9.7.2.2.&nbsp;Failover</h4></div></div></div><p>When a RegionServer fails (short version):
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">The regions immediately become unavailable because the RegionServer is down.
              </li><li class="listitem">The Master will detect that the RegionServer has failed.
              </li><li class="listitem">The region assignments will be considered invalid and will be re-assigned just
                like the startup sequence.    
              </li></ol></div><p>
           </p></div><div class="section" title="9.7.2.3.&nbsp;Region Load Balancing"><div class="titlepage"><div><div><h4 class="title"><a name="regions.arch.balancer"></a>9.7.2.3.&nbsp;Region Load Balancing</h4></div></div></div><p>
          Regions can be periodically moved by the <a class="xref" href="master.html#master.processes.loadbalancer" title="9.5.4.1.&nbsp;LoadBalancer">Section&nbsp;9.5.4.1, “LoadBalancer”</a>.
          </p></div></div><div class="section" title="9.7.3.&nbsp;Region-RegionServer Locality"><div class="titlepage"><div><div><h3 class="title"><a name="regions.arch.locality"></a>9.7.3.&nbsp;Region-RegionServer Locality</h3></div></div></div><p>Over time, Region-RegionServer locality is achieved via HDFS block replication.
          The HDFS client does the following by default when choosing locations to write replicas:
           </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">First replica is written to local node
             </li><li class="listitem">Second replica is written to another node in same rack
             </li><li class="listitem">Third replica is written to a node in another rack (if sufficient nodes)
             </li></ol></div><p>
          Thus, HBase eventually achieves locality for a region after a flush or a compaction. 
          In a RegionServer failover situation a RegionServer may be assigned regions with non-local
          StoreFiles (because none of the replicas are local), however as new data is written
          in the region, or the table is compacted and StoreFiles are re-written, they will become "local"
          to the RegionServer.  
        </p><p>For more information, see <a class="link" href="http://hadoop.apache.org/common/docs/r0.20.205.0/hdfs_design.html#Replica+Placement%3A+The+First+Baby+Steps" target="_top">HDFS Design on Replica Placement</a>
        and also Lars George's blog on <a class="link" href="http://www.larsgeorge.com/2010/05/hbase-file-locality-in-hdfs.html" target="_top">HBase and HDFS locality</a>.      
        </p></div><div class="section" title="9.7.4.&nbsp;Region Splits"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e5583"></a>9.7.4.&nbsp;Region Splits</h3></div></div></div>
        <p>RegionServer的Splits操作是不可见的，因为Master不会参与其中。RegionServer切割region的步骤是，先将该region下线，然后切割，将其子region加入到META元信息中，再将他们加入到原本的RegionServer中，最后汇报Master.参见<a class="xref" href="book.htm#disable.splitting" title="3.6.6. 管理 Splitting">Section&nbsp;2.8.2.7, “管理 Splitting”</a>来手动管理切割操作(以及为何这么做)。</p><div class="section" title="9.7.4.1.&nbsp;Custom Split Policies"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e5590"></a>9.7.4.1.&nbsp;Custom Split Policies</h4></div></div></div><p>The default split policy can be overwritten using a custom <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/RegionSplitPolicy.html" target="_top">RegionSplitPolicy</a> (HBase 0.94+).
          Typically a custom split policy should extend HBase's default split policy: <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/ConstantSizeRegionSplitPolicy.html" target="_top">ConstantSizeRegionSplitPolicy</a>.
          </p><p>The policy can set globally through the HBaseConfiguration used or on a per table basis: 
</p><pre class="programlisting">HTableDescriptor myHtd = ...;
myHtd.setValue(HTableDescriptor.SPLIT_POLICY, MyCustomSplitPolicy.class.getName());
</pre><p>
          </p></div></div><div class="section" title="9.7.5.&nbsp;Store"><div class="titlepage"><div><div><h3 class="title"><a name="store"></a>9.7.5.&nbsp;Store</h3></div></div></div><p>一个Store包含了一个MemStore和若干个StoreFile(HFile).一个Store可以定位到一个column family中的一个region.</p><div class="section" title="9.7.5.1.&nbsp;MemStore"><div class="titlepage"><div><div><h4 class="title"><a name="store.memstore"></a>9.7.5.1.&nbsp;MemStore</h4></div></div></div><p>MemStores是Store中的内存Store,可以进行修改操作。修改的内容是KeyValues。当flush的是，现有的memstore会生成快照，然后清空。在执行快照的时候，Hbase会继续接收修改操作，保存在memstore外面，直到快照完成。</p></div><div class="section" title="9.7.5.2.&nbsp;StoreFile (HFile)"><div class="titlepage"><div><div><h4 class="title"><a name="hfile"></a>9.7.5.2.&nbsp;StoreFile (HFile)</h4></div></div></div><p>StoreFiles are where your data lives.
      </p><div class="section" title="9.7.5.2.1.&nbsp;HFile Format"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e5621"></a>9.7.5.2.1.&nbsp;HFile Format</h5></div></div></div><p><span class="emphasis"><em>hfile</em></span>文件格式是基于<a class="link" href="http://labs.google.com/papers/bigtable.html" target="_top">BigTable [2006]</a>论文中的SSTable。构建在Hadoop的<a class="link" href="http://hadoop.apache.org/common/docs/current/api/org/apache/hadoop/io/file/tfile/TFile.html" target="_top">tfile</a>上面(直接使用了tfile的单元测试和压缩工具)。 Schubert Zhang's的博客<a class="link" href="http://cloudepr.blogspot.com/2009/09/hfile-block-indexed-file-format-to.html" target="_top">HFile: A Block-Indexed File Format to Store Sorted Key-Value Pairs</a>详细介绍了Hbases的hfile。Matteo Bertozzi也做了详细的介绍<a class="link" href="http://th30z.blogspot.com/2011/02/hbase-io-hfile.html?spref=tw" target="_top">HBase I/O: HFile</a>。 </p>
      <p>For more information, see the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/io/hfile/HFile.html" target="_top">HFile source code</a>.
          Also see <a class="xref" href="book.htm#hfilev2" title="Appendix&nbsp;E.&nbsp;HFile format version 2">Appendix&nbsp;E, <i>HFile format version 2</i></a> for information about the HFile v2 format that was included in 0.92.
          </p></div><div class="section" title="9.7.5.2.2.&nbsp;HFile Tool"><div class="titlepage"><div><div>
            <h5 class="title"><a name="hfile_tool"></a>9.7.5.2.2.&nbsp;HFile 工具</h5></div></div></div><p>要想看到hfile内容的文本化版本，你可以使用<code class="classname">org.apache.hadoop.hbase.io.hfile.HFile </code>工具。可以这样用：</p>
            <pre class="programlisting"><code class="code">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile </code> </pre>
            <p>例如，你想看文件 <code class="filename">hdfs://10.81.47.41:9000/hbase/TEST/1418428042/DSMP/4759508618286845475</code>的内容,
              就执行如下的命令:</p>
            <pre class="programlisting"> <code class="code">$ ${HBASE_HOME}/bin/hbase org.apache.hadoop.hbase.io.hfile.HFile -v -f hdfs://10.81.47.41:9000/hbase/TEST/1418428042/DSMP/4759508618286845475 </code> </pre>
            <p>如果你没有输入-v,就仅仅能看到一个hfile的汇总信息。其他功能的用法可以看<code class="classname">HFile</code>的文档。</p>
          </div><div class="section" title="9.7.5.2.3.&nbsp;StoreFile Directory Structure on HDFS"><div class="titlepage"><div><div><h5 class="title"><a name="store.file.dir"></a>9.7.5.2.3.&nbsp;StoreFile Directory Structure on HDFS</h5></div></div></div><p>For more information of what StoreFiles look like on HDFS with respect to the directory structure, see <a class="xref" href="trouble.namenode.html#trouble.namenode.hbase.objects" title="12.7.2.&nbsp;Browsing HDFS for HBase Objects">Section&nbsp;12.7.2, “Browsing HDFS for HBase Objects”</a>.
        </p></div></div><div class="section" title="9.7.5.3.&nbsp;Blocks"><div class="titlepage"><div><div><h4 class="title"><a name="hfile.blocks"></a>9.7.5.3.&nbsp;Blocks</h4></div></div></div><p>StoreFiles are composed of blocks.  The blocksize is configured on a per-ColumnFamily basis.
        </p><p>Compression happens at the block level within StoreFiles.  For more information on compression, see <a class="xref" href="compression.html" title="Appendix&nbsp;C.&nbsp;Compression In HBase">Appendix&nbsp;C, <i>Compression In HBase</i></a>.
        </p><p>For more information on blocks, see the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/io/hfile/HFileBlock.html" target="_top">HFileBlock source code</a>.
        </p></div><div class="section" title="9.7.5.4.&nbsp;KeyValue"><div class="titlepage"><div><div><h4 class="title"><a name="keyvalue"></a>9.7.5.4.&nbsp;KeyValue</h4></div></div></div><p>The KeyValue class is the heart of data storage in HBase.  KeyValue wraps a byte array and takes offsets and lengths into passed array
         at where to start interpreting the content as KeyValue.  
        </p><p>The KeyValue format inside a byte array is:
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">keylength</li><li class="listitem">valuelength</li><li class="listitem">key</li><li class="listitem">value</li></ul></div><p>
        </p><p>The Key is further decomposed as:
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">rowlength</li><li class="listitem">row (i.e., the rowkey)</li><li class="listitem">columnfamilylength</li><li class="listitem">columnfamily</li><li class="listitem">columnqualifier</li><li class="listitem">timestamp</li><li class="listitem">keytype (e.g., Put, Delete, DeleteColumn, DeleteFamily)</li></ul></div><p>
        </p><p>KeyValue instances are <span class="emphasis"><em>not</em></span> split across blocks.
         For example, if there is an 8 MB KeyValue, even if the block-size is 64kb this KeyValue will be read
         in as a coherent block.  For more information, see the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/KeyValue.html" target="_top">KeyValue source code</a>.
        </p><div class="section" title="9.7.5.4.1.&nbsp;Example"><div class="titlepage"><div><div><h5 class="title"><a name="keyvalue.example"></a>9.7.5.4.1.&nbsp;Example</h5></div></div></div><p>To emphasize the points above, examine what happens with two Puts for two different columns for the same row:</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Put #1:  <code class="code">rowkey=row1, cf:attr1=value1</code></li><li class="listitem">Put #2:  <code class="code">rowkey=row1, cf:attr2=value2</code></li></ul></div><p>Even though these are for the same row, a KeyValue is created for each column:</p><p>Key portion for Put #1:
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">rowlength <code class="code">------------&gt; 4</code></li><li class="listitem">row <code class="code">-----------------&gt; row1</code></li><li class="listitem">columnfamilylength <code class="code">---&gt; 2</code></li><li class="listitem">columnfamily <code class="code">--------&gt; cf</code></li><li class="listitem">columnqualifier <code class="code">------&gt; attr1</code></li><li class="listitem">timestamp <code class="code">-----------&gt; server time of Put</code></li><li class="listitem">keytype <code class="code">-------------&gt; Put</code></li></ul></div><p>
          </p><p>Key portion for Put #2:
           </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">rowlength <code class="code">------------&gt; 4</code></li><li class="listitem">row <code class="code">-----------------&gt; row1</code></li><li class="listitem">columnfamilylength <code class="code">---&gt; 2</code></li><li class="listitem">columnfamily <code class="code">--------&gt; cf</code></li><li class="listitem">columnqualifier <code class="code">------&gt; attr2</code></li><li class="listitem">timestamp <code class="code">-----------&gt; server time of Put</code></li><li class="listitem">keytype <code class="code">-------------&gt; Put</code></li></ul></div><p>
           
          </p></div><p>It is critical to understand that the rowkey, ColumnFamily, and column (aka columnqualifier) are embedded within
       the KeyValue instance.  The longer these identifiers are, the bigger the KeyValue is.</p></div><div class="section" title="9.7.5.5.&nbsp;Compaction"><div class="titlepage"><div><div><h4 class="title"><a name="compaction"></a>9.7.5.5.&nbsp;Compaction</h4></div></div></div><p>有两种类型的压缩:minor和major。minor压缩通常会将数个小的相邻的文件合并成一个大的。Minor不会删除打上删除标记的数据，也不会删除过期的数据，Major压缩会删除过期的数据。有些时候minor压缩就会将一个store中的全部文件压缩，实际上这个时候他本身就是一个major压缩。对于一个minor压缩是如何压缩的，可以参见<a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/Store.html#836" target="_top">ascii diagram in the Store source code.</a></p>
         <p>在执行一个major压缩之后，一个store只会有一个sotrefile,通常情况下这样可以提供性能。注意：major压缩将会将store中的数据全部重写，在一个负载很大的系统中，这个操作是很伤的。所以在大型系统中，通常会自己<a class="xref" href="book.htm#disable.splitting" title="3.6.6. 管理 Splitting">Section&nbsp;2.8.2.8, “管理 Splitting”</a>。 </p>
         <p>Compactions will <span class="emphasis"><em>not</em></span> perform region merges.  See <a class="xref" href="ops.regionmgt.html#ops.regionmgt.merge" title="14.2.2.&nbsp;Merge">Section&nbsp;14.2.2, “Merge”</a> for more information on region merging.        </p><div class="section" title="9.7.5.5.1.&nbsp;Compaction File Selection"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.file.selection"></a>9.7.5.5.1.&nbsp;Compaction File Selection</h5></div></div></div><p>To understand the core algorithm for StoreFile selection, there is some ASCII-art in the <a class="link" href="http://hbase.apache.org/xref/org/apache/hadoop/hbase/regionserver/Store.html#836" target="_top">Store source code</a> that 
          will serve as useful reference.  It has been copied below:
</p><pre class="programlisting">/* normal skew:
 *
 *         older ----&gt; newer
 *     _
 *    | |   _
 *    | |  | |   _
 *  --|-|- |-|- |-|---_-------_-------  minCompactSize
 *    | |  | |  | |  | |  _  | |
 *    | |  | |  | |  | | | | | |
 *    | |  | |  | |  | | | | | |
 */
</pre><p>
          Important knobs:
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">hbase.store.compaction.ratio</code> Ratio used in compaction
            file selection algorithm (default 1.2f). </li><li class="listitem"><code class="code">hbase.hstore.compaction.min</code> (.90 hbase.hstore.compactionThreshold) (files) Minimum number
            of StoreFiles per Store to be selected for a compaction to occur (default 2).</li><li class="listitem"><code class="code">hbase.hstore.compaction.max</code> (files) Maximum number of StoreFiles to compact per minor compaction (default 10).</li><li class="listitem"><code class="code">hbase.hstore.compaction.min.size</code> (bytes) 
            Any StoreFile smaller than this setting with automatically be a candidate for compaction.  Defaults to 
            <code class="code">hbase.hregion.memstore.flush.size</code> (128 mb). </li><li class="listitem"><code class="code">hbase.hstore.compaction.max.size</code> (.92) (bytes) 
            Any StoreFile larger than this setting with automatically be excluded from compaction (default Long.MAX_VALUE). </li></ul></div><p>
          </p><p>The minor compaction StoreFile selection logic is size based, and selects a file for compaction when the file
           &lt;= sum(smaller_files) * <code class="code">hbase.hstore.compaction.ratio</code>.
          </p></div><div class="section" title="9.7.5.5.2.&nbsp;Minor Compaction File Selection - Example #1 (Basic Example)"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.file.selection.example1"></a>9.7.5.5.2.&nbsp;Minor Compaction File Selection - Example #1 (Basic Example)</h5></div></div></div><p>This example mirrors an example from the unit test <code class="code">TestCompactSelection</code>.
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">hbase.store.compaction.ratio</code> = 1.0f </li><li class="listitem"><code class="code">hbase.hstore.compaction.min</code> = 3 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max</code> = 5 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.min.size</code> = 10 (bytes) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max.size</code> = 1000 (bytes) </li></ul></div><p>
          The following StoreFiles exist: 100, 50, 23, 12, and 12 bytes apiece (oldest to newest).
          With the above parameters, the files that would be selected for minor compaction are 23, 12, and 12.
          </p><p>Why?
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">100 --&gt;  No, because sum(50, 23, 12, 12) * 1.0 = 97. </li><li class="listitem">50 --&gt;  No, because sum(23, 12, 12) * 1.0 = 47. </li><li class="listitem">23 --&gt;  Yes, because sum(12, 12) * 1.0 = 24. </li><li class="listitem">12 --&gt;  Yes, because the previous file has been included, and because this 
          does not exceed the the max-file limit of 5  </li><li class="listitem">12 --&gt;  Yes, because the previous file had been included, and because this 
          does not exceed the the max-file limit of 5.</li></ul></div><p>
          </p></div><div class="section" title="9.7.5.5.3.&nbsp;Minor Compaction File Selection - Example #2 (Not Enough Files To Compact)"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.file.selection.example2"></a>9.7.5.5.3.&nbsp;Minor Compaction File Selection - Example #2 (Not Enough Files To Compact)</h5></div></div></div><p>This example mirrors an example from the unit test <code class="code">TestCompactSelection</code>.
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">hbase.store.compaction.ratio</code> = 1.0f </li><li class="listitem"><code class="code">hbase.hstore.compaction.min</code> = 3 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max</code> = 5 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.min.size</code> = 10 (bytes) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max.size</code> = 1000 (bytes) </li></ul></div><p>
          </p><p>The following StoreFiles exist: 100, 25, 12, and 12 bytes apiece (oldest to newest).
          With the above parameters, the files that would be selected for minor compaction are 23, 12, and 12.         
          </p><p>Why?
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">100 --&gt; No, because sum(25, 12, 12) * 1.0 = 47</li><li class="listitem">25 --&gt;  No, because sum(12, 12) * 1.0 = 24</li><li class="listitem">12 --&gt;  No. Candidate because sum(12) * 1.0 = 12, there are only 2 files to compact and that is less than the threshold of 3</li><li class="listitem">12 --&gt;  No. Candidate because the previous StoreFile was, but there are not enough files to compact</li></ul></div><p>
          </p></div><div class="section" title="9.7.5.5.4.&nbsp;Minor Compaction File Selection - Example #3 (Limiting Files To Compact)"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.file.selection.example2"></a>9.7.5.5.4.&nbsp;Minor Compaction File Selection - Example #3 (Limiting Files To Compact)</h5></div></div></div><p>This example mirrors an example from the unit test <code class="code">TestCompactSelection</code>.
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">hbase.store.compaction.ratio</code> = 1.0f </li><li class="listitem"><code class="code">hbase.hstore.compaction.min</code> = 3 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max</code> = 5 (files) </li><li class="listitem"><code class="code">hbase.hstore.compaction.min.size</code> = 10 (bytes) </li><li class="listitem"><code class="code">hbase.hstore.compaction.max.size</code> = 1000 (bytes) </li></ul></div><p>
          The following StoreFiles exist: 7, 6, 5, 4, 3, 2, and 1 bytes apiece (oldest to newest).
          With the above parameters, the files that would be selected for minor compaction are 7, 6, 5, 4, 3.         
          </p><p>Why?
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">7 --&gt;  Yes, because sum(6, 5, 4, 3, 2, 1) * 1.0 = 21.  Also, 7 is less than the min-size</li><li class="listitem">6 --&gt;  Yes, because sum(5, 4, 3, 2, 1) * 1.0 = 15.  Also, 6 is less than the min-size. </li><li class="listitem">5 --&gt;  Yes, because sum(4, 3, 2, 1) * 1.0 = 10.  Also, 5 is less than the min-size. </li><li class="listitem">4 --&gt;  Yes, because sum(3, 2, 1) * 1.0 = 6.  Also, 4 is less than the min-size. </li><li class="listitem">3 --&gt;  Yes, because sum(2, 1) * 1.0 = 3.  Also, 3 is less than the min-size. </li><li class="listitem">2 --&gt;  No.  Candidate because previous file was selected and 2 is less than the min-size, but the max-number of files to compact has been reached. </li><li class="listitem">1 --&gt;  No.  Candidate because previous file was selected and 1 is less than the min-size, but max-number of files to compact has been reached. </li></ul></div><p>
          </p></div><div class="section" title="9.7.5.5.5.&nbsp;Impact of Key Configuration Options"><div class="titlepage"><div><div><h5 class="title"><a name="compaction.config.impact"></a>9.7.5.5.5.&nbsp;Impact of Key Configuration Options</h5></div></div></div><p><code class="code">hbase.store.compaction.ratio</code>.  A large ratio (e.g., 10) will produce a single giant file.  Conversely, a value of .25 will
          produce behavior similar to the BigTable compaction algorithm - resulting in 4 StoreFiles.
          </p><p><code class="code">hbase.hstore.compaction.min.size</code>.  Because
          this limit represents the "automatic include" limit for all StoreFiles smaller than this value, this value may need to
          be adjusted downwards in write-heavy environments where many 1 or 2 mb StoreFiles are being flushed, because every file
          will be targeted for compaction and the resulting files may still be under the min-size and require further compaction, etc. 
          </p></div></div></div><div class="section" title="9.7.6.&nbsp;Bloom Filters"><div class="titlepage"><div><div><h3 class="title"><a name="blooms"></a>9.7.6.&nbsp;Bloom Filters</h3></div></div></div><p><a class="link" href="http://en.wikipedia.org/wiki/Bloom_filter" target="_top">Bloom filters</a> were developed over in <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200
    Add bloomfilters</a>.<sup>[<a name="d1934e6050" href="#ftn.d1934e6050" class="footnote">25</a>]</sup><sup>[<a name="d1934e6062" href="#ftn.d1934e6062" class="footnote">26</a>]</sup></p><p>See also <a class="xref" href="perf.schema.html#schema.bloom" title="11.6.4.&nbsp;Bloom Filters">Section&nbsp;11.6.4, “Bloom Filters”</a> and <a class="xref" href="config.bloom.html" title="2.9.&nbsp;Bloom Filter Configuration">Section&nbsp;2.9, “Bloom Filter Configuration”</a>.
        </p><div class="section" title="9.7.6.1.&nbsp;Bloom StoreFile footprint"><div class="titlepage"><div><div><h4 class="title"><a name="bloom_footprint"></a>9.7.6.1.&nbsp;Bloom StoreFile footprint</h4></div></div></div><p>Bloom filters add an entry to the <code class="classname">StoreFile</code>
      general <code class="classname">FileInfo</code> data structure and then two
      extra entries to the <code class="classname">StoreFile</code> metadata
      section.</p><div class="section" title="9.7.6.1.1.&nbsp;BloomFilter in the StoreFile FileInfo data structure"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e6088"></a>9.7.6.1.1.&nbsp;BloomFilter in the <code class="classname">StoreFile</code>
        <code class="classname">FileInfo</code> data structure</h5></div></div></div><p><code class="classname">FileInfo</code> has a
          <code class="varname">BLOOM_FILTER_TYPE</code> entry which is set to
          <code class="varname">NONE</code>, <code class="varname">ROW</code> or
          <code class="varname">ROWCOL.</code></p></div><div class="section" title="9.7.6.1.2.&nbsp;BloomFilter entries in StoreFile metadata"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e6112"></a>9.7.6.1.2.&nbsp;BloomFilter entries in <code class="classname">StoreFile</code>
        metadata</h5></div></div></div><p><code class="varname">BLOOM_FILTER_META</code> holds Bloom Size, Hash
          Function used, etc. Its small in size and is cached on
          <code class="classname">StoreFile.Reader</code> load</p><p><code class="varname">BLOOM_FILTER_DATA</code> is the actual bloomfilter
          data. Obtained on-demand. Stored in the LRU cache, if it is enabled
          (Its enabled by default).</p></div></div></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d1934e6050" href="#d1934e6050" class="para">25</a>] </sup>For description of the development process -- why static blooms
        rather than dynamic -- and for an overview of the unique properties
        that pertain to blooms in HBase, as well as possible future
        directions, see the <span class="emphasis"><em>Development Process</em></span> section
        of the document <a class="link" href="https://issues.apache.org/jira/secure/attachment/12444007/Bloom_Filters_in_HBase.pdf" target="_top">BloomFilters
        in HBase</a> attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1200" target="_top">HBase-1200</a>.</p></div><div class="footnote"><p><sup>[<a id="ftn.d1934e6062" href="#d1934e6062" class="para">26</a>] </sup>The bloom filters described here are actually version two of
        blooms in HBase. In versions up to 0.19.x, HBase had a dynamic bloom
        option based on work done by the <a class="link" href="http://www.one-lab.org" target="_top">European Commission One-Lab
        Project 034819</a>. The core of the HBase bloom work was later
        pulled up into Hadoop to implement org.apache.hadoop.io.BloomMapFile.
        Version 1 of HBase blooms never worked that well. Version 2 is a
        rewrite from scratch though again it starts with the one-lab
        work.</p></div></div></div>
        
        <div class="section" title="9.8.&nbsp;Bulk Loading"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.bulk.load"></a>9.8.&nbsp;Bulk Loading</h2></div></div></div><div class="section" title="9.8.1.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.overview"></a>9.8.1.&nbsp;Overview</h3></div></div></div><p>
        HBase includes several methods of loading data into tables.
        The most straightforward method is to either use the <code class="code">TableOutputFormat</code>
        class from a MapReduce job, or use the normal client APIs; however,
        these are not always the most efficient methods.
      </p><p>
        The bulk load feature uses a MapReduce job to output table data in HBase's internal
        data format, and then directly loads the generated StoreFiles into a running
        cluster. Using bulk load will use less CPU and network resources than
        simply using the HBase API.
      </p></div><div class="section" title="9.8.2.&nbsp;Bulk Load Architecture"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.arch"></a>9.8.2.&nbsp;Bulk Load Architecture</h3></div></div></div><p>
        The HBase bulk load process consists of two main steps.
      </p><div class="section" title="9.8.2.1.&nbsp;Preparing data via a MapReduce job"><div class="titlepage"><div><div><h4 class="title"><a name="arch.bulk.load.prep"></a>9.8.2.1.&nbsp;Preparing data via a MapReduce job</h4></div></div></div><p>
          The first step of a bulk load is to generate HBase data files (StoreFiles) from
          a MapReduce job using <code class="code">HFileOutputFormat</code>. This output format writes
          out data in HBase's internal storage format so that they can be
          later loaded very efficiently into the cluster.
        </p><p>
          In order to function efficiently, <code class="code">HFileOutputFormat</code> must be
          configured such that each output HFile fits within a single region.
          In order to do this, jobs whose output will be bulk loaded into HBase
          use Hadoop's <code class="code">TotalOrderPartitioner</code> class to partition the map output
          into disjoint ranges of the key space, corresponding to the key
          ranges of the regions in the table.
        </p><p>
          <code class="code">HFileOutputFormat</code> includes a convenience function,
          <code class="code">configureIncrementalLoad()</code>, which automatically sets up
          a <code class="code">TotalOrderPartitioner</code> based on the current region boundaries of a
          table.
        </p></div><div class="section" title="9.8.2.2.&nbsp;Completing the data load"><div class="titlepage"><div><div><h4 class="title"><a name="arch.bulk.load.complete"></a>9.8.2.2.&nbsp;Completing the data load</h4></div></div></div><p>
          After the data has been prepared using
          <code class="code">HFileOutputFormat</code>, it is loaded into the cluster using
          <code class="code">completebulkload</code>. This command line tool iterates
          through the prepared data files, and for each one determines the
          region the file belongs to. It then contacts the appropriate Region
          Server which adopts the HFile, moving it into its storage directory
          and making the data available to clients.
        </p><p>
          If the region boundaries have changed during the course of bulk load
          preparation, or between the preparation and completion steps, the
          <code class="code">completebulkloads</code> utility will automatically split the
          data files into pieces corresponding to the new boundaries. This
          process is not optimally efficient, so users should take care to
          minimize the delay between preparing a bulk load and importing it
          into the cluster, especially if other clients are simultaneously
          loading data through other means.
        </p></div></div><div class="section" title="9.8.3.&nbsp;Importing the prepared data using the completebulkload tool"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.import"></a>9.8.3.&nbsp;Importing the prepared data using the completebulkload tool</h3></div></div></div><p>
        After a data import has been prepared, either by using the
        <code class="code">importtsv</code> tool with the
        "<code class="code">importtsv.bulk.output</code>" option or by some other MapReduce
        job using the <code class="code">HFileOutputFormat</code>, the
        <code class="code">completebulkload</code> tool is used to import the data into the
        running cluster.
      </p><p>
        The <code class="code">completebulkload</code> tool simply takes the output path
        where <code class="code">importtsv</code> or your MapReduce job put its results, and
        the table name to import into. For example:
      </p><code class="code">$ hadoop jar hbase-VERSION.jar completebulkload [-c /path/to/hbase/config/hbase-site.xml] /user/todd/myoutput mytable</code><p>
        The <code class="code">-c config-file</code> option can be used to specify a file
        containing the appropriate hbase parameters (e.g., hbase-site.xml) if
        not supplied already on the CLASSPATH (In addition, the CLASSPATH must
        contain the directory that has the zookeeper configuration file if
        zookeeper is NOT managed by HBase).
      </p><p>
        Note: If the target table does not already exist in HBase, this
        tool will create the table automatically.</p><p>
        This tool will run quickly, after which point the new data will be visible in
        the cluster.
      </p></div><div class="section" title="9.8.4.&nbsp;See Also"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.also"></a>9.8.4.&nbsp;See Also</h3></div></div></div><p>For more information about the referenced utilities, see <a class="xref" href="ops_mgt.html#importtsv" title="14.1.9.&nbsp;ImportTsv">Section&nbsp;14.1.9, “ImportTsv”</a> and  <a class="xref" href="ops_mgt.html#completebulkload" title="14.1.10.&nbsp;CompleteBulkLoad">Section&nbsp;14.1.10, “CompleteBulkLoad”</a>.
      </p></div><div class="section" title="9.8.5.&nbsp;Advanced Usage"><div class="titlepage"><div><div><h3 class="title"><a name="arch.bulk.load.adv"></a>9.8.5.&nbsp;Advanced Usage</h3></div></div></div><p>
        Although the <code class="code">importtsv</code> tool is useful in many cases, advanced users may
        want to generate data programatically, or import data from other formats. To get
        started doing so, dig into <code class="code">ImportTsv.java</code> and check the JavaDoc for
        HFileOutputFormat.
      </p><p>
        The import step of the bulk load can also be done programatically. See the
        <code class="code">LoadIncrementalHFiles</code> class for more information.
      </p></div></div>
           
       <div class="section" title="9.9.&nbsp;HDFS"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="arch.hdfs"></a>9.9.&nbsp;HDFS</h2></div></div></div><p>As HBase runs on HDFS (and each StoreFile is written as a file on HDFS),
        it is important to have an understanding of the HDFS Architecture
         especially in terms of how it stores files, handles failovers, and replicates blocks.
       </p><p>See the Hadoop documentation on <a class="link" href="http://hadoop.apache.org/common/docs/current/hdfs_design.html" target="_top">HDFS Architecture</a>
       for more information.
       </p><div class="section" title="9.9.1.&nbsp;NameNode"><div class="titlepage"><div><div><h3 class="title"><a name="arch.hdfs.nn"></a>9.9.1.&nbsp;NameNode</h3></div></div></div><p>The NameNode is responsible for maintaining the filesystem metadata.  See the above HDFS Architecture link
         for more information.
         </p></div><div class="section" title="9.9.2.&nbsp;DataNode"><div class="titlepage"><div><div><h3 class="title"><a name="arch.hdfs.dn"></a>9.9.2.&nbsp;DataNode</h3></div></div></div><p>The DataNodes are responsible for storing HDFS blocks.  See the above HDFS Architecture link
         for more information.
         </p></div></div>
         
         <div class="chapter" title="Chapter&nbsp;10.&nbsp;External APIs"><div class="titlepage"><div><div><h2 class="title"><a name="external_apis"></a>Chapter&nbsp;10.&nbsp;External APIs</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="external_apis.html#nonjava.jvm">10.1. Non-Java Languages Talking to the JVM</a></span></dt><dt><span class="section"><a href="rest.html">10.2. REST</a></span></dt><dt><span class="section"><a href="thrift.html">10.3. Thrift</a></span></dt><dd><dl><dt><span class="section"><a href="thrift.html#thrift.filter-language">10.3.1. Filter Language</a></span></dt></dl></dd></dl></div>
  This chapter will cover access to HBase either through non-Java languages, or through custom protocols.
  
  <div class="section" title="10.1.&nbsp;Non-Java Languages Talking to the JVM"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="nonjava.jvm"></a>10.1.&nbsp;Non-Java Languages Talking to the JVM</h2></div></div></div><p>Currently the documentation on this topic in the 
      <a class="link" href="http://wiki.apache.org/hadoop/Hbase" target="_top">HBase Wiki</a>.
      See also the <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/thrift/package-summary.html#package_description" target="_top">Thrift API Javadoc</a>.
    </p></div></div>
    <div class="section" title="10.2.&nbsp;REST"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="rest"></a>10.2.&nbsp;REST</h2></div></div></div><p>Currently most of the documentation on REST exists in the 
      <a class="link" href="http://wiki.apache.org/hadoop/Hbase/Stargate" target="_top">HBase Wiki on REST</a>.
    </p></div>
    <div class="section" title="10.3.&nbsp;Thrift"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="thrift"></a>10.3.&nbsp;Thrift</h2></div></div></div><p>Currently most of the documentation on Thrift exists in the 
      <a class="link" href="http://wiki.apache.org/hadoop/Hbase/ThriftApi" target="_top">HBase Wiki on Thrift</a>.
    </p><div class="section" title="10.3.1.&nbsp;Filter Language"><div class="titlepage"><div><div><h3 class="title"><a name="thrift.filter-language"></a>10.3.1.&nbsp;Filter Language</h3></div></div></div><div class="section" title="10.3.1.1.&nbsp;Use Case"><div class="titlepage"><div><div><h4 class="title"><a name="use-case"></a>10.3.1.1.&nbsp;Use Case</h4></div></div></div><p>Note:  this feature was introduced in HBase 0.92</p><p>This allows the user to perform server-side filtering when accessing HBase over Thrift. The user specifies a filter via a string. The string is parsed on the server to construct the filter</p></div><div class="section" title="10.3.1.2.&nbsp;General Filter String Syntax"><div class="titlepage"><div><div><h4 class="title"><a name="general-syntax"></a>10.3.1.2.&nbsp;General Filter String Syntax</h4></div></div></div><p>A simple filter expression is expressed as: <code class="code">“FilterName (argument, argument, ... , argument)”</code></p><p>You must specify the name of the filter followed by the argument list in parenthesis. Commas separate the individual arguments</p><p>If the argument represents a string, it should be enclosed in single quotes.</p><p>If it represents a boolean, an integer or a comparison operator like &lt;,
                 &gt;, != etc. it should not be enclosed in quotes</p><p>The filter name must be one word. All ASCII characters are allowed except for whitespace, single quotes and parenthesis.</p><p>The filter’s arguments can contain any ASCII character. <code class="code">If single quotes are present in the argument, they must be escaped by a
                   preceding single quote</code></p></div><div class="section" title="10.3.1.3.&nbsp;Compound Filters and Operators"><div class="titlepage"><div><div><h4 class="title"><a name="compound-filters-and-operators"></a>10.3.1.3.&nbsp;Compound Filters and Operators</h4></div></div></div><p>Currently, two binary operators – AND/OR and two unary operators – WHILE/SKIP are supported.</p><p>Note: the operators are all in uppercase</p><p><span class="bold"><strong>AND</strong></span> – as the name suggests, if this
                 operator is used, the key-value must pass both the filters</p><p><span class="bold"><strong>OR</strong></span> – as the name suggests, if this operator
                 is used, the key-value must pass at least one of the filters</p><p><span class="bold"><strong>SKIP</strong></span> – For a particular row, if any of the
                 key-values don’t pass the filter condition, the entire row is skipped</p><p><span class="bold"><strong>WHILE</strong></span> - For a particular row, it continues
                 to emit key-values until a key-value is reached that fails the filter condition</p><p><span class="bold"><strong>Compound Filters:</strong></span> Using these operators, a
                 hierarchy of filters can be created. For example: <code class="code">“(Filter1 AND Filter2) OR (Filter3 AND Filter4)”</code></p></div><div class="section" title="10.3.1.4.&nbsp;Order of Evaluation"><div class="titlepage"><div><div><h4 class="title"><a name="order-of-evaluation"></a>10.3.1.4.&nbsp;Order of Evaluation</h4></div></div></div><p>Parenthesis have the highest precedence. The SKIP and WHILE operators are next and have the same precedence.The AND operator has the next highest precedence followed by the OR operator.</p><p>For example:</p><p>A filter string of the form:<code class="code">“Filter1 AND Filter2 OR Filter3”</code>
                 will be evaluated as:<code class="code">“(Filter1 AND Filter2) OR Filter3”</code></p><p>A filter string of the form:<code class="code">“Filter1 AND SKIP Filter2 OR Filter3”</code>
                 will be evaluated as:<code class="code">“(Filter1 AND (SKIP Filter2)) OR Filter3”</code></p></div><div class="section" title="10.3.1.5.&nbsp;Compare Operator"><div class="titlepage"><div><div><h4 class="title"><a name="compare-operator"></a>10.3.1.5.&nbsp;Compare Operator</h4></div></div></div><p>A compare operator can be any of the following:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>LESS (&lt;)</p></li><li class="listitem"><p>LESS_OR_EQUAL (&lt;=)</p></li><li class="listitem"><p>EQUAL (=)</p></li><li class="listitem"><p>NOT_EQUAL (!=)</p></li><li class="listitem"><p>GREATER_OR_EQUAL (&gt;=)</p></li><li class="listitem"><p>GREATER (&gt;)</p></li><li class="listitem"><p>NO_OP (no operation)</p></li></ol></div><p>The client should use the symbols (&lt;, &lt;=, =, !=, &gt;, &gt;=) to express
                 compare operators.</p></div><div class="section" title="10.3.1.6.&nbsp;Comparator"><div class="titlepage"><div><div><h4 class="title"><a name="comparator"></a>10.3.1.6.&nbsp;Comparator</h4></div></div></div><p>A comparator can be any of the following:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><span class="bold"><strong>BinaryComparator</strong></span> - This
                     lexicographically compares against the specified byte array using
                     Bytes.compareTo(byte[], byte[])</p></li><li class="listitem"><p><span class="bold"><strong>BinaryPrefixComparator</strong></span> - This
                     lexicographically compares against a specified byte array. It only compares up to
                     the length of this byte array.</p></li><li class="listitem"><p><span class="bold"><strong>RegexStringComparator</strong></span> - This compares
                     against the specified byte array using the given regular expression. Only EQUAL
                     and NOT_EQUAL comparisons are valid with this comparator</p></li><li class="listitem"><p><span class="bold"><strong>SubStringComparator</strong></span> - This tests if
                     the given substring appears in a specified byte array. The comparison is case
                     insensitive. Only EQUAL and NOT_EQUAL comparisons are valid with this
                     comparator</p></li></ol></div><p>The general syntax of a comparator is:<code class="code"> ComparatorType:ComparatorValue</code></p><p>The ComparatorType for the various comparators is as follows:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><span class="bold"><strong>BinaryComparator</strong></span> - binary</p></li><li class="listitem"><p><span class="bold"><strong>BinaryPrefixComparator</strong></span> - binaryprefix</p></li><li class="listitem"><p><span class="bold"><strong>RegexStringComparator</strong></span> - regexstring</p></li><li class="listitem"><p><span class="bold"><strong>SubStringComparator</strong></span> - substring</p></li></ol></div><p>The ComparatorValue can be any value.</p><p>Example1:<code class="code"> &gt;, 'binary:abc' </code>will match everything that is lexicographically greater than "abc" </p><p>Example2:<code class="code"> =, 'binaryprefix:abc' </code>will match everything whose first 3 characters are lexicographically equal to "abc"</p><p>Example3:<code class="code"> !=, 'regexstring:ab*yz' </code>will match everything that doesn't begin with "ab" and ends with "yz"</p><p>Example4:<code class="code"> =, 'substring:abc123' </code>will match everything that begins with the substring "abc123"</p></div><div class="section" title="10.3.1.7.&nbsp;Example PHP Client Program that uses the Filter Language"><div class="titlepage"><div><div><h4 class="title"><a name="example PHP Client Program"></a>10.3.1.7.&nbsp;Example PHP Client Program that uses the Filter Language</h4></div></div></div><pre class="programlisting">&lt;? $_SERVER['PHP_ROOT'] = realpath(dirname(__FILE__).'/..');
   require_once $_SERVER['PHP_ROOT'].'/flib/__flib.php';
   flib_init(FLIB_CONTEXT_SCRIPT);
   require_module('storage/hbase');
   $hbase = new HBase('&lt;server_name_running_thrift_server&gt;', &lt;port on which thrift server is running&gt;);
   $hbase-&gt;open();
   $client = $hbase-&gt;getClient();
   $result = $client-&gt;scannerOpenWithFilterString('table_name', "(PrefixFilter ('row2') AND (QualifierFilter (&gt;=, 'binary:xyz'))) AND (TimestampsFilter ( 123, 456))");
   $to_print = $client-&gt;scannerGetList($result,1);
   while ($to_print) {
      print_r($to_print);
      $to_print = $client-&gt;scannerGetList($result,1);
    }
   $client-&gt;scannerClose($result);
?&gt;
        </pre></div><div class="section" title="10.3.1.8.&nbsp;Example Filter Strings"><div class="titlepage"><div><div><h4 class="title"><a name="example-filter-strings"></a>10.3.1.8.&nbsp;Example Filter Strings</h4></div></div></div><p>
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="code">“PrefixFilter (‘Row’) AND PageFilter (1) AND FirstKeyOnlyFilter ()”</code> will return all key-value pairs that match the following conditions:</p><p>1) The row containing the key-value should have prefix “Row” </p><p>2) The key-value must be located in the first row of the table </p><p>3) The key-value pair must be the first key-value in the row </p></li></ul></div><p>
        </p><div class="orderedlist"><p>
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="code">“(RowFilter (=, ‘binary:Row 1’) AND TimeStampsFilter (74689, 89734)) OR
                    ColumnRangeFilter (‘abc’, true, ‘xyz’, false))”</code> will return all key-value pairs that match both the following conditions:</p><p>1) The key-value is in a row having row key “Row 1” </p><p>2) The key-value must have a timestamp of either 74689 or 89734.</p><p>Or it must match the following condition:</p><p>1) The key-value pair must be in a column that is lexicographically &gt;= abc and &lt; xyz&nbsp;</p></li></ul></div><p>
          </p><ol class="orderedlist" type="1"></ol></div><p>
          </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><p><code class="code">“SKIP ValueFilter (0)”</code> will skip the entire row if any of the values in the row is not 0</p></li></ul></div><p>
        </p></div><div class="section" title="10.3.1.9.&nbsp;Individual Filter Syntax"><div class="titlepage"><div><div><h4 class="title"><a name="Individual Filter Syntax"></a>10.3.1.9.&nbsp;Individual Filter Syntax</h4></div></div></div><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p><span class="bold"><strong><span class="underline">KeyOnlyFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter doesn’t take any
              arguments. It returns only the key component of each key-value. </p><p><span class="bold"><strong>Syntax:</strong></span> KeyOnlyFilter () </p><p><span class="bold"><strong>Example:</strong></span> "KeyOnlyFilter ()"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">FirstKeyOnlyFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter doesn’t take any
              arguments. It returns only the first key-value from each row. </p><p><span class="bold"><strong>Syntax:</strong></span> FirstKeyOnlyFilter () </p><p><span class="bold"><strong>Example:</strong></span> "FirstKeyOnlyFilter ()" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">PrefixFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument – a prefix of a
              row key. It returns only those key-values present in a row that starts with the
              specified row prefix</p><p><span class="bold"><strong>Syntax:</strong></span> PrefixFilter (‘&lt;row_prefix&gt;’) </p><p><span class="bold"><strong>Example:</strong></span> "PrefixFilter (‘Row’)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">
                  ColumnPrefixFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument
              – a column prefix. It returns only those key-values present in a column that starts
              with the specified column prefix. The column prefix must be of the form: <code class="code">“qualifier” </code></p><p><span class="bold"><strong>Syntax:</strong></span>ColumnPrefixFilter(‘&lt;column_prefix&gt;’)</p><p><span class="bold"><strong>Example:</strong></span> "ColumnPrefixFilter(‘Col’)"</p></li><li class="listitem"><p><span class="underline"><span class="bold"><strong>MultipleColumnPrefixFilter</strong></span></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a list of
              column prefixes. It returns key-values that are present in a column that starts with
              any of the specified column prefixes. Each of the column prefixes must be of the form: <code class="code">“qualifier”</code></p><p><span class="bold"><strong>Syntax:</strong></span>MultipleColumnPrefixFilter(‘&lt;column_prefix&gt;’, ‘&lt;column_prefix&gt;’, …, ‘&lt;column_prefix&gt;’)</p><p><span class="bold"><strong>Example:</strong></span> "MultipleColumnPrefixFilter(‘Col1’, ‘Col2’)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">ColumnCountGetFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument
              – a limit. It returns the first limit number of columns in the table</p><p><span class="bold"><strong>Syntax:</strong></span> ColumnCountGetFilter (‘&lt;limit&gt;’)</p><p><span class="bold"><strong>Example:</strong></span> "ColumnCountGetFilter (4)"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">PageFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument
              – a page size. It returns page size number of rows from the table. </p><p><span class="bold"><strong>Syntax:</strong></span> PageFilter (‘&lt;page_size&gt;’)</p><p><span class="bold"><strong>Example:</strong></span> "PageFilter (2)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">ColumnPaginationFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes two
              arguments – a limit and offset. It returns limit number of columns after offset number
              of columns. It does this for all the rows</p><p><span class="bold"><strong>Syntax:</strong></span> ColumnPaginationFilter(‘&lt;limit&gt;’, ‘&lt;offest&gt;’) </p><p><span class="bold"><strong>Example:</strong></span> "ColumnPaginationFilter (3, 5)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">InclusiveStopFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes one argument
              – a row key on which to stop scanning. It returns all key-values present in rows up to
              and including the specified row</p><p><span class="bold"><strong>Syntax:</strong></span> InclusiveStopFilter(‘&lt;stop_row_key&gt;’) </p><p><span class="bold"><strong>Example:</strong></span> "InclusiveStopFilter ('Row2')" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">TimeStampsFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a list of
              timestamps. It returns those key-values whose timestamps matches any of the specified
              timestamps</p><p> <span class="bold"><strong>Syntax:</strong></span> TimeStampsFilter (&lt;timestamp&gt;, &lt;timestamp&gt;, ... ,&lt;timestamp&gt;) </p><p> <span class="bold"><strong>Example:</strong></span> "TimeStampsFilter (5985489, 48895495, 58489845945)"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">RowFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a compare
              operator and a comparator. It compares each row key with the comparator using the
              compare operator and if the comparison returns true, it returns all the key-values in
              that row</p><p><span class="bold"><strong>Syntax:</strong></span> RowFilter (&lt;compareOp&gt;, ‘&lt;row_comparator&gt;’) </p><p><span class="bold"><strong>Example: </strong></span>"RowFilter (&lt;=, ‘xyz)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">Family Filter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a compare
              operator and a comparator. It compares each qualifier name with the comparator using
              the compare operator and if the comparison returns true, it returns all the key-values
              in that column</p><p><span class="bold"><strong>Syntax:</strong></span> QualifierFilter (&lt;compareOp&gt;, ‘&lt;qualifier_comparator&gt;’) </p><p><span class="bold"><strong>Example:</strong></span> "QualifierFilter (=, ‘Column1’)"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">QualifierFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a compare
              operator and a comparator. It compares each qualifier name with the comparator using
              the compare operator and if the comparison returns true, it returns all the key-values
              in that column</p><p><span class="bold"><strong>Syntax:</strong></span> QualifierFilter (&lt;compareOp&gt;,‘&lt;qualifier_comparator&gt;’) </p><p><span class="bold"><strong>Example:</strong></span> "QualifierFilter (=,‘Column1’)"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">ValueFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a compare operator and a
              comparator. It compares each value with the comparator using the compare operator and
              if the comparison returns true, it returns that key-value</p><p><span class="bold"><strong>Syntax:</strong></span> ValueFilter (&lt;compareOp&gt;,‘&lt;value_comparator&gt;’) </p><p><span class="bold"><strong>Example:</strong></span> "ValueFilter (!=, ‘Value’)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">DependentColumnFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes two arguments – a family
              and a qualifier. It tries to locate this column in each row and returns all key-values
              in that row that have the same timestamp. If the row doesn’t contain the specified
              column – none of the key-values in that row will be returned.</p><p>The filter can also take an optional boolean argument – dropDependentColumn. If set to true, the column we were depending on doesn’t get returned.</p><p>The filter can also take two more additional optional arguments – a compare operator and a value comparator, which are further checks in addition to the family and qualifier. If the dependent column is found, its value should also pass the value check and then only is its timestamp taken into consideration</p><p><span class="bold"><strong>Syntax:</strong></span> DependentColumnFilter (‘&lt;family&gt;’, ‘&lt;qualifier&gt;’, &lt;boolean&gt;, &lt;compare operator&gt;, ‘&lt;value comparator’)</p><p><span class="bold"><strong>Syntax:</strong></span> DependentColumnFilter (‘&lt;family&gt;’, ‘&lt;qualifier&gt;’, &lt;boolean&gt;) </p><p><span class="bold"><strong>Syntax:</strong></span> DependentColumnFilter (‘&lt;family&gt;’, ‘&lt;qualifier&gt;’) </p><p><span class="bold"><strong>Example:</strong></span> "DependentColumnFilter (‘conf’, ‘blacklist’, false, &gt;=, ‘zebra’)" </p><p><span class="bold"><strong>Example:</strong></span> "DependentColumnFilter (‘conf’, 'blacklist', true)"</p><p><span class="bold"><strong>Example:</strong></span> "DependentColumnFilter (‘conf’, 'blacklist')"</p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">SingleColumnValueFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes a column family, a
              qualifier, a compare operator and a comparator. If the specified column is not found –
              all the columns of that row will be emitted. If the column is found and the comparison
              with the comparator returns true, all the columns of the row will be emitted. If the
              condition fails, the row will not be emitted. </p><p>This filter also takes two additional optional boolean arguments – filterIfColumnMissing and setLatestVersionOnly</p><p>If the filterIfColumnMissing flag is set to true the columns of the row will not be emitted if the specified column to check is not found in the row. The default value is false.</p><p>If the setLatestVersionOnly flag is set to false, it will test previous versions (timestamps) too. The default value is true.</p><p>These flags are optional and if you must set neither or both</p><p><span class="bold"><strong>Syntax:</strong></span> SingleColumnValueFilter(&lt;compare operator&gt;, ‘&lt;comparator&gt;’, ‘&lt;family&gt;’, ‘&lt;qualifier&gt;’,&lt;filterIfColumnMissing_boolean&gt;, &lt;latest_version_boolean&gt;) </p><p><span class="bold"><strong>Syntax:</strong></span> SingleColumnValueFilter(&lt;compare operator&gt;, ‘&lt;comparator&gt;’, ‘&lt;family&gt;’, ‘&lt;qualifier&gt;) </p><p><span class="bold"><strong>Example:</strong></span> "SingleColumnValueFilter (&lt;=, ‘abc’,‘FamilyA’, ‘Column1’, true, false)" </p><p><span class="bold"><strong>Example:</strong></span> "SingleColumnValueFilter (&lt;=, ‘abc’,‘FamilyA’, ‘Column1’)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">SingleColumnValueExcludeFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter takes the same arguments and
              behaves same as SingleColumnValueFilter – however, if the column is found and the
              condition passes, all the columns of the row will be emitted except for the tested
              column value. </p><p><span class="bold"><strong>Syntax:</strong></span> SingleColumnValueExcludeFilter(&lt;compare operator&gt;, '&lt;comparator&gt;', '&lt;family&gt;', '&lt;qualifier&gt;',&lt;latest_version_boolean&gt;, &lt;filterIfColumnMissing_boolean&gt;)</p><p><span class="bold"><strong>Syntax:</strong></span> SingleColumnValueExcludeFilter(&lt;compare operator&gt;, '&lt;comparator&gt;', '&lt;family&gt;', '&lt;qualifier&gt;') </p><p><span class="bold"><strong>Example:</strong></span> "SingleColumnValueExcludeFilter (‘&lt;=’, ‘abc’,‘FamilyA’, ‘Column1’, ‘false’, ‘true’)"</p><p><span class="bold"><strong>Example:</strong></span> "SingleColumnValueExcludeFilter (‘&lt;=’, ‘abc’, ‘FamilyA’, ‘Column1’)" </p></li><li class="listitem"><p><span class="bold"><strong><span class="underline">ColumnRangeFilter</span></strong></span></p><p><span class="bold"><strong>Description:</strong></span> This filter is used for selecting only those
              keys with columns that are between minColumn and maxColumn. It also takes two boolean
              variables to indicate whether to include the minColumn and maxColumn or not.</p><p>If you don’t want to set the minColumn or the maxColumn – you can pass in an empty argument.</p><p><span class="bold"><strong>Syntax:</strong></span> ColumnRangeFilter (‘&lt;minColumn&gt;’, &lt;minColumnInclusive_bool&gt;, ‘&lt;maxColumn&gt;’, &lt;maxColumnInclusive_bool&gt;)</p><p><span class="bold"><strong>Example:</strong></span> "ColumnRangeFilter (‘abc’, true, ‘xyz’, false)"</p></li></ol></div></div></div></div>  
       <div class="section" title="12.1.3. Filters">
         <div class="titlepage">
           <div>
             <div></div>
           </div>
         </div>
       </div>
     </div>
     <div class="chapter" title="Chapter&nbsp;11.&nbsp;Performance Tuning"><div class="titlepage"><div><div>
       <h2 class="title"><a name="performance"></a>Chapter&nbsp;11.&nbsp;性能调优</h2>
     </div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="performance.html#perf.os">11.1. Operating System</a></span></dt><dd><dl><dt><span class="section"><a href="performance.html#perf.os.ram">11.1.1. Memory</a></span></dt><dt><span class="section"><a href="performance.html#perf.os.64">11.1.2. 64-bit</a></span></dt><dt><span class="section"><a href="performance.html#perf.os.swap">11.1.3. Swapping</a></span></dt></dl></dd><dt><span class="section"><a href="perf.network.html">11.2. Network</a></span></dt><dd><dl><dt><span class="section"><a href="perf.network.html#perf.network.1switch">11.2.1. Single Switch</a></span></dt><dt><span class="section"><a href="perf.network.html#perf.network.2switch">11.2.2. Multiple Switches</a></span></dt><dt><span class="section"><a href="perf.network.html#perf.network.multirack">11.2.3. Multiple Racks</a></span></dt><dt><span class="section"><a href="perf.network.html#perf.network.ints">11.2.4. Network Interfaces</a></span></dt></dl></dd>
         <dt>&nbsp;</dt>
         <dt><span class="section"><a href="jvm.html">11.3. Java</a></span></dt>
         </dl>
       <dl>
         <dt><a href="book.htm#jvm">13.1. Java</a></dt>
         <dd>
           <dl>
             <dt><a href="book.htm#gc">13.1.1. 垃圾收集和HBase</a></dt>
           </dl>
         </dd>
         <dt><a href="book.htm#perf.configurations">13.2. 配置</a></dt>
         <dd>
           <dl>
             <dt><a href="book.htm#perf.number.of.regions">13.2.1. Regions的数目</a></dt>
             <dt><a href="book.htm#perf.compactions.and.splits">13.2.2. 管理压缩</a></dt>
             <dt><a href="book.htm#perf.compression">13.2.3. 压缩</a></dt>
             <dt><a href="book.htm#perf.handlers">13.2.4. <code class="varname">hbase.regionserver.handler.count</code></a></dt>
             <dt><a href="book.htm#perf.hfile.block.cache.size">13.2.5. <code class="varname">hfile.block.cache.size</code></a></dt>
             <dt><a href="book.htm#perf.rs.memstore.upperlimit">13.2.6. <code class="varname">hbase.regionserver.global.memstore.upperLimit</code></a></dt>
             <dt><a href="book.htm#perf.rs.memstore.lowerlimit">13.2.7. <code class="varname">hbase.regionserver.global.memstore.lowerLimit</code></a></dt>
             <dt><a href="book.htm#perf.hstore.blockingstorefiles">13.2.8. <code class="varname">hbase.hstore.blockingStoreFiles</code></a></dt>
             <dt><a href="book.htm#perf.hregion.memstore.block.multiplier">13.2.9. <code class="varname">hbase.hregion.memstore.block.multiplier</code></a></dt>
           </dl>
         </dd>
         <dt><a href="book.htm#perf.number.of.cfs">13.3. Column Families的数目</a></dt>
         <dt><a href="book.htm#perf.one.region">13.4. 数据聚集</a></dt>
         <dt><a href="book.htm#perf.batch.loading">13.5. 批量Loading</a></dt>
         <dd>
           <dl>
             <dt><a href="book.htm#precreate.regions">13.5.1. 
               Table创建: 预创建Regions </a></dt>
           </dl>
         </dd>
         <dt><a href="book.htm#d613e3446">13.6. HBase客户端</a></dt>
         <dd>
           <dl>
             <dt><a href="book.htm#perf.hbase.client.autoflush">13.6.1. AutoFlush</a></dt>
             <dt><a href="book.htm#perf.hbase.client.caching">13.6.2. Scan Caching</a></dt>
             <dt><a href="book.htm#perf.hbase.client.selection">13.6.3. Scan 属性选择</a></dt>
             <dt><a href="book.htm#perf.hbase.client.scannerclose">13.6.4. 关闭 ResultScanners</a></dt>
             <dt><a href="book.htm#perf.hbase.client.blockcache">13.6.5. 块缓存</a></dt>
             <dt><a href="book.htm#perf.hbase.client.rowkeyonly">13.6.6. Row Keys的负载优化</a></dt>
           </dl>
         </dd>
       </dl>
       <p>&nbsp;</p>
       <dl>
         <dd><dl><dt><span class="section"><a href="jvm.html#gc">11.3.1. The Garbage Collector and HBase</a></span></dt></dl></dd><dt><span class="section"><a href="perf.configurations.html">11.4. HBase Configurations</a></span></dt><dd><dl><dt><span class="section"><a href="perf.configurations.html#perf.number.of.regions">11.4.1. Number of Regions</a></span></dt><dt><span class="section"><a href="perf.configurations.html#perf.compactions.and.splits">11.4.2. Managing Compactions</a></span></dt><dt><span class="section"><a href="perf.configurations.html#perf.handlers">11.4.3. <code class="varname">hbase.regionserver.handler.count</code></a></span></dt><dt><span class="section"><a href="perf.configurations.html#perf.hfile.block.cache.size">11.4.4. <code class="varname">hfile.block.cache.size</code></a></span></dt><dt><span class="section"><a href="perf.configurations.html#perf.rs.memstore.upperlimit">11.4.5. <code class="varname">hbase.regionserver.global.memstore.upperLimit</code></a></span></dt><dt><span class="section"><a href="perf.configurations.html#perf.rs.memstore.lowerlimit">11.4.6. <code class="varname">hbase.regionserver.global.memstore.lowerLimit</code></a></span></dt><dt><span class="section"><a href="perf.configurations.html#perf.hstore.blockingstorefiles">11.4.7. <code class="varname">hbase.hstore.blockingStoreFiles</code></a></span></dt><dt><span class="section"><a href="perf.configurations.html#perf.hregion.memstore.block.multiplier">11.4.8. <code class="varname">hbase.hregion.memstore.block.multiplier</code></a></span></dt></dl></dd><dt><span class="section"><a href="perf.zookeeper.html">11.5. ZooKeeper</a></span></dt><dt><span class="section"><a href="perf.schema.html">11.6. Schema Design</a></span></dt><dd><dl><dt><span class="section"><a href="perf.schema.html#perf.number.of.cfs">11.6.1. Number of Column Families</a></span></dt><dt><span class="section"><a href="perf.schema.html#perf.schema.keys">11.6.2. Key and Attribute Lengths</a></span></dt><dt><span class="section"><a href="perf.schema.html#schema.regionsize">11.6.3. Table RegionSize</a></span></dt><dt><span class="section"><a href="perf.schema.html#schema.bloom">11.6.4. Bloom Filters</a></span></dt><dt><span class="section"><a href="perf.schema.html#schema.cf.blocksize">11.6.5. ColumnFamily BlockSize</a></span></dt><dt><span class="section"><a href="perf.schema.html#cf.in.memory">11.6.6. In-Memory ColumnFamilies</a></span></dt><dt><span class="section"><a href="perf.schema.html#perf.compression">11.6.7. Compression</a></span></dt></dl></dd><dt><span class="section"><a href="perf.writing.html">11.7. Writing to HBase</a></span></dt><dd><dl><dt><span class="section"><a href="perf.writing.html#perf.batch.loading">11.7.1. Batch Loading</a></span></dt><dt><span class="section"><a href="perf.writing.html#precreate.regions">11.7.2. 
           Table Creation: Pre-Creating Regions
           </a></span></dt><dt><span class="section"><a href="perf.writing.html#def.log.flush">11.7.3. 
             Table Creation: Deferred Log Flush
           </a></span></dt><dt><span class="section"><a href="perf.writing.html#perf.hbase.client.autoflush">11.7.4. HBase Client:  AutoFlush</a></span></dt><dt><span class="section"><a href="perf.writing.html#perf.hbase.client.putwal">11.7.5. HBase Client:  Turn off WAL on Puts</a></span></dt><dt><span class="section"><a href="perf.writing.html#perf.hbase.client.regiongroup">11.7.6. HBase Client: Group Puts by RegionServer</a></span></dt><dt><span class="section"><a href="perf.writing.html#perf.hbase.write.mr.reducer">11.7.7. MapReduce:  Skip The Reducer</a></span></dt><dt><span class="section"><a href="perf.writing.html#perf.one.region">11.7.8. Anti-Pattern:  One Hot Region</a></span></dt></dl></dd><dt><span class="section"><a href="perf.reading.html">11.8. Reading from HBase</a></span></dt><dd><dl><dt><span class="section"><a href="perf.reading.html#perf.hbase.client.caching">11.8.1. Scan Caching</a></span></dt><dt><span class="section"><a href="perf.reading.html#perf.hbase.client.selection">11.8.2. Scan Attribute Selection</a></span></dt><dt><span class="section"><a href="perf.reading.html#perf.hbase.mr.input">11.8.3. MapReduce - Input Splits</a></span></dt><dt><span class="section"><a href="perf.reading.html#perf.hbase.client.scannerclose">11.8.4. Close ResultScanners</a></span></dt><dt><span class="section"><a href="perf.reading.html#perf.hbase.client.blockcache">11.8.5. Block Cache</a></span></dt><dt><span class="section"><a href="perf.reading.html#perf.hbase.client.rowkeyonly">11.8.6. Optimal Loading of Row Keys</a></span></dt><dt><span class="section"><a href="perf.reading.html#perf.hbase.read.dist">11.8.7. Concurrency:  Monitor Data Spread</a></span></dt></dl></dd><dt><span class="section"><a href="perf.deleting.html">11.9. Deleting from HBase</a></span></dt><dd><dl><dt><span class="section"><a href="perf.deleting.html#perf.deleting.queue">11.9.1. Using HBase Tables as Queues</a></span></dt><dt><span class="section"><a href="perf.deleting.html#perf.deleting.rpc">11.9.2. Delete RPC Behavior</a></span></dt></dl></dd><dt><span class="section"><a href="perf.hdfs.html">11.10. HDFS</a></span></dt><dd><dl><dt><span class="section"><a href="perf.hdfs.html#perf.hdfs.curr">11.10.1. Current Issues With Low-Latency Reads</a></span></dt><dt><span class="section"><a href="perf.hdfs.html#perf.hdfs.comp">11.10.2. Performance Comparisons of HBase vs. HDFS</a></span></dt></dl></dd><dt><span class="section"><a href="perf.ec2.html">11.11. Amazon EC2</a></span></dt><dt><span class="section"><a href="perf.casestudy.html">11.12. Case Studies</a></span></dt>
       </dl>
     </div><div class="section" title="11.1.&nbsp;Operating System"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.os"></a>11.1.&nbsp;Operating System</h2></div></div></div><div class="section" title="11.1.1.&nbsp;Memory"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.ram"></a>11.1.1.&nbsp;Memory</h3></div></div></div><p>RAM, RAM, RAM.  Don't starve HBase.</p></div><div class="section" title="11.1.2.&nbsp;64-bit"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.64"></a>11.1.2.&nbsp;64-bit</h3></div></div></div><p>Use a 64-bit platform (and 64-bit JVM).</p></div><div class="section" title="11.1.3.&nbsp;Swapping"><div class="titlepage"><div><div><h3 class="title"><a name="perf.os.swap"></a>11.1.3.&nbsp;Swapping</h3></div></div></div><p>Watch out for swapping.  Set swappiness to 0.</p></div></div></div>
    
    <div class="section" title="11.2.&nbsp;Network"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.network"></a>11.2.&nbsp;Network</h2></div></div></div><p>
    Perhaps the most important factor in avoiding network issues degrading Hadoop and HBbase performance is the switching hardware
    that is used, decisions made early in the scope of the project can cause major problems when you double or triple the size of your cluster (or more). 
    </p><p>
    Important items to consider:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Switching capacity of the device</li><li class="listitem">Number of systems connected</li><li class="listitem">Uplink capacity</li></ul></div><p>
    </p><div class="section" title="11.2.1.&nbsp;Single Switch"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.1switch"></a>11.2.1.&nbsp;Single Switch</h3></div></div></div><p>The single most important factor in this configuration is that the switching capacity of the hardware is capable of 
      handling the traffic which can be generated by all systems connected to the switch. Some lower priced commodity hardware
      can have a slower switching capacity than could be utilized by a full switch. 
      </p></div><div class="section" title="11.2.2.&nbsp;Multiple Switches"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.2switch"></a>11.2.2.&nbsp;Multiple Switches</h3></div></div></div><p>Multiple switches are a potential pitfall in the architecture.   The most common configuration of lower priced hardware is a
      simple 1Gbps uplink from one switch to another. This often overlooked pinch point can easily become a bottleneck for cluster communication. 
      Especially with MapReduce jobs that are both reading and writing a lot of data the communication across this uplink could be saturated.
      </p><p>Mitigation of this issue is fairly simple and can be accomplished in multiple ways:
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Use appropriate hardware for the scale of the cluster which you're attempting to build.</li><li class="listitem">Use larger single switch configurations i.e. single 48 port as opposed to 2x 24 port</li><li class="listitem">Configure port trunking for uplinks to utilize multiple interfaces to increase cross switch bandwidth.</li></ul></div><p>
      </p></div><div class="section" title="11.2.3.&nbsp;Multiple Racks"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.multirack"></a>11.2.3.&nbsp;Multiple Racks</h3></div></div></div><p>Multiple rack configurations carry the same potential issues as multiple switches, and can suffer performance degradation from two main areas:
         </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Poor switch capacity performance</li><li class="listitem">Insufficient uplink to another rack</li></ul></div><p>
      If the the switches in your rack have appropriate switching capacity to handle all the hosts at full speed, the next most likely issue will be caused by homing 
      more of your cluster across racks.  The easiest way to avoid issues when spanning multiple racks is to use port trunking to create a bonded uplink to other racks.
      The downside of this method however, is in the overhead of ports that could potentially be used. An example of this is, creating an 8Gbps port channel from rack
      A to rack B, using 8 of your 24 ports to communicate between racks gives you a poor ROI, using too few however can mean you're not getting the most out of your cluster. 
      </p><p>Using 10Gbe links between racks will greatly increase performance, and assuming your switches support a 10Gbe uplink or allow for an expansion card will allow you to
      save your ports for machines as opposed to uplinks.
      </p></div><div class="section" title="11.2.4.&nbsp;Network Interfaces"><div class="titlepage"><div><div><h3 class="title"><a name="perf.network.ints"></a>11.2.4.&nbsp;Network Interfaces</h3></div></div></div><p>Are all the network interfaces functioning correctly?  Are you sure?  See the Troubleshooting Case Study in <a class="xref" href="casestudies.perftroub.html#casestudies.slownode" title="13.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)">Section&nbsp;13.3.1, “Case Study #1 (Performance Issue On A Single Node)”</a>.
      </p></div></div>
      
     <div class="section" title="12.2. Daemons">
       <div class="titlepage">
         <div>
           <div>
             <div class="titlepage">
               <div>
                 <div>
                   <h2 class="title">&nbsp;</h2>
                 </div>
               </div>
             </div>
             <div class="toc">
               <p>&nbsp;</p>
             </div>
             <p>可以从 <a class="link" href="http://wiki.apache.org/hadoop/PerformanceTuning" target="_top">wiki
               Performance Tuning</a>看起。这个文档讲了一些主要的影响性能的方面:RAM, 压缩, JVM 设置, 等等。然后，可以看看下面的补充内容。</p>
             <div class="note" title="打开RPC-level日志" style="margin-left: 0.5in; margin-right: 0.5in;">
               <h3 class="title"><a name="rpc.logging"></a>打开RPC-level日志</h3>
               <p>在RegionServer打开RPC-level的日志对于深度的优化是有好处的。一旦打开，日志将喷涌而出。所以不建议长时间打开，只能看一小段时间。要想启用RPC-level的职责，可以使用RegionServer UI点击<span class="emphasis"><em>Log Level</em></span>。将 <code class="classname">org.apache.hadoop.ipc</code> 的日志级别设为<code class="varname">DEBUG</code>。然后tail RegionServer的日志，进行分析。</p>
               <p>要想关闭，只要把日志级别设为<code class="varname">INFO</code>就可以了. </p>
             </div>
             <div class="section" title="13.1. Java">
               <div class="titlepage">
                 <div>
                   <div>
                     <h2 class="title" style="clear: both"><a name="jvm"></a>11.3.&nbsp;Java</h2>
                   </div>
                 </div>
               </div>
               <div class="section" title="13.1.1. 垃圾收集和HBase">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="gc"></a>11.3.1.&nbsp;垃圾收集和HBase</h3>
                     </div>
                   </div>
                 </div>
                 <div class="section" title="13.1.1.1. 长时间GC停顿">
                   <div class="titlepage">
                     <div>
                       <div>
                         <h4 class="title"><a name="gcpause"></a>11.3.1.1.&nbsp;长时间GC停顿</h4>
                       </div>
                     </div>
                   </div>
                   <p>在这个PPT <a class="link" href="http://www.slideshare.net/cloudera/hbase-hug-presentation" target="_top">Avoiding
                     Full GCs with MemStore-Local Allocation Buffers</a>, Todd Lipcon描述列在Hbase中常见的两种stop-the-world的GC操作，尤其是在loading的时候。一种是CMS失败的模式(译者注:CMS是一种GC的算法)，另一种是老一代的堆碎片导致的。要想定位第一种，只要将CMS执行的时间提前就可以了，加入<code class="code">-XX:CMSInitiatingOccupancyFraction</code>参数，把值调低。可以先从60%和70%开始(这个值调的越低，触发的GC次数就越多，消耗的CPU时间就越长)。要想定位第二种错误，Todd加入了一个实验性的功能，在Hbase 0.90.x中这个是要明确指定的(在0.92.x中，这个是默认项)，将你的<code class="classname">Configuration</code>中的<code class="code">hbase.hregion.memstore.mslab.enabled</code>设置为true。详细信息，可以看这个PPT. </p>
                 </div>
               </div>
             </div>
             <div class="section" title="13.2. 配置">
               <div class="titlepage">
                 <div>
                   <div>
                     <h2 class="title" style="clear: both"><a name="perf.configurations"></a>11.4.&nbsp;配置</h2>
                   </div>
                 </div>
               </div>
               <p>参见<a class="xref" href="book.htm#recommended_configurations" title="3.6. 推荐的配置">Section&nbsp;2.8.2, “推荐的配置”</a>.</p>
               <div class="section" title="13.2.1. Regions的数目">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="perf.number.of.regions"></a>11.4.1.&nbsp;Regions的数目</h3>
                     </div>
                   </div>
                 </div>
                 <p>Hbase中region的数目可以根据<a class="xref" href="book.htm#bigger.regions" title="3.6.5. 更大的 Regions">Section&nbsp;3.6.5, “更大的 Regions”</a>调整.也可以参见 <a class="xref" href="book.htm#arch.regions.size" title="12.3.1. Region大小">Section&nbsp;12.3.1, “Region大小”</a></p>
               </div>
               <div class="section" title="13.2.2. 管理压缩">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="perf.compactions.and.splits"></a>11.4.2.&nbsp;管理压缩</h3>
                     </div>
                   </div>
                 </div>
                 <p>对于大型的系统，你需要考虑管理<a class="link" href="book.htm#disable.splitting" title="3.6.6. 管理 Splitting">压缩和分割</a></p>
               </div>
               <div class="section" title="13.2.3. 压缩">
                 <div class="titlepage">
                   <div>
                     <div></div>
                   </div>
                 </div>
               </div>
               <div class="section" title="13.2.4. hbase.regionserver.handler.count">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="perf.handlers"></a>11.4.3.&nbsp;<code class="varname">hbase.regionserver.handler.count</code></h3>
                     </div>
                   </div>
                 </div>
                 <p>参见<a class="xref" href="book.htm#hbase.regionserver.handler.count" title="hbase.regionserver.handler.count"><code class="varname">hbase.regionserver.handler.count</code></a>.这个参数的本质是设置一个RegsionServer可以同时处理多少请求。 如果定的太高，吞吐量反而会降低;如果定的太低，请求会被阻塞，得不到响应。你可以<a class="xref" href="book.htm#rpc.logging" title="打开RPC-level日志">打开RPC-level日志</a>读Log，来决定对于你的集群什么值是合适的。(请求队列也是会消耗内存的) </p>
               </div>
               <div class="section" title="13.2.5. hfile.block.cache.size">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="perf.hfile.block.cache.size"></a>11.4.4.&nbsp;<code class="varname">hfile.block.cache.size</code></h3>
                     </div>
                   </div>
                 </div>
                 <p>参见 <a class="xref" href="book.htm#hfile.block.cache.size" title="hfile.block.cache.size"><code class="varname">hfile.block.cache.size</code></a>. 对于RegionServer进程的内存设置。 </p>
               </div>
               <div class="section" title="13.2.6. hbase.regionserver.global.memstore.upperLimit">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="perf.rs.memstore.upperlimit"></a>11.4.5.&nbsp;<code class="varname">hbase.regionserver.global.memstore.upperLimit</code></h3>
                     </div>
                   </div>
                 </div>
                 <p>参见 <a class="xref" href="book.htm#hbase.regionserver.global.memstore.upperLimit" title="hbase.regionserver.global.memstore.upperLimit"><code class="varname">hbase.regionserver.global.memstore.upperLimit</code></a>.  这个内存设置是根据RegionServer的需要来设定。 </p>
               </div>
               <div class="section" title="13.2.7. hbase.regionserver.global.memstore.lowerLimit">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="perf.rs.memstore.lowerlimit"></a>11.4.6.&nbsp;<code class="varname">hbase.regionserver.global.memstore.lowerLimit</code></h3>
                     </div>
                   </div>
                 </div>
                 <p>参见 <a class="xref" href="book.htm#hbase.regionserver.global.memstore.lowerLimit" title="hbase.regionserver.global.memstore.lowerLimit"><code class="varname">hbase.regionserver.global.memstore.lowerLimit</code></a>.  
                   这个内存设置是根据RegionServer的需要来设定。 </p>
               </div>
               <div class="section" title="13.2.8. hbase.hstore.blockingStoreFiles">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="perf.hstore.blockingstorefiles"></a>11.4.7.&nbsp;<code class="varname">hbase.hstore.blockingStoreFiles</code></h3>
                     </div>
                   </div>
                 </div>
                 <p>参见<a class="xref" href="book.htm#hbase.hstore.blockingStoreFiles" title="hbase.hstore.blockingStoreFiles"><code class="varname">hbase.hstore.blockingStoreFiles</code></a>.  
                   如果在RegionServer的Log中block,提高这个值是有帮助的。 </p>
               </div>
               <div class="section" title="13.2.9. hbase.hregion.memstore.block.multiplier">
                 <div class="titlepage">
                   <div>
                     <div>
                       <h3 class="title"><a name="perf.hregion.memstore.block.multiplier"></a>11.4.8.&nbsp;<code class="varname">hbase.hregion.memstore.block.multiplier</code></h3>
                     </div>
                   </div>
                 </div>
                 <p>参见 <a class="xref" href="book.htm#hbase.hregion.memstore.block.multiplier" title="hbase.hregion.memstore.block.multiplier"><code class="varname">hbase.hregion.memstore.block.multiplier</code></a>.  
                   如果有足够的RAM，提高这个值。 </p>
               </div>
             </div>
             <div class="section" title="13.3. Column Families的数目">
               <div class="titlepage">
                 <div>
                   <div>
                     <div>
                       <div>
                         <div>
                           <h2><a name="perf.zookeeper" id="perf.zookeeper"></a>11.5. ZooKeeper</h2>
                         </div>
                       </div>
                     </div>
                     <p>配置ZooKeeper信息，请参考 <a href="book.htm#zookeeper.html" title="2.5. ZooKeeper">Section 2.5, “ZooKeeper”</a>  , 参看关于使用专用磁盘部分。</p>
                     <div>
                       <div>
                         <div>
                           <h2><a name="perf.schema" id="perf.schema"></a>11.6. Schema Design</h2>
                         </div>
                       </div>
                     </div>
                     <div title="11.6.1. Number of Column Families">
                       <div>
                         <div>
                           <div>
                             <h3><a name="perf.number.of.cfs"></a>11.6.1.  Column Families<span class="title" style="clear: both"> 的数目</span></h3>
                           </div>
                         </div>
                       </div>
                       <p>参见 <a href="book.htm#number.of.cfs.html" title="6.2.  On the number of column families">Section 6.2, “ On the number of column families ”</a>.</p>
                     </div>
                     <div title="11.6.2. Key and Attribute Lengths">
                       <div>
                         <div>
                           <div>
                             <h3><a name="perf.schema.keys"></a>11.6.2. Key and Attribute Lengths</h3>
                           </div>
                         </div>
                       </div>
                       <p>See <a href="book.htm#rowkey.design.html#keysize" title="6.3.2. Try to minimize row and column sizes">Section 6.3.2, “Try to minimize row and column sizes”</a>. See also <a href="book.htm#perf.schema.html#perf.compression.however" title="11.6.7.1. However...">Section 11.6.7.1, “However...”</a> for compression caveats.</p>
                     </div>
                     <div title="11.6.3. Table RegionSize">
                       <div>
                         <div>
                           <div>
                             <h3><a name="schema.regionsize"></a>11.6.3. Table RegionSize</h3>
                           </div>
                         </div>
                       </div>
                       <p>The regionsize can be set on a per-table basis via setFileSize on <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a> in the event where certain tables require different regionsizes than the configured default regionsize.</p>
                       <p>See <a href="book.htm#perf.number.of.regions" title="11.4.1. Number of Regions">Section 11.4.1, “Number of Regions”</a> for more information.</p>
                     </div>
                     <div title="11.6.4. Bloom Filters">
                       <div>
                         <div>
                           <div>
                             <h3><a name="schema.bloom"></a>11.6.4. Bloom Filters</h3>
                           </div>
                         </div>
                       </div>
                       <p>Bloom Filters can be enabled per-ColumnFamily. Use HColumnDescriptor.setBloomFilterType(NONE | ROW | ROWCOL) to enable blooms per Column Family. Default = NONE for no bloom filters. If ROW, the hash of the row will be added to the bloom on each insert. If ROWCOL, the hash of the row + column family + column family qualifier will be added to the bloom on each key insert.</p>
                       <p>See <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> and <a href="book.htm#blooms" title="9.7.6. Bloom Filters">Section 9.7.6, “Bloom Filters”</a> for more information.</p>
                     </div>
                     <div title="11.6.5. ColumnFamily BlockSize">
                       <div>
                         <div>
                           <div>
                             <h3><a name="schema.cf.blocksize"></a>11.6.5. ColumnFamily BlockSize</h3>
                           </div>
                         </div>
                       </div>
                       <p>The blocksize can be configured for each ColumnFamily in a table, and this defaults to 64k. Larger cell values require larger blocksizes. There is an inverse relationship between blocksize and the resulting StoreFile indexes (i.e., if the blocksize is doubled then the resulting indexes should be roughly halved).</p>
                       <p>See <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> and <a href="book.htm#store" title="9.7.5. Store">Section 9.7.5, “Store”</a>for more information.</p>
                     </div>
                     <div title="11.6.6. In-Memory ColumnFamilies">
                       <div>
                         <div>
                           <div>
                             <h3><a name="cf.in.memory"></a>11.6.6. In-Memory ColumnFamilies</h3>
                           </div>
                         </div>
                       </div>
                       <p>ColumnFamilies can optionally be defined as in-memory. Data is still persisted to disk, just like any other ColumnFamily. In-memory blocks have the highest priority in the <a href="book.htm#block.cache" title="9.6.4. Block Cache">Section 9.6.4, “Block Cache”</a>, but it is not a guarantee that the entire table will be in memory.</p>
                       <p>See <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HColumnDescriptor.html" target="_top">HColumnDescriptor</a> for more information.</p>
                     </div>
                     <div title="11.6.7. Compression">
                       <div>
                         <div>
                           <div>
                             <h3><a name="perf.compression"></a>11.6.7. Compression</h3>
                           </div>
                         </div>
                       </div>
                       <p>Production systems should use compression with their ColumnFamily definitions. See <a href="book.htm#compression.html" title="Appendix C. Compression In HBase">Appendix C, <em>Compression In HBase</em></a> for more information.</p>
                       <div title="11.6.7.1. However...">
                         <div>
                           <div>
                             <div>
                               <h4><a name="perf.compression.however"></a>11.6.7.1. However...</h4>
                             </div>
                           </div>
                         </div>
                         <p>Compression deflates data <em>on disk</em>. When it's in-memory (e.g., in the MemStore) or on the wire (e.g., transferring between RegionServer and Client) it's inflated. So while using ColumnFamily compression is a best practice, but it's not going to completely eliminate the impact of over-sized Keys, over-sized ColumnFamily names, or over-sized Column names.</p>
                         <p>See <a href="book.htm#rowkey.design.html#keysize" title="6.3.2. Try to minimize row and column sizes">Section 6.3.2, “Try to minimize row and column sizes”</a> on for schema design tips, and <a href="book.htm#regions.arch.html#keyvalue" title="9.7.5.4. KeyValue">Section 9.7.5.4, “KeyValue”</a> for more information on HBase stores data internally.</p>
                       </div>
                     </div>
<p>&nbsp;</p>

<div class="section" title="11.7.&nbsp;Writing to HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.writing"></a>11.7.&nbsp;Writing to HBase</h2></div></div></div><div class="section" title="11.7.1.&nbsp;Batch Loading"><div class="titlepage"><div><div><h3 class="title"><a name="perf.batch.loading"></a>11.7.1.&nbsp;<span class="title" style="clear: both">批量</span> Loading</h3></div></div></div><p>如果可以的话，尽量使用批量导入工具，参见 <a class="xref" href="arch.bulk.load.html" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, “Bulk Loading”</a><a class="link" href="http://hbase.apache.org/bulk-loads.html" target="_top"></a>.否则就要详细看看下面的内容。</p></div><div class="section" title="11.7.2.&nbsp; Table Creation: Pre-Creating Regions"><div class="titlepage"><div><div><h3 class="title"><a name="precreate.regions"></a>11.7.2.&nbsp;Table创建: 预创建Regions </h3>
</div></div></div><p>默认情况下Hbase创建Table会新建一个region。执行批量导入，意味着所有的client会写入这个region，直到这个region足够大，以至于分裂。一个有效的提高批量导入的性能的方式，是预创建空的region。最好稍保守一点，因为过多的region会实实在在的降低性能。下面是一个预创建region的例子。
                   (注意：这个例子里需要根据应用的key进行调整。): </p>
<p>
</p><pre class="programlisting">public static boolean createTable(HBaseAdmin admin, HTableDescriptor table, byte[][] splits)
throws IOException {
  try {
    admin.createTable( table, splits );
    return true;
  } catch (TableExistsException e) {
    logger.info("table " + table.getNameAsString() + " already exists");
    // the table already exists...
    return false;  
  }
}

public static byte[][] getHexSplits(String startKey, String endKey, int numRegions) {
  byte[][] splits = new byte[numRegions-1][];
  BigInteger lowestKey = new BigInteger(startKey, 16);
  BigInteger highestKey = new BigInteger(endKey, 16);
  BigInteger range = highestKey.subtract(lowestKey);
  BigInteger regionIncrement = range.divide(BigInteger.valueOf(numRegions));
  lowestKey = lowestKey.add(regionIncrement);
  for(int i=0; i &lt; numRegions-1;i++) {
    BigInteger key = lowestKey.add(regionIncrement.multiply(BigInteger.valueOf(i)));
    byte[] b = String.format("%016x", key).getBytes();
    splits[i] = b;
  }
  return splits;
}</pre><p>
  </p></div><div class="section" title="11.7.3.&nbsp; Table Creation: Deferred Log Flush"><div class="titlepage"><div><div><h3 class="title"><a name="def.log.flush"></a>11.7.3.&nbsp;
    Table创建: Deferred Log Flush
    </h3></div></div></div><p>
The default behavior for Puts using the Write Ahead Log (WAL) is that <code class="classname">HLog</code> edits will be written immediately.  If deferred log flush is used, 
WAL edits are kept in memory until the flush period.  The benefit is aggregated and asynchronous <code class="classname">HLog</code>- writes, but the potential downside is that if
 the RegionServer goes down the yet-to-be-flushed edits are lost.  This is safer, however, than not using WAL at all with Puts.
</p><p>
Deferred log flush can be configured on tables via <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/HTableDescriptor.html" target="_top">HTableDescriptor</a>.  The default value of <code class="varname">hbase.regionserver.optionallogflushinterval</code> is 1000ms.
</p></div><div class="section" title="11.7.4.&nbsp;HBase Client: AutoFlush"><div class="titlepage"><div><div>
          <h3 class="title"><a name="perf.hbase.client.autoflush"></a>11.7.4.&nbsp;HBase 客户端:  
              AutoFlush</h3><div class="titlepage">
                <div>
            <div>
              <h3 class="title">&nbsp;</h3>
            </div>
          </div>
        </div>
        <p>当你进行大量的Put的时候，要确认你的<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html" target="_top">HTable</a>的setAutoFlush是关闭着的。否则的话，每执行一个Put就要想RegionServer发一个请求。通过<code class="code"> htable.add(Put)</code> 和 <code class="code"> htable.add( &lt;List&gt; Put)</code>来将Put添加到写缓冲中。如果 <code class="code">autoFlush = false</code>，要等到写缓冲都填满的时候才会发起请求。要想显式的发起请求，可以调用<code class="methodname">flushCommits</code>。在<code class="classname">HTable</code>实例上进行的<code class="methodname">close</code>操作也会发起<code class="methodname">flushCommits</code></p>
</div></div></div></div><div class="section" title="11.7.5.&nbsp;HBase Client: Turn off WAL on Puts"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.putwal"></a>11.7.5.&nbsp;HBase 客户端:  Turn off WAL on Puts</h3></div></div></div><p>A frequently discussed option for increasing throughput on <code class="classname">Put</code>s is to call <code class="code">writeToWAL(false)</code>.  Turning this off means
          that the RegionServer will <span class="emphasis"><em>not</em></span> write the <code class="classname">Put</code> to the Write Ahead Log,
          only into the memstore, HOWEVER the consequence is that if there
          is a RegionServer failure <span class="emphasis"><em>there will be data loss</em></span>.
          If <code class="code">writeToWAL(false)</code> is used, do so with extreme caution.  You may find in actuality that
          it makes little difference if your load is well distributed across the cluster.
      </p><p>In general, it is best to use WAL for Puts, and where loading throughput
          is a concern to use <a class="link" href="perf.writing.html#perf.batch.loading" title="11.7.1.&nbsp;Batch Loading">bulk loading</a> techniques instead.  
      </p></div><div class="section" title="11.7.6.&nbsp;HBase Client: Group Puts by RegionServer"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.regiongroup"></a>11.7.6.&nbsp;HBase 客户端: Group Puts by RegionServer</h3></div></div></div><p>In addition to using the writeBuffer, grouping <code class="classname">Put</code>s by RegionServer can reduce the number of client RPC calls per writeBuffer flush. 
      There is a utility <code class="classname">HTableUtil</code> currently on TRUNK that does this, but you can either copy that or implement your own verison for
      those still on 0.90.x or earlier.
      </p></div><div class="section" title="11.7.7.&nbsp;MapReduce: Skip The Reducer"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.write.mr.reducer"></a>11.7.7.&nbsp;MapReduce:  Skip The Reducer</h3></div></div></div><p>When writing a lot of data to an HBase table from a MR job (e.g., with <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html" target="_top">TableOutputFormat</a>), and specifically where Puts are being emitted
      from the Mapper, skip the Reducer step.  When a Reducer step is used, all of the output (Puts) from the Mapper will get spooled to disk, then sorted/shuffled to other 
      Reducers that will most likely be off-node.  It's far more efficient to just write directly to HBase.   
      </p><p>For summary jobs where HBase is used as a source and a sink, then writes will be coming from the Reducer step (e.g., summarize values then write out result). 
      This is a different processing problem than from the the above case. 
      </p></div><div class="section" title="11.7.8.&nbsp;Anti-Pattern: One Hot Region"><div class="titlepage"><div><div><h3 class="title"><a name="perf.one.region"></a>11.7.8.&nbsp;Anti-Pattern:  One Hot Region</h3></div></div></div><p>If all your data is being written to one region at a time, then re-read the
    section on processing <a class="link" href="rowkey.design.html#timeseries" title="6.3.1.&nbsp; Monotonically Increasing Row Keys/Timeseries Data">timeseries</a> data.</p><p>Also, if you are pre-splitting regions and all your data is <span class="emphasis"><em>still</em></span> winding up in a single region even though
    your keys aren't monotonically increasing, confirm that your keyspace actually works with the split strategy.  There are a 
    variety of reasons that regions may appear "well split" but won't work with your data.   As
    the HBase client communicates directly with the RegionServers, this can be obtained via 
    <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#getRegionLocation%28byte[]%29" target="_top">HTable.getRegionLocation</a>.
    </p><p>See <a class="xref" href="perf.writing.html#precreate.regions" title="11.7.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;11.7.2, “
    Table Creation: Pre-Creating Regions
    ”</a>, as well as <a class="xref" href="perf.configurations.html" title="11.4.&nbsp;HBase Configurations">Section&nbsp;11.4, “HBase Configurations”</a> </p></div></div>
    
    <div class="section" title="11.8.&nbsp;Reading from HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="perf.reading"></a>11.8.&nbsp;Reading from HBase</h2></div></div></div><div class="section" title="11.8.1.&nbsp;Scan Caching"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.caching"></a>11.8.1.&nbsp;Scan Caching</h3></div></div></div><p>如果Hbase的输入源是一个MapReduce Job，要确保输入的<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>的<code class="methodname">setCaching</code>值要比默认值0要大。使用默认值就意味着map-task每一行都会去请求一下region-server。可以把这个值设为500，这样就可以一次传输500行。当然这也是需要权衡的，过大的值会同时消耗客户端和服务端很大的内存，不是越大越好。</p>
    <div class="section" title="11.8.1.1.&nbsp;Scan Caching in MapReduce Jobs"><div class="titlepage"><div><div><h4 class="title"><a name="perf.hbase.client.caching.mr"></a>11.8.1.1.&nbsp;Scan Caching in MapReduce Jobs</h4></div></div></div><p>Scan settings in MapReduce jobs deserve special attention.  Timeouts can result (e.g., UnknownScannerException)
        in Map tasks if it takes longer to process a batch of records before the client goes back to the RegionServer for the
        next set of data.  This problem can occur because there is non-trivial processing occuring per row.  If you process
        rows quickly, set caching higher.  If you process rows more slowly (e.g., lots of transformations per row, writes), 
        then set caching lower.
        </p><p>Timeouts can also happen in a non-MapReduce use case (i.e., single threaded HBase client doing a Scan), but the
        processing that is often performed in MapReduce jobs tends to exacerbate this issue.
        </p></div></div><div class="section" title="11.8.2.&nbsp;Scan Attribute Selection"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.selection"></a>11.8.2.&nbsp;Scan 属性选择</h3></div></div></div><p>当Scan用来处理大量的行的时候(尤其是作为MapReduce的输入)，要注意的是选择了什么字段。如果调用了 <code class="code">scan.addFamily</code>，这个column family的所有属性都会返回。如果只是想过滤其中的一小部分，就指定那几个column，否则就会造成很大浪费，影响性能。 </p>
        </div><div class="section" title="11.8.3.&nbsp;MapReduce - Input Splits"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.mr.input"></a>11.8.3.&nbsp;MapReduce - Input Splits</h3></div></div></div><p>For MapReduce jobs that use HBase tables as a source, if there a pattern where the "slow" map tasks seem to 
        have the same Input Split (i.e., the RegionServer serving the data), see the 
        Troubleshooting Case Study in <a class="xref" href="casestudies.perftroub.html#casestudies.slownode" title="13.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)">Section&nbsp;13.3.1, “Case Study #1 (Performance Issue On A Single Node)”</a>.
        </p></div><div class="section" title="11.8.4.&nbsp;Close ResultScanners"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.scannerclose"></a>11.8.4.&nbsp;关闭 ResultScanners</h3></div></div></div><p>这与其说是提高性能，倒不如说是避免发生性能问题。如果你忘记了关闭<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/ResultScanner.html" target="_top">ResultScanners</a>，会导致RegionServer出现问题。所以一定要把ResultScanner包含在try/catch 块中... </p>
        <pre class="programlisting">Scan scan = new Scan();
// set attrs...
ResultScanner rs = htable.getScanner(scan);
try {
  for (Result r = rs.next(); r != null; r = rs.next()) {
  // process result...
} finally {
  rs.close();  // always close the ResultScanner!
}
htable.close();</pre></div><div class="section" title="11.8.5.&nbsp;Block Cache"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.client.blockcache"></a>11.8.5.&nbsp;块缓存</h3>
</div></div></div><p><a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">Scan</a>实例可以在RegionServer中使用块缓存，可以由<code class="methodname">setCacheBlocks</code>方法控制。如果Scan是MapReduce的输入源，要将这个值设置为 <code class="varname">false</code>。对于经常读到的行，就建议使用块缓冲。</p>
</div><div class="section" title="11.8.6.&nbsp;Optimal Loading of Row Keys"><div class="titlepage"><div><div>
  <h3 class="title"><a name="perf.hbase.client.rowkeyonly"></a>11.8.6.&nbsp;  Row Keys 的负载优化</h3></div></div></div><p>当<a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Scan.html" target="_top">scan</a>一个表的时候，
                   如果仅仅需要row key（不需要no families, qualifiers, values 和 timestamps）,在加入FilterList的时候，要使用Scanner的<code class="methodname">setFilter</code>方法的时候，要填上<code class="varname">MUST_PASS_ALL</code>操作参数(译者注：相当于And操作符)。一个FilterList要包含一个 <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/FirstKeyOnlyFilter.html" target="_top">FirstKeyOnlyFilter</a> 和一个 <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/KeyOnlyFilter.html" target="_top">KeyOnlyFilter</a>.通过这样的filter组合，就算在最坏的情况下，RegionServer只会从磁盘读一个值，同时最小化客户端的网络带宽占用。 </p>
</div><div class="section" title="11.8.7.&nbsp;Concurrency: Monitor Data Spread"><div class="titlepage"><div><div><h3 class="title"><a name="perf.hbase.read.dist"></a>11.8.7.&nbsp;Concurrency:  Monitor Data Spread</h3></div></div></div><p>When performing a high number of concurrent reads, monitor the data spread of the target tables.  If the target table(s) have 
      too few regions then the reads could likely be served from too few nodes.  </p><p>See <a class="xref" href="perf.writing.html#precreate.regions" title="11.7.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;11.7.2, “
    Table Creation: Pre-Creating Regions
    ”</a>, as well as <a class="xref" href="perf.configurations.html" title="11.4.&nbsp;HBase Configurations">Section&nbsp;11.4, “HBase Configurations”</a> </p></div></div>
    
    
    
                     <div>
                       <div>
                         <div>
                           <h2><a name="perf.deleting" id="perf.deleting"></a>11.9. Deleting from HBase</h2>
                         </div>
                       </div>
                     </div>
                     <div title="11.9.1. Using HBase Tables as Queues">
                       <div>
                         <div>
                           <div>
                             <h3><a name="perf.deleting.queue"></a>11.9.1. Using HBase Tables as Queues</h3>
                           </div>
                         </div>
                       </div>
                       <p>HBase tables are sometimes used as queues. In this case, special care must be taken to regularly perform major compactions on tables used in this manner. As is documented in <a href="book.htm#datamodel.html" title="Chapter 5. Data Model">Chapter 5, <em>Data Model</em></a>, marking rows as deleted creates additional StoreFiles which then need to be processed on reads. Tombstones only get cleaned up with major compactions.</p>
                       <p>See also <a href="book.htm#regions.arch.html#compaction" title="9.7.5.5. Compaction">Section 9.7.5.5, “Compaction”</a> and <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>.</p>
                     </div>
                     <div title="11.9.2. Delete RPC Behavior">
                       <div>
                         <div>
                           <div>
                             <h3><a name="perf.deleting.rpc"></a>11.9.2. Delete RPC Behavior</h3>
                           </div>
                         </div>
                       </div>
                       <p>Be aware that htable.delete(Delete) doesn't use the writeBuffer. It will execute an RegionServer RPC with each invocation. For a large number of deletes, consider htable.delete(List).</p>
                       <p>See <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29" target="_top">http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HTable.html#delete%28org.apache.hadoop.hbase.client.Delete%29</a></p>
                     </div>
                     <div>
                       <div>
                         <div>
                           <h2><a name="perf.hdfs" id="perf.hdfs"></a>11.10. HDFS</h2>
                         </div>
                       </div>
                     </div>
                     <p>Because HBase runs on <a href="book.htm#arch.hdfs.html" title="9.9. HDFS">Section 9.9, “HDFS”</a> it is important to understand how it works and how it affects HBase.</p>
                     <div title="11.10.1. Current Issues With Low-Latency Reads">
                       <div>
                         <div>
                           <div>
                             <h3><a name="perf.hdfs.curr"></a>11.10.1. Current Issues With Low-Latency Reads</h3>
                           </div>
                         </div>
                       </div>
                       <p>The original use-case for HDFS was batch processing. As such, there low-latency reads were historically not a priority. With the increased adoption of HBase this is changing, and several improvements are already in development. See the <a href="https://issues.apache.org/jira/browse/HDFS-1599" target="_top">Umbrella Jira Ticket for HDFS Improvements for HBase</a>.</p>
                     </div>
                     <div title="11.10.2. Performance Comparisons of HBase vs. HDFS">
                       <div>
                         <div>
                           <div>
                             <h3><a name="perf.hdfs.comp"></a>11.10.2. Performance Comparisons of HBase vs. HDFS</h3>
                           </div>
                         </div>
                       </div>
                       <p>A fairly common question on the dist-list is why HBase isn't as performant as HDFS files in a batch context (e.g., as a MapReduce source or sink). The short answer is that HBase is doing a lot more than HDFS (e.g., reading the KeyValues, returning the most current row or specified timestamps, etc.), and as such HBase is 4-5 times slower than HDFS in this processing context. Not that there isn't room for improvement (and this gap will, over time, be reduced), but HDFS will always be faster in this use-case.</p>
                     </div>
                     <div>
                       <div>
                         <div>
                           <h2><a name="perf.ec2" id="perf.ec2"></a>11.11. Amazon EC2</h2>
                         </div>
                       </div>
                     </div>
                     <p>Performance questions are common on Amazon EC2 environments because it is a shared environment. You will not see the same throughput as a dedicated server. In terms of running tests on EC2, run them several times for the same reason (i.e., it's a shared environment and you don't know what else is happening on the server).</p>
                     <p>If you are running on EC2 and post performance questions on the dist-list, please state this fact up-front that because EC2 issues are practically a separate class of performance issues.</p>
                     <div>
                       <div>
                         <div>
                           <h2><a name="perf.casestudy" id="perf.casestudy"></a>11.12. Case Studies</h2>
                         </div>
                       </div>
                     </div>
                     <p>For Performance and Troubleshooting Case Studies, see <a href="book.htm#casestudies.html" title="Chapter 13. Case Studies">Chapter 13, <em>Case Studies</em></a>.</p>
                     <div class="titlepage">
                       <div>
                         <div>
                           <h2 class="title"><a name="trouble"></a>Chapter&nbsp;12.&nbsp;Hbase的故障排除和Debug</h2>
                         </div>
                       </div>
                     </div>
                     <div class="toc">
                       <p><b>Table of Contents</b></p>
                       <dl>
                         <dt><a href="book.htm#trouble.general">12.1. 一般准则</a></dt>
                         <dt><a href="book.htm#trouble.log">12.2. Logs</a></dt>
                         <dd>
                           <dl>
                             <dt><a href="book.htm#trouble.log.locations">12.2.1. Log 位置</a></dt>
                           </dl>
                         </dd>
                         <dt><a href="book.htm#trouble.tools">12.3. 工具</a></dt>
                         <dd>
                           <dl>
                             <dt><a href="book.htm#trouble.tools.searchhadoop">12.3.1. search-hadoop.com</a></dt>
                             <dt><a href="book.htm#trouble.tools.tail">12.3.2. tail</a></dt>
                             <dt><a href="book.htm#trouble.tools.top">12.3.3. top</a></dt>
                             <dt><a href="book.htm#trouble.tools.jps">12.3.4. jps</a></dt>
                             <dt><a href="book.htm#trouble.tools.jstack">12.3.5. jstack</a></dt>
                             <dt><a href="book.htm#trouble.tools.opentsdb">12.3.6. OpenTSDB</a></dt>
                             <dt><a href="book.htm#trouble.tools.clustersshtop">12.3.7. clusterssh+top</a></dt>
                           </dl>
                         </dd>
                         <div class="section" title="12.5.&nbsp;Client"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.client"></a>12.5.&nbsp;客户端</h2></div></div></div><p>For more information on the HBase client, see <a class="xref" href="client.html" title="9.3.&nbsp;Client">Section&nbsp;9.3, “Client”</a>. 
       </p><div class="section" title="12.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.scantimeout"></a>12.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException</h3></div></div></div><p>This is thrown if the time between RPC calls from the client to RegionServer exceeds the scan timeout.  
            For example, if <code class="code">Scan.setCaching</code> is set to 500, then there will be an RPC call to fetch the next batch of rows every 500 <code class="code">.next()</code> calls on the ResultScanner
            because data is being transferred in blocks of 500 rows to the client.  Reducing the setCaching value may be an option, but setting this value too low makes for inefficient
            processing on numbers of rows.
            </p><p>See <a class="xref" href="perf.reading.html#perf.hbase.client.caching" title="11.8.1.&nbsp;Scan Caching">Section&nbsp;11.8.1, “Scan Caching”</a>.
            </p></div><div class="section" title="12.5.2.&nbsp;Shell or client application throws lots of scary exceptions during normal operation"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.scarylogs"></a>12.5.2.&nbsp;Shell or client application throws lots of scary exceptions during normal operation</h3></div></div></div><p>Since 0.20.0 the default log level for <code class="code">org.apache.hadoop.hbase.*</code>is DEBUG. </p><p>
            On your clients, edit <code class="filename">$HBASE_HOME/conf/log4j.properties</code> and change this: <code class="code">log4j.logger.org.apache.hadoop.hbase=DEBUG</code> to this: <code class="code">log4j.logger.org.apache.hadoop.hbase=INFO</code>, or even <code class="code">log4j.logger.org.apache.hadoop.hbase=WARN</code>. 
            </p></div><div class="section" title="12.5.3.&nbsp;Long Client Pauses With Compression"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.longpauseswithcompression"></a>12.5.3.&nbsp;Long Client Pauses With Compression</h3></div></div></div><p>This is a fairly frequent question on the HBase dist-list.  The scenario is that a client is typically inserting a lot of data into a 
            relatively un-optimized HBase cluster.  Compression can exacerbate the pauses, although it is not the source of the problem.</p><p>See <a class="xref" href="perf.writing.html#precreate.regions" title="11.7.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;11.7.2, “
    Table Creation: Pre-Creating Regions
    ”</a> on the pattern for pre-creating regions and confirm that the table isn't starting with a single region.</p><p>See <a class="xref" href="perf.configurations.html" title="11.4.&nbsp;HBase Configurations">Section&nbsp;11.4, “HBase Configurations”</a> for cluster configuration, particularly <code class="code">hbase.hstore.blockingStoreFiles</code>, <code class="code">hbase.hregion.memstore.block.multiplier</code>, 
            <code class="code">MAX_FILESIZE</code> (region size), and <code class="code">MEMSTORE_FLUSHSIZE.</code>  </p><p>A slightly longer explanation of why pauses can happen is as follows:  Puts are sometimes blocked on the MemStores which are blocked by the flusher thread which is blocked because there are 
            too many files to compact because the compactor is given too many small files to compact and has to compact the same data repeatedly.  This situation can occur even with minor compactions.
            Compounding this situation, HBase doesn't compress data in memory.  Thus, the 64MB that lives in the MemStore could become a 6MB file after compression - which results in a smaller StoreFile.  The upside is that
            more data is packed into the same region, but performance is achieved by being able to write larger files - which is why HBase waits until the flushize before writing a new StoreFile.  And smaller StoreFiles
            become targets for compaction.  Without compression the files are much bigger and don't need as much compaction, however this is at the expense of I/O.   
            </p><p>
            For additional information, see this thread on <a class="link" href="http://search-hadoop.com/m/WUnLM6ojHm1/Long+client+pauses+with+compression&amp;subj=Long+client+pauses+with+compression" target="_top">Long client pauses with compression</a>.
            </p></div><div class="section" title="12.5.4.&nbsp;ZooKeeper Client Connection Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.zookeeper"></a>12.5.4.&nbsp;ZooKeeper Client Connection Errors</h3></div></div></div><p>Errors like this...
</p><pre class="programlisting">11/07/05 11:26:41 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
 unexpected error, closing socket connection and attempting reconnect
 java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
 11/07/05 11:26:43 INFO zookeeper.ClientCnxn: Opening socket connection to
 server localhost/127.0.0.1:2181
 11/07/05 11:26:44 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
 unexpected error, closing socket connection and attempting reconnect
 java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
 11/07/05 11:26:45 INFO zookeeper.ClientCnxn: Opening socket connection to
 server localhost/127.0.0.1:2181
</pre><p>
            ... are either due to ZooKeeper being down, or unreachable due to network issues.            
            </p><p>The utility <a class="xref" href="trouble.tools.html#trouble.tools.builtin.zkcli" title="12.4.1.3.&nbsp;zkcli">Section&nbsp;12.4.1.3, “zkcli”</a> may help investigate ZooKeeper issues.
            </p></div><div class="section" title="12.5.5.&nbsp;Client running out of memory though heap size seems to be stable (but the off-heap/direct heap keeps growing)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.oome.directmemory.leak"></a>12.5.5.&nbsp;Client running out of memory though heap size seems to be stable (but the off-heap/direct heap keeps growing)</h3></div></div></div><p>
You are likely running into the issue that is described and worked through in
the mail thread HBase, mail # user - Suspected memory leak
and continued over in HBase, mail # dev - FeedbackRe: Suspected memory leak.
A workaround is passing your client-side JVM a reasonable value for <code class="code">-XX:MaxDirectMemorySize</code>.  By default,
the <code class="varname">MaxDirectMemorySize</code> is equal to your <code class="code">-Xmx</code> max heapsize setting (if <code class="code">-Xmx</code> is set).
Try seting it to something smaller (for example, one user had success setting it to <code class="code">1g</code> when
they had a client-side heap of <code class="code">12g</code>).  If you set it too small, it will bring on <code class="code">FullGCs</code> so keep
it  a bit hefty.  You want to make this setting client-side only especially if you are running the new experiemental
server-side off-heap cache since this feature depends on being able to use big direct buffers (You may have to keep
separate client-side and server-side config dirs).
            </p></div><div class="section" title="12.5.6.&nbsp;Client Slowdown When Calling Admin Methods (flush, compact, etc.)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.slowdown.admin"></a>12.5.6.&nbsp;Client Slowdown When Calling Admin Methods (flush, compact, etc.)</h3></div></div></div><p>
This is a client issue fixed by <a class="link" href="https://issues.apache.org/jira/browse/HBASE-5073" target="_top">HBASE-5073</a> in 0.90.6.
There was a ZooKeeper leak in the client and the client was getting pummeled by ZooKeeper events with each additional 
invocation of the admin API. 
            </p></div><div class="section" title="12.5.7.&nbsp;Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)])"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.security.rpc"></a>12.5.7.&nbsp;Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)])</h3></div></div></div><p>
There can be several causes that produce this symptom.
           </p><p>
First, check that you have a valid Kerberos ticket. One is required in order to set up communication with a secure HBase cluster. Examine the ticket currently in the credential cache, if any, by running the <span style="color: red">&lt;tt&gt;klist&lt;/tt&gt;</span> command line utility. If no ticket is listed, you must obtain a ticket by running the <span style="color: red">&lt;tt&gt;kinit&lt;/tt&gt;</span> command with either a keytab specified, or by interactively entering a password for the desired principal.
           </p><p>
Then, consult the <span style="color: red">&lt;a&gt;Java Security Guide troubleshooting section&lt;/a&gt;</span>. The most common problem addressed there is resolved by setting <span style="color: red">&lt;tt&gt;javax.security.auth.useSubjectCredsOnly&lt;/tt&gt;</span> system property value to <span style="color: red">&lt;tt&gt;false&lt;/tt&gt;</span>.
           </p><p>
Because of a change in the format in which MIT Kerberos writes its credentials cache, there is a bug in the Oracle JDK 6 Update 26 and earlier that causes Java to be unable to read the Kerberos credentials cache created by versions of MIT Kerberos 1.8.1 or higher. If you have this problematic combination of components in your environment, to work around this problem, first log in with <span style="color: red">&lt;tt&gt;kinit&lt;/tt&gt;</span> and then immediately refresh the credential cache with <span style="color: red">&lt;tt&gt;kinit -R&lt;/tt&gt;</span>. The refresh will rewrite the credential cache without the problematic formatting.
           </p><p>
Finally, depending on your Kerberos configuration, you may need to install the <span style="color: red">&lt;a&gt;Java Cryptography Extension&lt;/a&gt;</span>, or JCE. Insure the JCE jars are on the classpath on both server and client systems.
           </p><p>
You may also need to download the <span style="color: red">&lt;a&gt;unlimited strength JCE policy files&lt;/a&gt;</span>. Uncompress and extract the downloaded file, and install the policy jars into <span style="color: red">&lt;tt&gt;&lt;java-home&gt;/lib/security&lt;/tt&gt;</span>.
           </p></div></div>
           
                         <dt><a href="book.htm#trouble.client">12.4. 客户端</a></dt>
                         <dd>
                           <dl>
                             <dt><a href="book.htm#trouble.client.scantimeout">12.4.1. ScannerTimeoutException</a></dt>
                           </dl>
                         </dd>
                         <dt><a href="book.htm#trouble.rs">12.5. RegionServer</a></dt>
                         <dd>
                           <dl>
                             <dt><a href="book.htm#trouble.rs.startup">12.5.1. 启动错误</a></dt>
                             <dt><a href="book.htm#trouble.rs.runtime">12.5.2. 运行时错误</a></dt>
                             <dt><a href="book.htm#trouble.rs.shutdown">12.5.3. 终止错误</a></dt>
                           </dl>
                         </dd>
                         <dt><a href="book.htm#trouble.master">12.6. Master</a></dt>
                         <dd>
                           <dl>
                             <dt><a href="book.htm#trouble.master.startup">12.6.1. 启动错误</a></dt>
                             <dt><a href="book.htm#trouble.master.startup">12.6.2. 终止错误</a></dt>
                           </dl>
                         </dd>
                       </dl>
                     </div>
                     <div class="section" title="15.1. 一般准则">
                       <div class="titlepage">
                         <div>
                           <div>
                             <h2 class="title" style="clear: both"><a name="trouble.general"></a>12.1.&nbsp;一般准则</h2>
                           </div>
                         </div>
                       </div>
                       <p> 首先可以看看master的log。通常情况下，他总是一行一行的重复信息。如果不是这样，说明有问题，可以Google或是用<a class="link" href="http://search-hadoop.com/" target="_top">search-hadoop.com</a>来搜索遇到的exception。 </p>
                       <p> 一个错误通常不是单独出现在Hbase中的，通常是某一个地方发生了异常，然后对其他的地方发生影响。到处都是exception和stack straces。遇到这样的错误，最好的办法是查日志，找到最初的异常。例如Region会在abort的时候打印一下信息。Grep这个<span class="emphasis"><em>Dump</em></span>就有可能找到最初的异常信息。 </p>
                       <p> RegionServer的自杀是很“正常”的。当一些事情发生错误的，他们就会自杀。如果ulimit和xcievers(最重要的两个设定，详见<a class="xref" href="book.htm#ulimit" title="1.3.1.6.  ulimit 和 nproc">Section&nbsp;2.2.5, “ <code class="varname">ulimit</code> 和 <code class="varname">nproc</code> ”</a>)没有修改，HDFS将无法运转正常，在HBase看来，HDFS死掉了。假想一下，你的MySQL突然无法访问它的文件系统，他会怎么做。同样的事情会发生在Hbase和HDFS上。还有一个造成RegionServer切腹(译者注:竟然用日文词)自杀的常见的原因是，他们执行了一个长时间的GC操作，这个时间超过了ZooKeeper的session timeout。关于GC停顿的详细信息，参见Todd Lipcon的 <a class="link" href="http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" target="_top">3 part blog post</a> by Todd Lipcon
                         和上面的 <a class="xref" href="book.htm#gcpause" title="13.1.1.1. 长时间GC停顿">Section&nbsp;11.3.1.1, “长时间GC停顿”</a>. </p>
                     </div>
                     <div class="section" title="15.2. Logs">
                       <div class="titlepage">
                         <div>
                           <div>
                             <h2 class="title" style="clear: both"><a name="trouble.log"></a>12.2.&nbsp;Logs</h2>
                           </div>
                         </div>
                       </div>
                       <p> 重要日志的位置( &lt;user&gt;是启动服务的用户，&lt;hostname&gt; 是机器的名字) </p>
                       <p> NameNode: <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-namenode-&lt;hostname&gt;.log</code> </p>
                       <p> DataNode: <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-datanode-&lt;hostname&gt;.log</code> </p>
                       <p> JobTracker: <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-jobtracker-&lt;hostname&gt;.log</code> </p>
                       <p> TaskTracker: <code class="filename">$HADOOP_HOME/logs/hadoop-&lt;user&gt;-jobtracker-&lt;hostname&gt;.log</code> </p>
                       <p> HMaster: <code class="filename">$HBASE_HOME/logs/hbase-&lt;user&gt;-master-&lt;hostname&gt;.log</code> </p>
                       <p> RegionServer: <code class="filename">$HBASE_HOME/logs/hbase-&lt;user&gt;-regionserver-&lt;hostname&gt;.log</code> </p>
                       <p> ZooKeeper: <code class="filename">TODO</code> </p>
                       <div class="section" title="15.2.1. Log 位置">
                         <div class="titlepage">
                           <div>
                             <div>
                               <h3 class="title"><a name="trouble.log.locations"></a>12.2.1.&nbsp;Log 位置</h3>
                             </div>
                           </div>
                         </div>
                         <p>对于单节点模式，Log都会在一台机器上，但是对于生产环境，都会运行在一个集群上。</p>
                         <div class="section" title="15.2.1.1. NameNode">
                           <div class="titlepage">
                             <div>
                               <div>
                                 <h4 class="title"><a name="trouble.log.locations.namenode"></a>12.2.1.1.&nbsp;NameNode</h4>
                               </div>
                             </div>
                           </div>
                           <p>NameNode的日志在NameNode server上。HBase Master 通常也运行在NameNode server上，ZooKeeper通常也是这样。</p>
                           <p>对于小一点的机器，JobTracker也通常运行在NameNode server上面。</p>
                         </div>
                         <div class="section" title="15.2.1.2. DataNode">
                           <div class="titlepage">
                             <div>
                               <div>
                                 <h4 class="title"><a name="trouble.log.locations.datanode"></a>12.2.1.2.&nbsp;DataNode</h4>
                               </div>
                             </div>
                           </div>
                           <p>每一台DataNode server有一个HDFS的日志，Region有一个Hbase日志。</p>
                           <p>每个DataNode server还有一份TaskTracker的日志，来记录MapReduce的Task信息。</p>
                         </div>
                       </div>
                     </div>
                     <div class="section" title="12.2.2.&nbsp;Log Levels"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.log.levels"></a>12.2.2.&nbsp;Log Levels</h3></div></div></div><div class="section" title="12.2.2.1.&nbsp;Enabling RPC-level logging"><div class="titlepage"><div><div><h4 class="title"><a name="rpc.logging"></a>12.2.2.1.&nbsp;Enabling RPC-level logging</h4></div></div></div><p>Enabling the RPC-level logging on a RegionServer can often given
           insight on timings at the server.  Once enabled, the amount of log
           spewed is voluminous.  It is not recommended that you leave this
           logging on for more than short bursts of time.  To enable RPC-level
           logging, browse to the RegionServer UI and click on 
           <span class="emphasis"><em>Log Level</em></span>.  Set the log level to <code class="varname">DEBUG</code> for the package
           <code class="classname">org.apache.hadoop.ipc</code> (Thats right, for
           <code class="classname">hadoop.ipc</code>, NOT, <code class="classname">hbase.ipc</code>).  Then tail the RegionServers log.  Analyze.</p><p>To disable, set the logging level back to <code class="varname">INFO</code> level.
           </p></div></div>
           <div class="section" title="12.2.3.&nbsp;JVM Garbage Collection Logs"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.log.gc"></a>12.2.3.&nbsp;JVM Garbage Collection Logs</h3></div></div></div><p>HBase is memory intensive, and using the default GC you can see long pauses in all threads including the <span class="emphasis"><em>Juliet Pause</em></span> aka "GC of Death". 
           To help debug this or confirm this is happening GC logging can be turned on in the Java virtual machine.  
          </p><p>
          To enable, in <code class="filename">hbase-env.sh</code> add:
          </p><pre class="programlisting"> 
export HBASE_OPTS="-XX:+UseConcMarkSweepGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/home/hadoop/hbase/logs/gc-hbase.log"
          </pre><p>
           Adjust the log directory to wherever you log.  Note:  The GC log does NOT roll automatically, so you'll have to keep an eye on it so it doesn't fill up the disk. 
          </p><p>
         At this point you should see logs like so:
          </p><pre class="programlisting">64898.952: [GC [1 CMS-initial-mark: 2811538K(3055704K)] 2812179K(3061272K), 0.0007360 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 
64898.953: [CMS-concurrent-mark-start]
64898.971: [GC 64898.971: [ParNew: 5567K-&gt;576K(5568K), 0.0101110 secs] 2817105K-&gt;2812715K(3061272K), 0.0102200 secs] [Times: user=0.07 sys=0.00, real=0.01 secs] 
          </pre><p>
          </p><p>
           In this section, the first line indicates a 0.0007360 second pause for the CMS to initially mark. This pauses the entire VM, all threads for that period of time.
            </p><p>
           The third line indicates a "minor GC", which pauses the VM for 0.0101110 seconds - aka 10 milliseconds. It has reduced the "ParNew" from about 5.5m to 576k.
           Later on in this cycle we see:
           </p><pre class="programlisting"> 
64901.445: [CMS-concurrent-mark: 1.542/2.492 secs] [Times: user=10.49 sys=0.33, real=2.49 secs] 
64901.445: [CMS-concurrent-preclean-start]
64901.453: [GC 64901.453: [ParNew: 5505K-&gt;573K(5568K), 0.0062440 secs] 2868746K-&gt;2864292K(3061272K), 0.0063360 secs] [Times: user=0.05 sys=0.00, real=0.01 secs] 
64901.476: [GC 64901.476: [ParNew: 5563K-&gt;575K(5568K), 0.0072510 secs] 2869283K-&gt;2864837K(3061272K), 0.0073320 secs] [Times: user=0.05 sys=0.01, real=0.01 secs] 
64901.500: [GC 64901.500: [ParNew: 5517K-&gt;573K(5568K), 0.0120390 secs] 2869780K-&gt;2865267K(3061272K), 0.0121150 secs] [Times: user=0.09 sys=0.00, real=0.01 secs] 
64901.529: [GC 64901.529: [ParNew: 5507K-&gt;569K(5568K), 0.0086240 secs] 2870200K-&gt;2865742K(3061272K), 0.0087180 secs] [Times: user=0.05 sys=0.00, real=0.01 secs] 
64901.554: [GC 64901.555: [ParNew: 5516K-&gt;575K(5568K), 0.0107130 secs] 2870689K-&gt;2866291K(3061272K), 0.0107820 secs] [Times: user=0.06 sys=0.00, real=0.01 secs] 
64901.578: [CMS-concurrent-preclean: 0.070/0.133 secs] [Times: user=0.48 sys=0.01, real=0.14 secs] 
64901.578: [CMS-concurrent-abortable-preclean-start]
64901.584: [GC 64901.584: [ParNew: 5504K-&gt;571K(5568K), 0.0087270 secs] 2871220K-&gt;2866830K(3061272K), 0.0088220 secs] [Times: user=0.05 sys=0.00, real=0.01 secs] 
64901.609: [GC 64901.609: [ParNew: 5512K-&gt;569K(5568K), 0.0063370 secs] 2871771K-&gt;2867322K(3061272K), 0.0064230 secs] [Times: user=0.06 sys=0.00, real=0.01 secs] 
64901.615: [CMS-concurrent-abortable-preclean: 0.007/0.037 secs] [Times: user=0.13 sys=0.00, real=0.03 secs] 
64901.616: [GC[YG occupancy: 645 K (5568 K)]64901.616: [Rescan (parallel) , 0.0020210 secs]64901.618: [weak refs processing, 0.0027950 secs] [1 CMS-remark: 2866753K(3055704K)] 2867399K(3061272K), 0.0049380 secs] [Times: user=0.00 sys=0.01, real=0.01 secs] 
64901.621: [CMS-concurrent-sweep-start]
            </pre><p>
            </p><p>
            The first line indicates that the CMS concurrent mark (finding garbage) has taken 2.4 seconds. But this is a _concurrent_ 2.4 seconds, Java has not been paused at any point in time.
            </p><p>
            There are a few more minor GCs, then there is a pause at the 2nd last line:
            </p><pre class="programlisting">  
64901.616: [GC[YG occupancy: 645 K (5568 K)]64901.616: [Rescan (parallel) , 0.0020210 secs]64901.618: [weak refs processing, 0.0027950 secs] [1 CMS-remark: 2866753K(3055704K)] 2867399K(3061272K), 0.0049380 secs] [Times: user=0.00 sys=0.01, real=0.01 secs] 
            </pre><p>
            </p><p>
            The pause here is 0.0049380 seconds (aka 4.9 milliseconds) to 'remark' the heap.  
            </p><p>
            At this point the sweep starts, and you can watch the heap size go down:
            </p><pre class="programlisting">64901.637: [GC 64901.637: [ParNew: 5501K-&gt;569K(5568K), 0.0097350 secs] 2871958K-&gt;2867441K(3061272K), 0.0098370 secs] [Times: user=0.05 sys=0.00, real=0.01 secs] 
...  lines removed ...
64904.936: [GC 64904.936: [ParNew: 5532K-&gt;568K(5568K), 0.0070720 secs] 1365024K-&gt;1360689K(3061272K), 0.0071930 secs] [Times: user=0.05 sys=0.00, real=0.01 secs] 
64904.953: [CMS-concurrent-sweep: 2.030/3.332 secs] [Times: user=9.57 sys=0.26, real=3.33 secs] 
            </pre><p>
            At this point, the CMS sweep took 3.332 seconds, and heap went from about ~ 2.8 GB to 1.3 GB (approximate).
            </p><p>
            The key points here is to keep all these pauses low. CMS pauses are always low, but if your ParNew starts growing, you can see minor GC pauses approach 100ms, exceed 100ms and hit as high at 400ms.
            </p><p>
            This can be due to the size of the ParNew, which should be relatively small. If your ParNew is very large after running HBase for a while, in one example a ParNew was about 150MB, then you might have to constrain the size of ParNew (The larger it is, the longer the collections take but if its too small, objects are promoted to old gen too quickly). In the below we constrain new gen size to 64m.
            </p><p>
             Add this to HBASE_OPTS:
            </p><pre class="programlisting"> 
export HBASE_OPTS="-XX:NewSize=64m -XX:MaxNewSize=64m &lt;cms options from above&gt; &lt;gc logging options from above&gt;"
            </pre><p>
            </p><p>
            For more information on GC pauses, see the <a class="link" href="http://www.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/" target="_top">3 part blog post</a>  by Todd Lipcon
            and <a class="xref" href="jvm.html#gcpause" title="11.3.1.1.&nbsp;Long GC pauses">Section&nbsp;11.3.1.1, “Long GC pauses”</a> above.
            </p></div>
                     <div class="section" title="15.3. 工具">
                       <div class="titlepage">
                         <div>
                           <div>
                             <div>
                               <div>
                                 <div>
                                   <h2><a name="trouble.resources" id="trouble.resources"></a>12.3. Resources</h2>
                                 </div>
                               </div>
                             </div>
                             <div title="12.3.1. search-hadoop.com">
                               <div>
                                 <div>
                                   <div>
                                     <h3><a name="trouble.resources.searchhadoop"></a>12.3.1. search-hadoop.com</h3>
                                   </div>
                                 </div>
                               </div>
                               <p><a href="http://search-hadoop.com/" target="_top">search-hadoop.com</a> indexes all the mailing lists and is great for historical searches. Search here first when you have an issue as its more than likely someone has already had your problem.</p>
                             </div>
                             <div title="12.3.2. Mailing Lists">
                               <div>
                                 <div>
                                   <div>
                                     <h3><a name="trouble.resources.lists"></a>12.3.2. Mailing Lists</h3>
                                   </div>
                                 </div>
                               </div>
                               <p>Ask a question on the <a href="http://hbase.apache.org/mail-lists.html" target="_top">HBase mailing lists</a>. The 'dev' mailing list is aimed at the community of developers actually building HBase and for features currently under development, and 'user' is generally used for questions on released versions of HBase. Before going to the mailing list, make sure your question has not already been answered by searching the mailing list archives first. Use <a href="book.htm#trouble.resources.html#trouble.resources.searchhadoop" title="12.3.1. search-hadoop.com">Section 12.3.1, “search-hadoop.com”</a>. Take some time crafting your question[<a name="d1934e7699" href="book.htm#trouble.resources.html#ftn.d1934e7699">28</a>]; a quality question that includes all context and exhibits evidence the author has tried to find answers in the manual and out on lists is more likely to get a prompt response.</p>
                             </div>
                             <div title="12.3.3. IRC">
                               <div>
                                 <div>
                                   <div>
                                     <h3><a name="trouble.resources.irc"></a>12.3.3. IRC</h3>
                                   </div>
                                 </div>
                               </div>
                               <p>#hbase on irc.freenode.net</p>
                             </div>
                             <div title="12.3.4. JIRA">
                               <div>
                                 <div>
                                   <div>
                                     <h3><a name="trouble.resources.jira"></a>12.3.4. JIRA</h3>
                                   </div>
                                 </div>
                               </div>
                               <p><a href="https://issues.apache.org/jira/browse/HBASE" target="_top">JIRA</a> is also really helpful when looking for Hadoop/HBase-specific issues.</p>
                             </div>
                             <div><br>
                               <hr width="100" align="left">
                               <div>
                                 <p>[<a id="ftn.d1934e7699" href="book.htm#d1934e7699">28</a>] See Getting Answers</p>
                               </div>
                             </div>
<h2 class="title" style="clear: both"><a name="trouble.tools"></a>12.4.&nbsp;工具</h2>
</div>
                         </div>
                       </div>
                       <div class="section" title="15.3.1. search-hadoop.com">
                         <div class="titlepage">
                           <div>
                             <div>
                               <div>
                                 <div>
                                   <div>
                                     <h3>12.4.1. Builtin Tools</h3>
                                   </div>
                                 </div>
                               </div>
                               <div title="12.4.1.1. Master Web Interface">
                                 <div>
                                   <div>
                                     <div>
                                       <h4><a name="trouble.tools.builtin.webmaster"></a>12.4.1.1. Master Web Interface</h4>
                                     </div>
                                   </div>
                                 </div>
                                 <p>The Master starts a web-interface on port 60010 by default.</p>
                                 <p>The Master web UI lists created tables and their definition (e.g., ColumnFamilies, blocksize, etc.). Additionally, the available RegionServers in the cluster are listed along with selected high-level metrics (requests, number of regions, usedHeap, maxHeap). The Master web UI allows navigation to each RegionServer's web UI.</p>
                               </div>
                               <div title="12.4.1.2. RegionServer Web Interface">
                                 <div>
                                   <div>
                                     <div>
                                       <h4><a name="trouble.tools.builtin.webregion"></a>12.4.1.2. RegionServer Web Interface</h4>
                                     </div>
                                   </div>
                                 </div>
                                 <p>RegionServers starts a web-interface on port 60030 by default.</p>
                                 <p>The RegionServer web UI lists online regions and their start/end keys, as well as point-in-time RegionServer metrics (requests, regions, storeFileIndexSize, compactionQueueSize, etc.).</p>
                                 <p>See <a href="book.htm#hbase_metrics.html" title="14.4. HBase Metrics">Section 14.4, “HBase Metrics”</a> for more information in metric definitions.</p>
                               </div>
                               
                               <div class="section" title="12.4.1.3.&nbsp;zkcli"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.tools.builtin.zkcli"></a>12.4.1.3.&nbsp;zkcli</h4></div></div></div><p><code class="code">zkcli</code> is a very useful tool for investigating ZooKeeper-related issues.  To invoke:
</p><pre class="programlisting">./hbase zkcli -server host:port &lt;cmd&gt; &lt;args&gt;
</pre><p>
              The commands (and arguments) are:
</p><pre class="programlisting">	connect host:port
	get path [watch]
	ls path [watch]
	set path data [version]
	delquota [-n|-b] path
	quit 
	printwatches on|off
	create [-s] [-e] path data acl
	stat path [watch]
	close 
	ls2 path [watch]
	history 
	listquota path
	setAcl path acl
	getAcl path
	sync path
	redo cmdno
	addauth scheme auth
	delete path [version]
	setquota -n|-b val path
</pre><p>
            </p></div>
            
                               
                               <h3>12.4.2. External Tools</h3>
                             </div>
                           </div>
                         </div>
                       </div>
                       <div class="section" title="15.3.2. tail">
                         <div class="titlepage">
                           <div>
                             <div>
                               <h3 class="title"><a name="trouble.tools.tail"></a>12.4.2.1&nbsp;tail</h3>
                             </div>
                           </div>
                         </div>
                         <p> <code class="code">tail</code>是一个命令行工具，可以用来看日志的尾巴。加入的"-f"参数后，就会在数据更新的时候自己刷新。用它来看日志很方便。例如，一个机器需要花很多时间来启动或关闭，你可以tail他的master log(也可以是region server的log)。 </p>
                       </div>
                       <div class="section" title="15.3.3. top">
                         <div class="titlepage">
                           <div>
                             <div>
                               <h3 class="title"><a name="trouble.tools.top"></a>12.4.2.2&nbsp;top</h3>
                             </div>
                           </div>
                         </div>
                         <p> <code class="code">top</code>是一个很重要的工具来看你的机器各个进程的资源占用情况。下面是一个生产环境的例子： </p>
                         <pre class="programlisting">top - 14:46:59 up 39 days, 11:55,  1 user,  load average: 3.75, 3.57, 3.84
Tasks: 309 total,   1 running, 308 sleeping,   0 stopped,   0 zombie
Cpu(s):  4.5%us,  1.6%sy,  0.0%ni, 91.7%id,  1.4%wa,  0.1%hi,  0.6%si,  0.0%st
Mem:  24414432k total, 24296956k used,   117476k free,     7196k buffers
Swap: 16008732k total,	14348k used, 15994384k free, 11106908k cached
 
  PID USER  	PR  NI  VIRT  RES  SHR S %CPU %MEM	TIME+  COMMAND                                                                                                                                                                      
15558 hadoop	18  -2 3292m 2.4g 3556 S   79 10.4   6523:52 java                                                                                                                                                                          
13268 hadoop	18  -2 8967m 8.2g 4104 S   21 35.1   5170:30 java                                                                                                                                                                          
 8895 hadoop	18  -2 1581m 497m 3420 S   11  2.1   4002:32 java
…
                     </pre>
                         <p></p>
                         <p> 这里你可以看到系统的load average在最近5分钟是3.75，意思就是说这5分钟里面平均有3.75个线程在CPU时间的等待队列里面。通常来说，最完美的情况是这个值和CPU和核数相等，比这个值低意味着资源闲置，比这个值高就是过载了。这是一个重要的概念，要想理解的更多，可以看这篇文章 <a class="link" href="http://www.linuxjournal.com/article/9001" target="_top">http://www.linuxjournal.com/article/9001</a>. </p>
                         <p> 处理负载，我们可以看到系统已经几乎使用了他的全部RAM，其中大部分都是用于OS cache(这是一件好事).Swap只使用了一点点KB,这正是我们期望的，如果数值很高的话，就意味着在进行交换，这对Java程序的性能是致命的。另一种检测交换的方法是看Load average是否过高(load average过高还可能是磁盘损坏或者其它什么原因导致的)。 </p>
                         <p> 默认情况下进程列表不是很有用，我们可以看到3个Java进程使用了111%的CPU。要想知道哪个进程是什么，可以输入"c"，每一行就会扩展信息。输入“1”可以显示CPU的每个核的具体状况。 </p>
                       </div>
                       <div class="section" title="15.3.4. jps">
                         <div class="titlepage">
                           <div>
                             <div>
                               <h3 class="title"><a name="trouble.tools.jps"></a>12.4.2.3 &nbsp;jps</h3>
                             </div>
                           </div>
                         </div>
                         <p> <code class="code">jps</code>是JDK集成的一个工具，可以用来看当前用户的Java进程id。(如果是root,可以看到所有用户的id)，例如: </p>
                         <pre class="programlisting">hadoop@sv4borg12:~$ jps
1322 TaskTracker
17789 HRegionServer
27862 Child
1158 DataNode
25115 HQuorumPeer
2950 Jps
19750 ThriftServer
18776 jmx
                     </pre>
                         <p> 按顺序看 </p>
                         <div class="itemizedlist">
                           <ul class="itemizedlist" type="disc">
                             <li class="listitem">Hadoop TaskTracker,管理本地的Task</li>
                             <li class="listitem">HBase RegionServer,提供region的服务</li>
                             <li class="listitem">Child, 一个 MapReduce task,无法看出详细类型 </li>
                             <li class="listitem">Hadoop DataNode, 管理blocks</li>
                             <li class="listitem">HQuorumPeer, ZooKeeper集群的成员</li>
                             <li class="listitem">Jps, 就是这个进程</li>
                             <li class="listitem">ThriftServer, 当thrif启动后，就会有这个进程</li>
                             <li class="listitem">jmx, 这个是本地监控平台的进程。你可以不用这个。</li>
                           </ul>
                         </div>
                         <p></p>
                         <p> 你可以看到这个进程启动是全部命令行信息。 </p>
                         <pre class="programlisting">hadoop@sv4borg12:~$ ps aux | grep HRegionServer
hadoop   17789  155 35.2 9067824 8604364 ?     S&lt;l  Mar04 9855:48 /usr/java/jdk1.6.0_14/bin/java -Xmx8000m -XX:+DoEscapeAnalysis -XX:+AggressiveOpts -XX:+UseConcMarkSweepGC -XX:NewSize=64m -XX:MaxNewSize=64m -XX:CMSInitiatingOccupancyFraction=88 -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/export1/hadoop/logs/gc-hbase.log -Dcom.sun.management.jmxremote.port=10102 -Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.password.file=/home/hadoop/hbase/conf/jmxremote.password -Dcom.sun.management.jmxremote -Dhbase.log.dir=/export1/hadoop/logs -Dhbase.log.file=hbase-hadoop-regionserver-sv4borg12.log -Dhbase.home.dir=/home/hadoop/hbase -Dhbase.id.str=hadoop -Dhbase.root.logger=INFO,DRFA -Djava.library.path=/home/hadoop/hbase/lib/native/Linux-amd64-64 -classpath /home/hadoop/hbase/bin/../conf:[many jars]:/home/hadoop/hadoop/conf org.apache.hadoop.hbase.regionserver.HRegionServer start
                     </pre>
                         <p> </p>
                       </div>
                       <div class="section" title="15.3.5. jstack">
                         <div class="titlepage">
                           <div>
                             <div>
                               <h3 class="title"><a name="trouble.tools.jstack"></a>12.4.2.4 &nbsp;jstack</h3>
                             </div>
                           </div>
                         </div>
                         <p> <code class="code">jstack</code> 是一个最重要(除了看Log)的java工具，可以看到具体的Java进程的在做什么。可以先用Jps看到进程的Id,然后就可以用jstack。他会按线程的创建顺序显示线程的列表，还有这个线程在做什么。下面是例子： </p>
                         <p> 这个主线程是一个RegionServer正在等master返回什么信息。 </p>
                         <pre class="programlisting">      "regionserver60020" prio=10 tid=0x0000000040ab4000 nid=0x45cf waiting on condition [0x00007f16b6a96000..0x00007f16b6a96a70]
   java.lang.Thread.State: TIMED_WAITING (parking)
        	at sun.misc.Unsafe.park(Native Method)
        	- parking to wait for  &lt;0x00007f16cd5c2f30&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
        	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:1963)
        	at java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:395)
        	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:647)
        	at java.lang.Thread.run(Thread.java:619)
 
        	The MemStore flusher thread that is currently flushing to a file:
"regionserver60020.cacheFlusher" daemon prio=10 tid=0x0000000040f4e000 nid=0x45eb in Object.wait() [0x00007f16b5b86000..0x00007f16b5b87af0]
   java.lang.Thread.State: WAITING (on object monitor)
        	at java.lang.Object.wait(Native Method)
        	at java.lang.Object.wait(Object.java:485)
        	at org.apache.hadoop.ipc.Client.call(Client.java:803)
        	- locked &lt;0x00007f16cb14b3a8&gt; (a org.apache.hadoop.ipc.Client$Call)
        	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:221)
        	at $Proxy1.complete(Unknown Source)
        	at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
        	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        	at java.lang.reflect.Method.invoke(Method.java:597)
        	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
        	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
        	at $Proxy1.complete(Unknown Source)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3390)
        	- locked &lt;0x00007f16cb14b470&gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3304)
        	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
        	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
        	at org.apache.hadoop.hbase.io.hfile.HFile$Writer.close(HFile.java:650)
        	at org.apache.hadoop.hbase.regionserver.StoreFile$Writer.close(StoreFile.java:853)
        	at org.apache.hadoop.hbase.regionserver.Store.internalFlushCache(Store.java:467)
        	- locked &lt;0x00007f16d00e6f08&gt; (a java.lang.Object)
        	at org.apache.hadoop.hbase.regionserver.Store.flushCache(Store.java:427)
        	at org.apache.hadoop.hbase.regionserver.Store.access$100(Store.java:80)
        	at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.flushCache(Store.java:1359)
        	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:907)
        	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:834)
        	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:786)
        	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:250)
        	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:224)
        	at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.run(MemStoreFlusher.java:146)
                     </pre>
                         <p></p>
                         <p> 一个处理线程是在等一些东西(例如put, delete, scan...): </p>
                         <pre class="programlisting">"IPC Server handler 16 on 60020" daemon prio=10 tid=0x00007f16b011d800 nid=0x4a5e waiting on condition [0x00007f16afefd000..0x00007f16afefd9f0]
   java.lang.Thread.State: WAITING (parking)
        	at sun.misc.Unsafe.park(Native Method)
        	- parking to wait for  &lt;0x00007f16cd3f8dd8&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
        	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1925)
        	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:358)
        	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1013)
                     </pre>
                         <p></p>
                         <p> 有一个线程正在忙，在递增一个counter(这个阶段是正在创建一个scanner来读最新的值): </p>
                         <pre class="programlisting">"IPC Server handler 66 on 60020" daemon prio=10 tid=0x00007f16b006e800 nid=0x4a90 runnable [0x00007f16acb77000..0x00007f16acb77cf0]
   java.lang.Thread.State: RUNNABLE
        	at org.apache.hadoop.hbase.regionserver.KeyValueHeap.&lt;init&gt;(KeyValueHeap.java:56)
        	at org.apache.hadoop.hbase.regionserver.StoreScanner.&lt;init&gt;(StoreScanner.java:79)
        	at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:1202)
        	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.&lt;init&gt;(HRegion.java:2209)
        	at org.apache.hadoop.hbase.regionserver.HRegion.instantiateInternalScanner(HRegion.java:1063)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1055)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1039)
        	at org.apache.hadoop.hbase.regionserver.HRegion.getLastIncrement(HRegion.java:2875)
        	at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:2978)
        	at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2433)
        	at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)
        	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        	at java.lang.reflect.Method.invoke(Method.java:597)
        	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:560)
        	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1027)
                     </pre>
                         <p></p>
                         <p> 还有一个线程在从HDFS获取数据。 </p>
                         <pre class="programlisting">        	
"IPC Client (47) connection to sv4borg9/10.4.24.40:9000 from hadoop" daemon prio=10 tid=0x00007f16a02d0000 nid=0x4fa3 runnable [0x00007f16b517d000..0x00007f16b517dbf0]
   java.lang.Thread.State: RUNNABLE
        	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
        	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:215)
        	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
        	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
        	- locked &lt;0x00007f17d5b68c00&gt; (a sun.nio.ch.Util$1)
        	- locked &lt;0x00007f17d5b68be8&gt; (a java.util.Collections$UnmodifiableSet)
        	- locked &lt;0x00007f1877959b50&gt; (a sun.nio.ch.EPollSelectorImpl)
        	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
        	at org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool.select(SocketIOWithTimeout.java:332)
        	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:157)
        	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
        	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
        	at java.io.FilterInputStream.read(FilterInputStream.java:116)
        	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:304)
        	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
        	- locked &lt;0x00007f1808539178&gt; (a java.io.BufferedInputStream)
        	at java.io.DataInputStream.readInt(DataInputStream.java:370)
        	at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:569)
        	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:477)
                     </pre>
                         <p></p>
                         <p> 这里是一个RegionServer死了，master正在试着恢复。 </p>
                         <pre class="programlisting">"LeaseChecker" daemon prio=10 tid=0x00000000407ef800 nid=0x76cd waiting on condition [0x00007f6d0eae2000..0x00007f6d0eae2a70]
--
   java.lang.Thread.State: WAITING (on object monitor)
        	at java.lang.Object.wait(Native Method)
        	at java.lang.Object.wait(Object.java:485)
        	at org.apache.hadoop.ipc.Client.call(Client.java:726)
        	- locked &lt;0x00007f6d1cd28f80&gt; (a org.apache.hadoop.ipc.Client$Call)
        	at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)
        	at $Proxy1.recoverBlock(Unknown Source)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.processDatanodeError(DFSClient.java:2636)
        	at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.&lt;init&gt;(DFSClient.java:2832)
        	at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:529)
        	at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:186)
        	at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:530)
        	at org.apache.hadoop.hbase.util.FSUtils.recoverFileLease(FSUtils.java:619)
        	at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1322)
        	at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1210)
        	at org.apache.hadoop.hbase.master.HMaster.splitLogAfterStartup(HMaster.java:648)
        	at org.apache.hadoop.hbase.master.HMaster.joinCluster(HMaster.java:572)
        	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:503)
                     </pre>
                         <p></p>
                       </div>
                       <div class="section" title="15.3.6. OpenTSDB">
                         <div class="titlepage">
                           <div>
                             <div>
                               <h3 class="title"><a name="trouble.tools.opentsdb"></a>12.4.2.5 &nbsp;OpenTSDB</h3>
                             </div>
                           </div>
                         </div>
                         <p> <a class="link" href="http://opentsdb.net/" target="_top">OpenTSDB</a>是一个Ganglia的很好的替代品，因为他使用Hbase来存储所有的时序而不需要采样。使用OpenTSDB来监控你的Hbase是一个很好的实践 </p>
                         <p> 这里有一个例子，集群正在同时进行上百个compaction，严重影响了IO性能。(TODO:  在这里插入compactionQueueSize的图片)(译者注:囧) </p>
                         <p> 给集群构建一个图表监控是一个很好的实践。包括集群和每台机器。这样就可以快速定位到问题。例如，在StumbleUpon，每个机器有一个图表监控，包括OS和Hbase，涵盖所有的重要的信息。你也可以登录到机器上，获取更多的信息。 </p>
                       </div>
                       <div class="section" title="15.3.7. clusterssh+top">
                         <div class="titlepage">
                           <div>
                             <div>
                               <h3 class="title"><a name="trouble.tools.clustersshtop"></a>12.4.2.6 &nbsp;clusterssh+top</h3>
                             </div>
                           </div>
                         </div>
                         <p> clusterssh+top,感觉是一个穷人用的监控系统，但是他确实很有效，当你只有几台机器的是，很好设置。启动clusterssh后，你就会每台机器有个终端，还有一个终端，你在这个终端的操作都会反应到其他的每一个终端上。 这就意味着，你在一天机器执行“top”,集群中的所有机器都会给你全部的top信息。你还可以这样tail全部的log，等等。 </p>
                       </div>
                     </div>
                     
                     <div class="section" title="12.5.&nbsp;Client"><div class="titlepage"><div><div>
                       <h2 class="title" style="clear: both"><a name="trouble.client"></a>12.5.&nbsp;客户端</h2>
                             <p style="clear: both">&nbsp;</p>
                     </div></div></div>
                     <p> HBase 客户端的更多信息， 参考 <a class="xref" href="client.html" title="9.3.&nbsp;Client">Section&nbsp;9.3, “Client”</a>. 
       </p><div class="section" title="12.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.scantimeout"></a>12.5.1.&nbsp;ScannerTimeoutException or UnknownScannerException</h3></div></div></div><p>当从客户端到RegionServer的RPC请求超时。例如如果Scan.setCacheing的值设置为500，RPC请求就要去获取500行的数据，每500次<code class="code">.next()</code>操作获取一次。因为数据是以大块的形式传到客户端的，就可能造成超时。将这个 serCacheing的值调小是一个解决办法，但是这个值要是设的太小就会影响性能。 </p>
       <p>See <a class="xref" href="perf.reading.html#perf.hbase.client.caching" title="11.8.1.&nbsp;Scan Caching">Section&nbsp;11.8.1, “Scan Caching”</a>.
            </p></div><div class="section" title="12.5.2.&nbsp;Shell or client application throws lots of scary exceptions during normal operation"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.scarylogs"></a>12.5.2.&nbsp;Shell or client application throws lots of scary exceptions during normal operation</h3></div></div></div><p>Since 0.20.0 the default log level for <code class="code">org.apache.hadoop.hbase.*</code>is DEBUG. </p><p>
            On your clients, edit <code class="filename">$HBASE_HOME/conf/log4j.properties</code> and change this: <code class="code">log4j.logger.org.apache.hadoop.hbase=DEBUG</code> to this: <code class="code">log4j.logger.org.apache.hadoop.hbase=INFO</code>, or even <code class="code">log4j.logger.org.apache.hadoop.hbase=WARN</code>. 
            </p></div><div class="section" title="12.5.3.&nbsp;Long Client Pauses With Compression"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.longpauseswithcompression"></a>12.5.3.&nbsp;Long Client Pauses With Compression</h3></div></div></div><p>This is a fairly frequent question on the HBase dist-list.  The scenario is that a client is typically inserting a lot of data into a 
            relatively un-optimized HBase cluster.  Compression can exacerbate the pauses, although it is not the source of the problem.</p><p>See <a class="xref" href="perf.writing.html#precreate.regions" title="11.7.2.&nbsp; Table Creation: Pre-Creating Regions">Section&nbsp;11.7.2, “
    Table Creation: Pre-Creating Regions
    ”</a> on the pattern for pre-creating regions and confirm that the table isn't starting with a single region.</p><p>See <a class="xref" href="perf.configurations.html" title="11.4.&nbsp;HBase Configurations">Section&nbsp;11.4, “HBase Configurations”</a> for cluster configuration, particularly <code class="code">hbase.hstore.blockingStoreFiles</code>, <code class="code">hbase.hregion.memstore.block.multiplier</code>, 
            <code class="code">MAX_FILESIZE</code> (region size), and <code class="code">MEMSTORE_FLUSHSIZE.</code>  </p><p>A slightly longer explanation of why pauses can happen is as follows:  Puts are sometimes blocked on the MemStores which are blocked by the flusher thread which is blocked because there are 
            too many files to compact because the compactor is given too many small files to compact and has to compact the same data repeatedly.  This situation can occur even with minor compactions.
            Compounding this situation, HBase doesn't compress data in memory.  Thus, the 64MB that lives in the MemStore could become a 6MB file after compression - which results in a smaller StoreFile.  The upside is that
            more data is packed into the same region, but performance is achieved by being able to write larger files - which is why HBase waits until the flushize before writing a new StoreFile.  And smaller StoreFiles
            become targets for compaction.  Without compression the files are much bigger and don't need as much compaction, however this is at the expense of I/O.   
            </p><p>
            For additional information, see this thread on <a class="link" href="http://search-hadoop.com/m/WUnLM6ojHm1/Long+client+pauses+with+compression&amp;subj=Long+client+pauses+with+compression" target="_top">Long client pauses with compression</a>.
            </p></div><div class="section" title="12.5.4.&nbsp;ZooKeeper Client Connection Errors"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.zookeeper"></a>12.5.4.&nbsp;ZooKeeper Client Connection Errors</h3></div></div></div><p>Errors like this...
</p><pre class="programlisting">11/07/05 11:26:41 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
 unexpected error, closing socket connection and attempting reconnect
 java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
 11/07/05 11:26:43 INFO zookeeper.ClientCnxn: Opening socket connection to
 server localhost/127.0.0.1:2181
 11/07/05 11:26:44 WARN zookeeper.ClientCnxn: Session 0x0 for server null,
 unexpected error, closing socket connection and attempting reconnect
 java.net.ConnectException: Connection refused: no further information
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
        at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1078)
 11/07/05 11:26:45 INFO zookeeper.ClientCnxn: Opening socket connection to
 server localhost/127.0.0.1:2181
</pre><p>
            ... are either due to ZooKeeper being down, or unreachable due to network issues.            
            </p><p>The utility <a class="xref" href="trouble.tools.html#trouble.tools.builtin.zkcli" title="12.4.1.3.&nbsp;zkcli">Section&nbsp;12.4.1.3, “zkcli”</a> may help investigate ZooKeeper issues.
            </p></div><div class="section" title="12.5.5.&nbsp;Client running out of memory though heap size seems to be stable (but the off-heap/direct heap keeps growing)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.oome.directmemory.leak"></a>12.5.5.&nbsp;Client running out of memory though heap size seems to be stable (but the off-heap/direct heap keeps growing)</h3></div></div></div><p>
You are likely running into the issue that is described and worked through in
the mail thread HBase, mail # user - Suspected memory leak
and continued over in HBase, mail # dev - FeedbackRe: Suspected memory leak.
A workaround is passing your client-side JVM a reasonable value for <code class="code">-XX:MaxDirectMemorySize</code>.  By default,
the <code class="varname">MaxDirectMemorySize</code> is equal to your <code class="code">-Xmx</code> max heapsize setting (if <code class="code">-Xmx</code> is set).
Try seting it to something smaller (for example, one user had success setting it to <code class="code">1g</code> when
they had a client-side heap of <code class="code">12g</code>).  If you set it too small, it will bring on <code class="code">FullGCs</code> so keep
it  a bit hefty.  You want to make this setting client-side only especially if you are running the new experiemental
server-side off-heap cache since this feature depends on being able to use big direct buffers (You may have to keep
separate client-side and server-side config dirs).
            </p></div><div class="section" title="12.5.6.&nbsp;Client Slowdown When Calling Admin Methods (flush, compact, etc.)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.slowdown.admin"></a>12.5.6.&nbsp;Client Slowdown When Calling Admin Methods (flush, compact, etc.)</h3></div></div></div><p>
This is a client issue fixed by <a class="link" href="https://issues.apache.org/jira/browse/HBASE-5073" target="_top">HBASE-5073</a> in 0.90.6.
There was a ZooKeeper leak in the client and the client was getting pummeled by ZooKeeper events with each additional 
invocation of the admin API. 
            </p></div><div class="section" title="12.5.7.&nbsp;Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)])"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.client.security.rpc"></a>12.5.7.&nbsp;Secure Client Cannot Connect ([Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)])</h3></div></div></div><p>
There can be several causes that produce this symptom.
           </p><p>
First, check that you have a valid Kerberos ticket. One is required in order to set up communication with a secure HBase cluster. Examine the ticket currently in the credential cache, if any, by running the <span style="color: red">&lt;tt&gt;klist&lt;/tt&gt;</span> command line utility. If no ticket is listed, you must obtain a ticket by running the <span style="color: red">&lt;tt&gt;kinit&lt;/tt&gt;</span> command with either a keytab specified, or by interactively entering a password for the desired principal.
           </p><p>
Then, consult the <span style="color: red">&lt;a&gt;Java Security Guide troubleshooting section&lt;/a&gt;</span>. The most common problem addressed there is resolved by setting <span style="color: red">&lt;tt&gt;javax.security.auth.useSubjectCredsOnly&lt;/tt&gt;</span> system property value to <span style="color: red">&lt;tt&gt;false&lt;/tt&gt;</span>.
           </p><p>
Because of a change in the format in which MIT Kerberos writes its credentials cache, there is a bug in the Oracle JDK 6 Update 26 and earlier that causes Java to be unable to read the Kerberos credentials cache created by versions of MIT Kerberos 1.8.1 or higher. If you have this problematic combination of components in your environment, to work around this problem, first log in with <span style="color: red">&lt;tt&gt;kinit&lt;/tt&gt;</span> and then immediately refresh the credential cache with <span style="color: red">&lt;tt&gt;kinit -R&lt;/tt&gt;</span>. The refresh will rewrite the credential cache without the problematic formatting.
           </p><p>
Finally, depending on your Kerberos configuration, you may need to install the <span style="color: red">&lt;a&gt;Java Cryptography Extension&lt;/a&gt;</span>, or JCE. Insure the JCE jars are on the classpath on both server and client systems.
           </p><p>
You may also need to download the <span style="color: red">&lt;a&gt;unlimited strength JCE policy files&lt;/a&gt;</span>. Uncompress and extract the downloaded file, and install the policy jars into <span style="color: red">&lt;tt&gt;&lt;java-home&gt;/lib/security&lt;/tt&gt;</span>.
           </p></div></div>
                     <div class="section" title="12.6.&nbsp;MapReduce"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.mapreduce"></a>12.6.&nbsp;MapReduce</h2></div></div></div><div class="section" title="12.6.1.&nbsp;You Think You're On The Cluster, But You're Actually Local"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.mapreduce.local"></a>12.6.1.&nbsp;You Think You're On The Cluster, But You're Actually Local</h3></div></div></div><p>This following stacktrace happened using <code class="code">ImportTsv</code>, but things like this
        can happen on any job with a mis-configuration.        
</p><pre class="programlisting">    WARN mapred.LocalJobRunner: job_local_0001
java.lang.IllegalArgumentException: Can't read partitions file
       at org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.setConf(TotalOrderPartitioner.java:111)
       at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
       at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
       at org.apache.hadoop.mapred.MapTask$NewOutputCollector.&lt;init&gt;(MapTask.java:560)
       at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:639)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)
       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
Caused by: java.io.FileNotFoundException: File _partition.lst does not exist.
       at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:383)
       at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:251)
       at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:776)
       at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1424)
       at org.apache.hadoop.io.SequenceFile$Reader.&lt;init&gt;(SequenceFile.java:1419)
       at org.apache.hadoop.hbase.mapreduce.hadoopbackport.TotalOrderPartitioner.readPartitions(TotalOrderPartitioner.java:296)
</pre><p>
      .. see the critical portion of the stack?  It's...
</p><pre class="programlisting">       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)
</pre><p>
       LocalJobRunner means the job is running locally, not on the cluster.
      </p><p>See 
      <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath" target="_top">
      http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpath</a> for more 
      information on HBase MapReduce jobs and classpaths.
      </p></div></div>
      <div class="section" title="12.7.&nbsp;NameNode"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.namenode"></a>12.7.&nbsp;NameNode</h2></div></div></div><p>For more information on the NameNode, see <a class="xref" href="arch.hdfs.html" title="9.9.&nbsp;HDFS">Section&nbsp;9.9, “HDFS”</a>. 
       </p><div class="section" title="12.7.1.&nbsp;HDFS Utilization of Tables and Regions"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.namenode.disk"></a>12.7.1.&nbsp;HDFS Utilization of Tables and Regions</h3></div></div></div><p>To determine how much space HBase is using on HDFS use the <code class="code">hadoop</code> shell commands from the NameNode.  For example... </p><pre class="programlisting">hadoop fs -dus /hbase/</pre><p> ...returns the summarized disk utilization for all HBase objects.  </p><pre class="programlisting">hadoop fs -dus /hbase/myTable</pre><p> ...returns the summarized disk utilization for the HBase table 'myTable'. </p><pre class="programlisting">hadoop fs -du /hbase/myTable</pre><p> ...returns a list of the regions under the HBase table 'myTable' and their disk utilization. </p><p>For more information on HDFS shell commands, see the <a class="link" href="http://hadoop.apache.org/common/docs/current/file_system_shell.html" target="_top">HDFS FileSystem Shell documentation</a>.
            </p></div><div class="section" title="12.7.2.&nbsp;Browsing HDFS for HBase Objects"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.namenode.hbase.objects"></a>12.7.2.&nbsp;Browsing HDFS for HBase Objects</h3></div></div></div><p>Somtimes it will be necessary to explore the HBase objects that exist on HDFS.  These objects could include the WALs (Write Ahead Logs), tables, regions, StoreFiles, etc.
            The easiest way to do this is with the NameNode web application that runs on port 50070.  The NameNode web application will provide links to the all the DataNodes in the cluster so that
            they can be browsed seamlessly. </p><p>The HDFS directory structure of HBase tables in the cluster is...
            </p><pre class="programlisting"><code class="filename">/hbase</code>
     <code class="filename">/&lt;Table&gt;</code>             (Tables in the cluster)
          <code class="filename">/&lt;Region&gt;</code>           (Regions for the table)
               <code class="filename">/&lt;ColumnFamiy&gt;</code>      (ColumnFamilies for the Region for the table)
                    <code class="filename">/&lt;StoreFile&gt;</code>        (StoreFiles for the ColumnFamily for the Regions for the table)
            </pre><p>
            </p><p>The HDFS directory structure of HBase WAL is..
            </p><pre class="programlisting"><code class="filename">/hbase</code>
     <code class="filename">/.logs</code>     
          <code class="filename">/&lt;RegionServer&gt;</code>    (RegionServers)
               <code class="filename">/&lt;HLog&gt;</code>           (WAL HLog files for the RegionServer)
            </pre><p>
            </p><p>See the <a class="link" href="see http://hadoop.apache.org/common/docs/current/hdfs_user_guide.html" target="_top">HDFS User Guide</a> for other non-shell diagnostic 
		    utilities like <code class="code">fsck</code>. 
            </p><div class="section" title="12.7.2.1.&nbsp;Use Cases"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.namenode.uncompaction"></a>12.7.2.1.&nbsp;Use Cases</h4></div></div></div><p>Two common use-cases for querying HDFS for HBase objects is research the degree of uncompaction of a table.  If there are a large number of StoreFiles for each ColumnFamily it could 
              indicate the need for a major compaction.  Additionally, after a major compaction if the resulting StoreFile is "small" it could indicate the need for a reduction of ColumnFamilies for  the table.
		    </p></div></div></div>
                     <div class="section" title="15.4. 客户端">
                       <div class="titlepage">
                         <div>
                           <div>
                             <div>
                               <div>
                                 <div>
                                   <h2><a name="trouble.network" id="trouble.network"></a>12.8. Network</h2>
                                 </div>
                               </div>
                             </div>
                             <div title="12.8.1. Network Spikes">
                               <div>
                                 <div>
                                   <div>
                                     <h3><a name="trouble.network.spikes"></a>12.8.1. Network Spikes</h3>
                                   </div>
                                 </div>
                               </div>
                               <p>If you are seeing periodic network spikes you might want to check the compactionQueues to see if major compactions are happening.</p>
                               <p>See <a href="book.htm#managed.compactions" title="2.8.2.8. Managed Compactions">Section 2.8.2.8, “Managed Compactions”</a> for more information on managing compactions.</p>
                             </div>
                             <div title="12.8.2. Loopback IP">
                               <div>
                                 <div>
                                   <div>
                                     <h3><a name="trouble.network.loopback"></a>12.8.2. Loopback IP</h3>
                                   </div>
                                 </div>
                               </div>
                               <p>HBase expects the loopback IP Address to be 127.0.0.1. See the Getting Started section on <a href="book.htm#loopback.ip" title="2.2.3. Loopback IP">Section 2.2.3, “Loopback IP”</a>.</p>
                             </div>
                             <div title="12.8.3. Network Interfaces">
                               <div>
                                 <div>
                                   <div>
                                     <h3><a name="trouble.network.ints"></a>12.8.3. Network Interfaces</h3>
                                   </div>
                                 </div>
                               </div>
                               <p>Are all the network interfaces functioning correctly? Are you sure? See the Troubleshooting Case Study in <a href="book.htm#trouble.casestudy.html" title="12.14. Case Studies">Section 12.14, “Case Studies”</a>.</p>
                             </div>
                           </div>
                         </div>
                       </div>
</div>
                     <div class="section" title="15.5. RegionServer">
                       <div class="titlepage">
                         <div>
                           <div>
                             <h2 class="title" style="clear: both"><a name="trouble.rs"></a>12.9.&nbsp;RegionServer</h2>
                             <p style="clear: both"> RegionServer 的更多信息，参考 <a href="book.htm#regionserver.arch.html" title="9.6. RegionServer">Section 9.6, “RegionServer”</a>. </p>
                           </div>
                         </div>
                       </div>
                       <div class="section" title="15.5.1. 启动错误">
                         <div class="titlepage">
                           <div>
                             <div></div>
                           </div>
                         </div>
                         <div class="section" title="12.9.&nbsp;RegionServer"><div class="titlepage"></div><div class="section" title="12.9.1.&nbsp;Startup Errors"><div class="titlepage"><div>
                           <div><h3 class="title"><a name="trouble.rs.startup"></a>12.9.1.&nbsp;启动错误</h3>
                           </div></div></div><div class="section" title="12.9.1.1.&nbsp;Master Starts, But RegionServers Do Not"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.startup.master-no-region"></a>12.9.1.1.&nbsp;Master Starts, But RegionServers Do Not</h4></div></div></div><p>The Master believes the RegionServers have the IP of 127.0.0.1 - which is localhost and resolves to the master's own localhost.
            </p><p>The RegionServers are erroneously informing the Master that their IP addresses are 127.0.0.1. 
            </p><p>Modify <code class="filename">/etc/hosts</code> on the region servers, from...  
            </p><pre class="programlisting"># Do not remove the following line, or various programs
# that require network functionality will fail.
127.0.0.1               fully.qualified.regionservername regionservername  localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6
            </pre><p>
            ... to (removing the master node's name from localhost)...
            </p><pre class="programlisting"># Do not remove the following line, or various programs
# that require network functionality will fail.
127.0.0.1               localhost.localdomain localhost
::1             localhost6.localdomain6 localhost6
            </pre><p>
            </p></div><div class="section" title="12.9.1.2.&nbsp;Compression Link Errors"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.startup.compression"></a>12.9.1.2.&nbsp;Compression Link Errors</h4></div></div></div><p>因为LZO压缩算法需要在集群中的每台机器都要安装，这是一个启动失败的常见错误。如果你获得了如下信息 </p>
              <pre class="programlisting">11/02/20 01:32:15 ERROR lzo.GPLNativeCodeLoader: Could not load native gpl library
java.lang.UnsatisfiedLinkError: no gplcompression in java.library.path
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1734)
        at java.lang.Runtime.loadLibrary0(Runtime.java:823)
        at java.lang.System.loadLibrary(System.java:1028)
                     </pre>
              <p> 就意味着你的压缩库出现了问题。参见配置章节的 <a class="link" href="book.htm#lzo" title="3.6.4. LZO 压缩">LZO compression configuration</a>. </p></div></div><div class="section" title="12.9.2.&nbsp;Runtime Errors"><div class="titlepage"><div>
                <div><h3 class="title"><a name="trouble.rs.runtime"></a>12.9.2.&nbsp;运行时错误</h3>
                </div></div></div><div class="section" title="12.9.2.1.&nbsp;RegionServer Hanging"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.hang"></a>12.9.2.1.&nbsp;RegionServer Hanging</h4></div></div></div><p>
            Are you running an old JVM (&lt; 1.6.0_u21?)?  When you look at a thread dump,
            does it look like threads are BLOCKED but no one holds the lock all are
            blocked on?  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3622" target="_top">HBASE 3622 Deadlock in HBaseServer (JVM bug?)</a>.
            Adding <code class="code">-XX:+UseMembar</code> to the HBase <code class="varname">HBASE_OPTS</code> in <code class="filename">conf/hbase-env.sh</code>
            may fix it.
            </p><p>Also, are you using <a class="xref" href="client.html#client.rowlocks" title="9.3.4.&nbsp;RowLocks">Section&nbsp;9.3.4, “RowLocks”</a>?  These are discouraged because they can lock up the 
            RegionServers if not managed properly.
            </p></div><div class="section" title="12.9.2.2.&nbsp;java.io.IOException...(Too many open files)"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.filehandles"></a>12.9.2.2.&nbsp;java.io.IOException...(Too many open files)</h4></div></div></div><p>
           If you see log messages like this...
</p><pre class="programlisting">2010-09-13 01:24:17,336 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: 
Disk-related IOException in BlockReceiver constructor. Cause is java.io.IOException: Too many open files
        at java.io.UnixFileSystem.createFileExclusively(Native Method)
        at java.io.File.createNewFile(File.java:883)
</pre><p>
           ... 参见快速入门的章节 <a class="link" href="os.html#ulimit" title="2.2.5.&nbsp; ulimit and nproc">ulimit and nproc configuration</a>.
           </p></div><div class="section" title="12.9.2.3.&nbsp;xceiverCount 258 exceeds the limit of concurrent xcievers 256"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.xceivers"></a>12.9.2.3.&nbsp;xceiverCount 258 exceeds the limit of concurrent xcievers 256</h4></div></div></div><p>这个时常会出现在DataNode的日志中。 </p>
             <p> 参见快速入门章节的 <a class="link" href="book.htm#dfs.datanode.max.xcievers" title="1.3.1.7. dfs.datanode.max.xcievers">xceivers configuration</a>. </p>
             <p>&nbsp;</p>
           </div><div class="section" title="12.9.2.4.&nbsp;System instability, and the presence of &quot;java.lang.OutOfMemoryError: unable to create new native thread in exceptions&quot; HDFS DataNode logs or that of any system daemon"><div class="titlepage"><div><div>
             <h4 class="title"><a name="trouble.rs.runtime.oom-nt"></a>12.9.2.4. 系统不稳定,DataNode或者其他系统进程有 "java.lang.OutOfMemoryError: unable to create new native thread in exceptions"的错误</h4>
           </div></div></div><p>参见快速入门章节的 <a class="link" href="book.htm#ulimit" title="1.3.1.6.  ulimit 和 nproc">ulimit and nproc configuration</a>..  The default on recent Linux
           distributions is 1024 - which is far too low for HBase.
           </p></div><div class="section" title="12.9.2.5.&nbsp;DFS instability and/or RegionServer lease timeouts"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.gc"></a>12.9.2.5.&nbsp;DFS不稳定或者RegionServer租期超时</h4>
           </div></div></div>
           <p>如果你收到了如下的消息:</p>
           <pre class="programlisting">2009-02-24 10:01:33,516 WARN org.apache.hadoop.hbase.util.Sleeper: We slept xxx ms, ten times longer than scheduled: 10000
2009-02-24 10:01:33,516 WARN org.apache.hadoop.hbase.util.Sleeper: We slept xxx ms, ten times longer than scheduled: 15000
2009-02-24 10:01:36,472 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: unable to report to master for xxx milliseconds - retrying      
           </pre><p>
           ... 或者看到了全GC压缩操作，你可能正在执行一个全GC。</p></div><div class="section" title="12.9.2.6.&nbsp;&quot;No live nodes contain current block&quot; and/or YouAreDeadException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.nolivenodes"></a>12.9.2.6.&nbsp;"No live nodes contain current block" and/or YouAreDeadException</h4></div></div></div><p>这个错误有可能是OS的文件句柄溢出，也可能是网络故障导致节点无法访问。 </p>
             <p> 参见快速入门章节 <a class="link" href="book.htm#ulimit" title="1.3.1.6.  ulimit 和 nproc">ulimit and nproc configuration</a>，检查你的网络。</p>
           </div><div class="section" title="12.9.2.7.&nbsp;ZooKeeper SessionExpired events"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.zkexpired"></a>12.9.2.7.&nbsp;ZooKeeper SessionExpired events</h4></div></div></div><p>Master or RegionServers shutting down with messages like those in the logs: </p><pre class="programlisting">WARN org.apache.zookeeper.ClientCnxn: Exception
closing session 0x278bd16a96000f to sun.nio.ch.SelectionKeyImpl@355811ec
java.io.IOException: TIMED OUT
       at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:906)
WARN org.apache.hadoop.hbase.util.Sleeper: We slept 79410ms, ten times longer than scheduled: 5000
INFO org.apache.zookeeper.ClientCnxn: Attempting connection to server hostname/IP:PORT
INFO org.apache.zookeeper.ClientCnxn: Priming connection to java.nio.channels.SocketChannel[connected local=/IP:PORT remote=hostname/IP:PORT]
INFO org.apache.zookeeper.ClientCnxn: Server connection successful
WARN org.apache.zookeeper.ClientCnxn: Exception closing session 0x278bd16a96000d to sun.nio.ch.SelectionKeyImpl@3544d65e
java.io.IOException: Session Expired
       at org.apache.zookeeper.ClientCnxn$SendThread.readConnectResult(ClientCnxn.java:589)
       at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:709)
       at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:945)
ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: ZooKeeper session expired           
           </pre><p>
           The JVM is doing a long running garbage collecting which is pausing every threads (aka "stop the world").
           Since the RegionServer's local ZooKeeper client cannot send heartbeats, the session times out.
           By design, we shut down any node that isn't able to contact the ZooKeeper ensemble after getting a timeout so that it stops serving data that may already be assigned elsewhere.  
           </p><p>
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Make sure you give plenty of RAM (in <code class="filename">hbase-env.sh</code>), the default of 1GB won't be able to sustain long running imports.</li><li class="listitem">Make sure you don't swap, the JVM never behaves well under swapping.</li><li class="listitem">Make sure you are not CPU starving the RegionServer thread. For example, if you are running a MapReduce job using 6 CPU-intensive tasks on a machine with 4 cores, you are probably starving the RegionServer enough to create longer garbage collection pauses.</li><li class="listitem">Increase the ZooKeeper session timeout</li></ul></div><p>
           If you wish to increase the session timeout, add the following to your <code class="filename">hbase-site.xml</code> to increase the timeout from the default of 60 seconds to 120 seconds. 
           </p><pre class="programlisting">&lt;property&gt;
    &lt;name&gt;zookeeper.session.timeout&lt;/name&gt;
    &lt;value&gt;1200000&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;hbase.zookeeper.property.tickTime&lt;/name&gt;
    &lt;value&gt;6000&lt;/value&gt;
&lt;/property&gt;
            </pre><p>
           </p><p>
           Be aware that setting a higher timeout means that the regions served by a failed RegionServer will take at least
           that amount of time to be transfered to another RegionServer. For a production system serving live requests, we would instead 
           recommend setting it lower than 1 minute and over-provision your cluster in order the lower the memory load on each machines (hence having 
           less garbage to collect per machine).
           </p><p>
           If this is happening during an upload which only happens once (like initially loading all your data into HBase), consider bulk loading.
           </p>
           See <a class="xref" href="trouble.zookeeper.html#trouble.zookeeper.general" title="12.11.2.&nbsp;ZooKeeper, The Cluster Canary">Section&nbsp;12.11.2, “ZooKeeper, The Cluster Canary”</a> for other general information about ZooKeeper troubleshooting.
        </div><div class="section" title="12.9.2.8.&nbsp;NotServingRegionException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.notservingregion"></a>12.9.2.8.&nbsp;NotServingRegionException</h4></div></div></div><p>This exception is "normal" when found in the RegionServer logs at DEBUG level.  This exception is returned back to the client
           and then the client goes back to .META. to find the new location of the moved region.</p><p>However, if the NotServingRegionException is logged ERROR, then the client ran out of retries and something probably wrong.</p></div><div class="section" title="12.9.2.9.&nbsp;Regions listed by domain name, then IP"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.double_listed_regions"></a>12.9.2.9.&nbsp;Regions listed by domain name, then IP</h4></div></div></div><p>
           Fix your DNS.  In versions of HBase before 0.92.x, reverse DNS needs to give same answer
           as forward lookup. See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3431" target="_top">HBASE 3431
           RegionServer is not using the name given it by the master; double entry in master listing of servers</a> for gorey details.
          </p></div><div class="section" title="12.9.2.10.&nbsp;Logs flooded with '2011-01-10 12:40:48,407 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor' messages"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.codecmsgs"></a>12.9.2.10.&nbsp;Logs flooded with '2011-01-10 12:40:48,407 INFO org.apache.hadoop.io.compress.CodecPool: Got
            brand-new compressor' messages</h4></div></div></div><p>We are not using the native versions of compression
                    libraries.  See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1900" target="_top">HBASE-1900 Put back native support when hadoop 0.21 is released</a>.
                    Copy the native libs from hadoop under hbase lib dir or
                    symlink them into place and the message should go away.
                </p></div><div class="section" title="12.9.2.11.&nbsp;Server handler X on 60020 caught: java.nio.channels.ClosedChannelException"><div class="titlepage"><div><div><h4 class="title"><a name="trouble.rs.runtime.client_went_away"></a>12.9.2.11.&nbsp;Server handler X on 60020 caught: java.nio.channels.ClosedChannelException</h4></div></div></div><p>
           If you see this type of message it means that the region server was trying to read/send data from/to a client but
           it already went away. Typical causes for this are if the client was killed (you see a storm of messages like this when a MapReduce
           job is killed or fails) or if the client receives a SocketTimeoutException. It's harmless, but you should consider digging in
           a bit more if you aren't doing something to trigger them.
           </p></div></div><div class="section" title="12.9.3.&nbsp;Shutdown Errors"><div class="titlepage"><div><div></div></div></div></div></div>
                       </div>
                     </div>
                   </div>
                 </div>
               </div>
               <div class="section" title="15.5.2. 运行时错误"> </div>
                       <div class="section" title="15.5.3. 终止错误">
                         <div class="titlepage">
                           <div>
                             <div>
                               <h3 class="title"><a name="trouble.rs.shutdown"></a>12.9.3.&nbsp;终止错误</h3>
                             </div>
                           </div>
                         </div>
                       </div>
                     </div>
                     <div class="section" title="15.6. Master">
                       <div class="titlepage">
                         <div>
                           <div>
                             <h2 class="title" style="clear: both"><a name="trouble.master"></a>12.10.&nbsp;Master</h2>
                           </div>
                         </div>
                       </div>
                       <div class="section" title="15.6.1. 启动错误">
                         <div class="titlepage">
                           <div>
                             <div>
                               <div>
                                 <div>
                                   <div></div>
                                 </div>
                               </div>
                               <p>For more information on the Master, see <a href="book.htm#master.html" title="9.5. Master">Section 9.5, “Master”</a>.</p>
                               <div title="12.10.1. Startup Errors">
                                 <div>
                                   <div>
                                     <div>
                                       <h3><a name="trouble.master.startup"></a>12.10.1. <span class="title">启动错误</span></h3>
                                     </div>
                                   </div>
                                 </div>
                                 <div title="12.10.1.1. Master says that you need to run the hbase migrations script">
                                   <div>
                                     <div>
                                       <div>
                                         <h4><a name="trouble.master.startup.migration"></a>12.10.1.1. Master says that you need to run the hbase migrations script</h4>
                                       </div>
                                     </div>
                                   </div>
                                   <p>Upon running that, the hbase migrations script says no files in root directory.</p>
                                   <p>HBase expects the root directory to either not exist, or to have already been initialized by hbase running a previous time. If you create a new directory for HBase using Hadoop DFS, this error will occur. Make sure the HBase root directory does not currently exist or has been initialized by a previous run of HBase. Sure fire solution is to just use Hadoop dfs to delete the HBase root and let HBase create and initialize the directory itself.</p>
                                 </div>
                               </div>
                               <div title="12.10.2. Shutdown Errors">
                                 <div>
                                   <div>
                                     <div>
                                       <h3><a name="trouble.master.shutdown"></a>12.10.2. <span class="title">终止错误</span></h3>
                                     </div>
                                   </div>
                                 </div>
                               </div>
                             </div>
                           </div>
                         </div>
                       </div>
                       <div class="section" title="15.6.2. 终止错误">
                         <div class="titlepage">
                           <div> </div>
                         </div>
                       </div>
                     </div>
                     <div>
                       <div>
                         <div>
                           <h2><a name="trouble.zookeeper" id="trouble.zookeeper"></a>12.11. ZooKeeper</h2>
                         </div>
                       </div>
                     </div>
                     <div title="12.11.1. Startup Errors">
                       <div>
                         <div>
                           <div>
                             <h3><a name="trouble.zookeeper.startup"></a>12.11.1. Startup Errors</h3>
                           </div>
                         </div>
                       </div>
                       <div title="12.11.1.1. Could not find my address: xyz in list of ZooKeeper quorum servers">
                         <div>
                           <div>
                             <div>
                               <h4><a name="trouble.zookeeper.startup.address"></a>12.11.1.1. Could not find my address: xyz in list of ZooKeeper quorum servers</h4>
                             </div>
                           </div>
                         </div>
                         <p>A ZooKeeper server wasn't able to start, throws that error. xyz is the name of your server.</p>
                         <p>This is a name lookup problem. HBase tries to start a ZooKeeper server on some machine but that machine isn't able to find itself in the hbase.zookeeper.quorumconfiguration.</p>
                         <p>Use the hostname presented in the error message instead of the value you used. If you have a DNS server, you can set hbase.zookeeper.dns.interface andhbase.zookeeper.dns.nameserver in hbase-site.xml to make sure it resolves to the correct FQDN.</p>
                       </div>
                     </div>
                     <div title="12.11.2. ZooKeeper, The Cluster Canary">
                       <div>
                         <div>
                           <div>
                             <h3><a name="trouble.zookeeper.general"></a>12.11.2. ZooKeeper, The Cluster Canary</h3>
                           </div>
                         </div>
                       </div>
                       <p>ZooKeeper is the cluster's "canary in the mineshaft". It'll be the first to notice issues if any so making sure its happy is the short-cut to a humming cluster.</p>
                       <p>See the <a href="http://wiki.apache.org/hadoop/ZooKeeper/Troubleshooting" target="_top">ZooKeeper Operating Environment Troubleshooting</a> page. It has suggestions and tools for checking disk and networking performance; i.e. the operating environment your ZooKeeper and HBase are running in.</p>
                       <p>Additionally, the utility <a href="book.htm#trouble.tools.builtin.zkcli" title="12.4.1.3. zkcli">Section 12.4.1.3, “zkcli”</a> may help investigate ZooKeeper issues.</p>
                     </div>
<p>&nbsp;</p><div class="section" title="12.12.&nbsp;Amazon EC2"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.ec2"></a>12.12.&nbsp;Amazon EC2</h2></div></div></div><div class="section" title="12.12.1.&nbsp;ZooKeeper does not seem to work on Amazon EC2"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.zookeeper"></a>12.12.1.&nbsp;ZooKeeper does not seem to work on Amazon EC2</h3></div></div></div><p>HBase does not start when deployed as Amazon EC2 instances.  Exceptions like the below appear in the Master and/or RegionServer logs: </p><pre class="programlisting">  2009-10-19 11:52:27,030 INFO org.apache.zookeeper.ClientCnxn: Attempting
  connection to server ec2-174-129-15-236.compute-1.amazonaws.com/10.244.9.171:2181
  2009-10-19 11:52:27,032 WARN org.apache.zookeeper.ClientCnxn: Exception
  closing session 0x0 to sun.nio.ch.SelectionKeyImpl@656dc861
  java.net.ConnectException: Connection refused
             </pre><p>
             Security group policy is blocking the ZooKeeper port on a public address. 
             Use the internal EC2 host names when configuring the ZooKeeper quorum peer list. 
             </p></div><div class="section" title="12.12.2.&nbsp;Instability on Amazon EC2"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.instability"></a>12.12.2.&nbsp;Instability on Amazon EC2</h3></div></div></div><p>Questions on HBase and Amazon EC2 come up frequently on the HBase dist-list. Search for old threads using <a class="link" href="http://search-hadoop.com/" target="_top">Search Hadoop</a>
             </p></div><div class="section" title="12.12.3.&nbsp;Remote Java Connection into EC2 Cluster Not Working"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.ec2.connection"></a>12.12.3.&nbsp;Remote Java Connection into EC2 Cluster Not Working</h3></div></div></div><p>
             See Andrew's answer here, up on the user list: <a class="link" href="http://search-hadoop.com/m/sPdqNFAwyg2" target="_top">Remote Java client connection into EC2 instance</a>.
             </p></div></div>
             <div class="section" title="12.13.&nbsp;HBase and Hadoop version issues"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.versions"></a>12.13.&nbsp;HBase and Hadoop version issues</h2></div></div></div><div class="section" title="12.13.1.&nbsp;NoClassDefFoundError when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)"><div class="titlepage"><div><div><h3 class="title"><a name="trouble.versions.205"></a>12.13.1.&nbsp;<code class="code">NoClassDefFoundError</code> when trying to run 0.90.x on hadoop-0.20.205.x (or hadoop-1.0.x)</h3></div></div></div><p>HBase 0.90.x does not ship with hadoop-0.20.205.x, etc.  To make it run, you need to replace the hadoop
             jars that HBase shipped with in its <code class="filename">lib</code> directory with those of the Hadoop you want to
             run HBase on.  If even after replacing Hadoop jars you get the below exception:
</p><pre class="programlisting">sv4r6s38: Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/commons/configuration/Configuration
sv4r6s38:       at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;init&gt;(DefaultMetricsSystem.java:37)
sv4r6s38:       at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.&lt;clinit&gt;(DefaultMetricsSystem.java:34)
sv4r6s38:       at org.apache.hadoop.security.UgiInstrumentation.create(UgiInstrumentation.java:51)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:209)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:177)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:229)
sv4r6s38:       at org.apache.hadoop.security.KerberosName.&lt;clinit&gt;(KerberosName.java:83)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:202)
sv4r6s38:       at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:177)
</pre><p>
you need to copy under <code class="filename">hbase/lib</code>, the <code class="filename">commons-configuration-X.jar</code> you find
in your Hadoop's <code class="filename">lib</code> directory.  That should fix the above complaint.
</p></div></div>
<div class="section" title="12.14.&nbsp;Case Studies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="trouble.casestudy"></a>12.14.&nbsp;Case Studies</h2></div></div></div><p>For Performance and Troubleshooting Case Studies, see <a class="xref" href="casestudies.html" title="Chapter&nbsp;13.&nbsp;Case Studies">Chapter&nbsp;13, <i>Case Studies</i></a>.
      </p></div>
      
      <div class="chapter" title="Chapter&nbsp;13.&nbsp;Case Studies"><div class="titlepage"><div><div><h2 class="title"><a name="casestudies"></a>Chapter&nbsp;13.&nbsp;Case Studies</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="casestudies.html#casestudies.overview">13.1. Overview</a></span></dt><dt><span class="section"><a href="casestudies.schema.html">13.2. Schema Design</a></span></dt><dd><dl><dt><span class="section"><a href="casestudies.schema.html#casestudies.schema.listdata">13.2.1. List Data</a></span></dt></dl></dd><dt><span class="section"><a href="casestudies.perftroub.html">13.3. Performance/Troubleshooting</a></span></dt><dd><dl><dt><span class="section"><a href="casestudies.perftroub.html#casestudies.slownode">13.3.1. Case Study #1 (Performance Issue On A Single Node)</a></span></dt><dt><span class="section"><a href="casestudies.perftroub.html#casestudies.perf.1">13.3.2. Case Study #2 (Performance Research 2012)</a></span></dt><dt><span class="section"><a href="casestudies.perftroub.html#casestudies.perf.2">13.3.3. Case Study #3 (Performance Research 2010))</a></span></dt><dt><span class="section"><a href="casestudies.perftroub.html#casestudies.xceivers">13.3.4. Case Study #4 (xcievers Config)</a></span></dt></dl></dd></dl></div><div class="section" title="13.1.&nbsp;Overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.overview"></a>13.1.&nbsp;Overview</h2></div></div></div><p>This chapter will describe a variety of performance and troubleshooting case studies that can 
      provide a useful blueprint on diagnosing cluster issues.</p><p>For more information on Performance and Troubleshooting, see <a class="xref" href="performance.html" title="Chapter&nbsp;11.&nbsp;Performance Tuning">Chapter&nbsp;11, <i>Performance Tuning</i></a> and <a class="xref" href="trouble.html" title="Chapter&nbsp;12.&nbsp;Troubleshooting and Debugging HBase">Chapter&nbsp;12, <i>Troubleshooting and Debugging HBase</i></a>.
      </p></div></div>
      <div class="section" title="13.2.&nbsp;Schema Design"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.schema"></a>13.2.&nbsp;Schema Design</h2></div></div></div><div class="section" title="13.2.1.&nbsp;List Data"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.schema.listdata"></a>13.2.1.&nbsp;List Data</h3></div></div></div><p>The following is an exchange from the user dist-list regarding a fairly common question:  
    		how to handle per-user list data in HBase. 
    		</p><p>*** QUESTION ***</p><p>
    		We're looking at how to store a large amount of (per-user) list data in
HBase, and we were trying to figure out what kind of access pattern made
the most sense.  One option is store the majority of the data in a key, so
we could have something like:
    		</p><pre class="programlisting">&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId1&gt;:"" (no value)
&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId2&gt;:"" (no value)
&lt;FixedWidthUserName&gt;&lt;FixedWidthValueId3&gt;:"" (no value)
			</pre>

The other option we had was to do this entirely using:
    		<pre class="programlisting">&lt;FixedWidthUserName&gt;&lt;FixedWidthPageNum0&gt;:&lt;FixedWidthLength&gt;&lt;FixedIdNextPageNum&gt;&lt;ValueId1&gt;&lt;ValueId2&gt;&lt;ValueId3&gt;...
&lt;FixedWidthUserName&gt;&lt;FixedWidthPageNum1&gt;:&lt;FixedWidthLength&gt;&lt;FixedIdNextPageNum&gt;&lt;ValueId1&gt;&lt;ValueId2&gt;&lt;ValueId3&gt;...
    		</pre><p>
where each row would contain multiple values.
So in one case reading the first thirty values would be:
			</p><pre class="programlisting">scan { STARTROW =&gt; 'FixedWidthUsername' LIMIT =&gt; 30}
    		</pre>
And in the second case it would be
    		<pre class="programlisting">get 'FixedWidthUserName\x00\x00\x00\x00'
    		</pre><p>
The general usage pattern would be to read only the first 30 values of
these lists, with infrequent access reading deeper into the lists.  Some
users would have &lt;= 30 total values in these lists, and some users would
have millions (i.e. power-law distribution)
			</p><p>
 The single-value format seems like it would take up more space on HBase,
but would offer some improved retrieval / pagination flexibility.  Would
there be any significant performance advantages to be able to paginate via
gets vs paginating with scans?
			</p><p>
  My initial understanding was that doing a scan should be faster if our
paging size is unknown (and caching is set appropriately), but that gets
should be faster if we'll always need the same page size.  I've ended up
hearing different people tell me opposite things about performance.  I
assume the page sizes would be relatively consistent, so for most use cases
we could guarantee that we only wanted one page of data in the
fixed-page-length case.  I would also assume that we would have infrequent
updates, but may have inserts into the middle of these lists (meaning we'd
need to update all subsequent rows).
			</p><p>
Thanks for help / suggestions / follow-up questions.
			</p><p>*** ANSWER ***</p><p>
If I understand you correctly, you're ultimately trying to store
triples in the form "user, valueid, value", right? E.g., something
like:
			</p><pre class="programlisting">"user123, firstname, Paul",
"user234, lastname, Smith"
			</pre><p>
(But the usernames are fixed width, and the valueids are fixed width).
			</p><p>
And, your access pattern is along the lines of: "for user X, list the
next 30 values, starting with valueid Y". Is that right? And these
values should be returned sorted by valueid?
			</p><p>
The tl;dr version is that you should probably go with one row per
user+value, and not build a complicated intra-row pagination scheme on
your own unless you're really sure it is needed.
			</p><p>
Your two options mirror a common question people have when designing
HBase schemas: should I go "tall" or "wide"? Your first schema is
"tall": each row represents one value for one user, and so there are
many rows in the table for each user; the row key is user + valueid,
and there would be (presumably) a single column qualifier that means
"the value". This is great if you want to scan over rows in sorted
order by row key (thus my question above, about whether these ids are
sorted correctly). You can start a scan at any user+valueid, read the
next 30, and be done. What you're giving up is the ability to have
transactional guarantees around all the rows for one user, but it
doesn't sound like you need that. Doing it this way is generally
recommended (see
here <a class="link" href="http://hbase.apache.org/book.html#schema.smackdown" target="_top">http://hbase.apache.org/book.html#schema.smackdown</a>).
			</p><p>
Your second option is "wide": you store a bunch of values in one row,
using different qualifiers (where the qualifier is the valueid). The
simple way to do that would be to just store ALL values for one user
in a single row. I'm guessing you jumped to the "paginated" version
because you're assuming that storing millions of columns in a single
row would be bad for performance, which may or may not be true; as
long as you're not trying to do too much in a single request, or do
things like scanning over and returning all of the cells in the row,
it shouldn't be fundamentally worse. The client has methods that allow
you to get specific slices of columns.
			</p><p>
Note that neither case fundamentally uses more disk space than the
other; you're just "shifting" part of the identifying information for
a value either to the left (into the row key, in option one) or to the
right (into the column qualifiers in option 2). Under the covers,
every key/value still stores the whole row key, and column family
name. (If this is a bit confusing, take an hour and watch Lars
George's excellent video about understanding HBase schema design:
<a class="link" href="http://www.youtube.com/watch?v=_HLoH_PgrLk)" target="_top">http://www.youtube.com/watch?v=_HLoH_PgrLk)</a>.
			</p><p>
A manually paginated version has lots more complexities, as you note,
like having to keep track of how many things are in each page,
re-shuffling if new values are inserted, etc. That seems significantly
more complex. It might have some slight speed advantages (or
disadvantages!) at extremely high throughput, and the only way to
really know that would be to try it out. If you don't have time to
build it both ways and compare, my advice would be to start with the
simplest option (one row per user+value). Start simple and iterate! :)
			</p></div></div>
            <div class="section" title="13.3.&nbsp;Performance/Troubleshooting"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="casestudies.perftroub"></a>13.3.&nbsp;Performance/Troubleshooting</h2></div></div></div><div class="section" title="13.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.slownode"></a>13.3.1.&nbsp;Case Study #1 (Performance Issue On A Single Node)</h3></div></div></div><div class="section" title="13.3.1.1.&nbsp;Scenario"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e8563"></a>13.3.1.1.&nbsp;Scenario</h4></div></div></div><p>Following a scheduled reboot, one data node began exhibiting unusual behavior.  Routine MapReduce 
         jobs run against HBase tables which regularly completed in five or six minutes began taking 30 or 40 minutes 
         to finish. These jobs were consistently found to be waiting on map and reduce tasks assigned to the troubled data node 
         (e.g., the slow map tasks all had the same Input Split).           
         The situation came to a head during a distributed copy, when the copy was severely prolonged by the lagging node.
		</p></div><div class="section" title="13.3.1.2.&nbsp;Hardware"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e8568"></a>13.3.1.2.&nbsp;Hardware</h4></div></div></div><p>Datanodes:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Two 12-core processors</li><li class="listitem">Six Enerprise SATA disks</li><li class="listitem">24GB of RAM</li><li class="listitem">Two bonded gigabit NICs</li></ul></div><p>
        </p><p>Network:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">10 Gigabit top-of-rack switches</li><li class="listitem">20 Gigabit bonded interconnects between racks.</li></ul></div><p>
        </p></div><div class="section" title="13.3.1.3.&nbsp;Hypotheses"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e8591"></a>13.3.1.3.&nbsp;Hypotheses</h4></div></div></div><div class="section" title="13.3.1.3.1.&nbsp;HBase &quot;Hot Spot&quot; Region"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e8594"></a>13.3.1.3.1.&nbsp;HBase "Hot Spot" Region</h5></div></div></div><p>We hypothesized that we were experiencing a familiar point of pain: a "hot spot" region in an HBase table, 
		  where uneven key-space distribution can funnel a huge number of requests to a single HBase region, bombarding the RegionServer 
		  process and cause slow response time. Examination of the HBase Master status page showed that the number of HBase requests to the 
		  troubled node was almost zero.  Further, examination of the HBase logs showed that there were no region splits, compactions, or other region transitions 
		  in progress.  This effectively ruled out a "hot spot" as the root cause of the observed slowness.
          </p></div><div class="section" title="13.3.1.3.2.&nbsp;HBase Region With Non-Local Data"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e8599"></a>13.3.1.3.2.&nbsp;HBase Region With Non-Local Data</h5></div></div></div><p>Our next hypothesis was that one of the MapReduce tasks was requesting data from HBase that was not local to the datanode, thus 
		  forcing HDFS to request data blocks from other servers over the network.  Examination of the datanode logs showed that there were very 
		  few blocks being requested over the network, indicating that the HBase region was correctly assigned, and that the majority of the necessary 
		  data was located on the node. This ruled out the possibility of non-local data causing a slowdown.
          </p></div><div class="section" title="13.3.1.3.3.&nbsp;Excessive I/O Wait Due To Swapping Or An Over-Worked Or Failing Hard Disk"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e8604"></a>13.3.1.3.3.&nbsp;Excessive I/O Wait Due To Swapping Or An Over-Worked Or Failing Hard Disk</h5></div></div></div><p>After concluding that the Hadoop and HBase were not likely to be the culprits, we moved on to troubleshooting the datanode's hardware. 
          Java, by design, will periodically scan its entire memory space to do garbage collection.  If system memory is heavily overcommitted, the Linux 
          kernel may enter a vicious cycle, using up all of its resources swapping Java heap back and forth from disk to RAM as Java tries to run garbage 
          collection.  Further, a failing hard disk will often retry reads and/or writes many times before giving up and returning an error. This can manifest 
          as high iowait, as running processes wait for reads and writes to complete.  Finally, a disk nearing the upper edge of its performance envelope will 
          begin to cause iowait as it informs the kernel that it cannot accept any more data, and the kernel queues incoming data into the dirty write pool in memory.  
          However, using <code class="code">vmstat(1)</code> and <code class="code">free(1)</code>, we could see that no swap was being used, and the amount of disk IO was only a few kilobytes per second.
          </p></div><div class="section" title="13.3.1.3.4.&nbsp;Slowness Due To High Processor Usage"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e8615"></a>13.3.1.3.4.&nbsp;Slowness Due To High Processor Usage</h5></div></div></div><p>Next, we checked to see whether the system was performing slowly simply due to very high computational load.  <code class="code">top(1)</code> showed that the system load 
          was higher than normal, but <code class="code">vmstat(1)</code> and <code class="code">mpstat(1)</code> showed that the amount of processor being used for actual computation was low.
          </p></div><div class="section" title="13.3.1.3.5.&nbsp;Network Saturation (The Winner)"><div class="titlepage"><div><div><h5 class="title"><a name="d1934e8629"></a>13.3.1.3.5.&nbsp;Network Saturation (The Winner)</h5></div></div></div><p>Since neither the disks nor the processors were being utilized heavily, we moved on to the performance of the network interfaces.  The datanode had two 
          gigabit ethernet adapters, bonded to form an active-standby interface.  <code class="code">ifconfig(8)</code> showed some unusual anomalies, namely interface errors, overruns, framing errors. 
          While not unheard of, these kinds of errors are exceedingly rare on modern hardware which is operating as it should:
</p><pre class="programlisting">		
$ /sbin/ifconfig bond0
bond0  Link encap:Ethernet  HWaddr 00:00:00:00:00:00  
inet addr:10.x.x.x  Bcast:10.x.x.255  Mask:255.255.255.0
UP BROADCAST RUNNING MASTER MULTICAST  MTU:1500  Metric:1
RX packets:2990700159 errors:12 dropped:0 overruns:1 frame:6          &lt;--- Look Here! Errors!
TX packets:3443518196 errors:0 dropped:0 overruns:0 carrier:0
collisions:0 txqueuelen:0 
RX bytes:2416328868676 (2.4 TB)  TX bytes:3464991094001 (3.4 TB)
</pre><p>
          </p><p>These errors immediately lead us to suspect that one or more of the ethernet interfaces might have negotiated the wrong line speed.  This was confirmed both by running an ICMP ping 
          from an external host and observing round-trip-time in excess of 700ms, and by running <code class="code">ethtool(8)</code> on the members of the bond interface and discovering that the active interface 
          was operating at 100Mbs/, full duplex.
</p><pre class="programlisting">		
$ sudo ethtool eth0
Settings for eth0:
Supported ports: [ TP ]
Supported link modes:   10baseT/Half 10baseT/Full 
                       100baseT/Half 100baseT/Full 
                       1000baseT/Full 
Supports auto-negotiation: Yes
Advertised link modes:  10baseT/Half 10baseT/Full 
                       100baseT/Half 100baseT/Full 
                       1000baseT/Full 
Advertised pause frame use: No
Advertised auto-negotiation: Yes
Link partner advertised link modes:  Not reported
Link partner advertised pause frame use: No
Link partner advertised auto-negotiation: No
Speed: 100Mb/s                                     &lt;--- Look Here!  Should say 1000Mb/s!
Duplex: Full
Port: Twisted Pair
PHYAD: 1
Transceiver: internal
Auto-negotiation: on
MDI-X: Unknown
Supports Wake-on: umbg
Wake-on: g
Current message level: 0x00000003 (3)
Link detected: yes
</pre><p>		
		  </p><p>In normal operation, the ICMP ping round trip time should be around 20ms, and the interface speed and duplex should read, "1000MB/s", and, "Full", respectively.  
		  </p></div></div><div class="section" title="13.3.1.4.&nbsp;Resolution"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e8650"></a>13.3.1.4.&nbsp;Resolution</h4></div></div></div><p>After determining that the active ethernet adapter was at the incorrect speed, we used the <code class="code">ifenslave(8)</code> command to make the standby interface 
   	  the active interface, which yielded an immediate improvement in MapReduce performance, and a 10 times improvement in network throughput:
	  </p><p>On the next trip to the datacenter, we determined that the line speed issue was ultimately caused by a bad network cable, which was replaced.
	  </p></div></div><div class="section" title="13.3.2.&nbsp;Case Study #2 (Performance Research 2012)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.perf.1"></a>13.3.2.&nbsp;Case Study #2 (Performance Research 2012)</h3></div></div></div><p>Investigation results of a self-described "we're not sure what's wrong, but it seems slow" problem. 
      <a class="link" href="http://gbif.blogspot.com/2012/03/hbase-performance-evaluation-continued.html" target="_top">http://gbif.blogspot.com/2012/03/hbase-performance-evaluation-continued.html</a>
      </p></div><div class="section" title="13.3.3.&nbsp;Case Study #3 (Performance Research 2010))"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.perf.2"></a>13.3.3.&nbsp;Case Study #3 (Performance Research 2010))</h3></div></div></div><p>
      Investigation results of general cluster performance from 2010.  Although this research is on an older version of the codebase, this writeup
      is still very useful in terms of approach.
      <a class="link" href="http://hstack.org/hbase-performance-testing/" target="_top">http://hstack.org/hbase-performance-testing/</a>
      </p></div><div class="section" title="13.3.4.&nbsp;Case Study #4 (xcievers Config)"><div class="titlepage"><div><div><h3 class="title"><a name="casestudies.xceivers"></a>13.3.4.&nbsp;Case Study #4 (xcievers Config)</h3></div></div></div><p>Case study of configuring <code class="code">xceivers</code>, and diagnosing errors from mis-configurations.
      <a class="link" href="http://www.larsgeorge.com/2012/03/hadoop-hbase-and-xceivers.html" target="_top">http://www.larsgeorge.com/2012/03/hadoop-hbase-and-xceivers.html</a>
      </p><p>See also <a class="xref" href="hadoop.html#dfs.datanode.max.xcievers" title="2.3.2.&nbsp;dfs.datanode.max.xcievers">Section&nbsp;2.3.2, “<code class="varname">dfs.datanode.max.xcievers</code>”</a>.
      </p></div></div>
      
      <div class="chapter" title="Chapter&nbsp;14.&nbsp;HBase Operational Management"><div class="titlepage"><div><div><h2 class="title"><a name="ops_mgt"></a>Chapter&nbsp;14.&nbsp;HBase Operational Management</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="ops_mgt.html#tools">14.1. HBase Tools and Utilities</a></span></dt><dd><dl><dt><span class="section"><a href="ops_mgt.html#driver">14.1.1. Driver</a></span></dt><dt><span class="section"><a href="ops_mgt.html#hbck">14.1.2. HBase <span class="application">hbck</span></a></span></dt><dt><span class="section"><a href="ops_mgt.html#hfile_tool2">14.1.3. HFile Tool</a></span></dt><dt><span class="section"><a href="ops_mgt.html#wal_tools">14.1.4. WAL Tools</a></span></dt><dt><span class="section"><a href="ops_mgt.html#compression.tool">14.1.5. Compression Tool</a></span></dt><dt><span class="section"><a href="ops_mgt.html#copytable">14.1.6. CopyTable</a></span></dt><dt><span class="section"><a href="ops_mgt.html#export">14.1.7. Export</a></span></dt><dt><span class="section"><a href="ops_mgt.html#import">14.1.8. Import</a></span></dt><dt><span class="section"><a href="ops_mgt.html#importtsv">14.1.9. ImportTsv</a></span></dt><dt><span class="section"><a href="ops_mgt.html#completebulkload">14.1.10. CompleteBulkLoad</a></span></dt><dt><span class="section"><a href="ops_mgt.html#walplayer">14.1.11. WALPlayer</a></span></dt><dt><span class="section"><a href="ops_mgt.html#rowcounter">14.1.12. RowCounter</a></span></dt></dl></dd><dt><span class="section"><a href="ops.regionmgt.html">14.2. Region Management</a></span></dt><dd><dl><dt><span class="section"><a href="ops.regionmgt.html#ops.regionmgt.majorcompact">14.2.1. Major Compaction</a></span></dt><dt><span class="section"><a href="ops.regionmgt.html#ops.regionmgt.merge">14.2.2. Merge</a></span></dt></dl></dd><dt><span class="section"><a href="node.management.html">14.3. Node Management</a></span></dt><dd><dl><dt><span class="section"><a href="node.management.html#decommission">14.3.1. Node Decommission</a></span></dt><dt><span class="section"><a href="node.management.html#rolling">14.3.2. Rolling Restart</a></span></dt></dl></dd><dt><span class="section"><a href="hbase_metrics.html">14.4. HBase Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="hbase_metrics.html#metric_setup">14.4.1. Metric Setup</a></span></dt><dt><span class="section"><a href="hbase_metrics.html#rs_metrics">14.4.2. RegionServer Metrics</a></span></dt></dl></dd><dt><span class="section"><a href="ops.monitoring.html">14.5. HBase Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="ops.monitoring.html#ops.monitoring.overview">14.5.1. Overview</a></span></dt><dt><span class="section"><a href="ops.monitoring.html#ops.slow.query">14.5.2. Slow Query Log</a></span></dt></dl></dd><dt><span class="section"><a href="cluster_replication.html">14.6. Cluster Replication</a></span></dt><dt><span class="section"><a href="ops.backup.html">14.7. HBase Backup</a></span></dt><dd><dl><dt><span class="section"><a href="ops.backup.html#ops.backup.fullshutdown">14.7.1. Full Shutdown Backup</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.replication">14.7.2. Live Cluster Backup - Replication</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.copytable">14.7.3. Live Cluster Backup - CopyTable</a></span></dt><dt><span class="section"><a href="ops.backup.html#ops.backup.live.export">14.7.4. Live Cluster Backup - Export</a></span></dt></dl></dd><dt><span class="section"><a href="ops.capacity.html">14.8. Capacity Planning</a></span></dt><dd><dl><dt><span class="section"><a href="ops.capacity.html#ops.capacity.storage">14.8.1. Storage</a></span></dt><dt><span class="section"><a href="ops.capacity.html#ops.capacity.regions">14.8.2. Regions</a></span></dt></dl></dd></dl></div>
  This chapter will cover operational tools and practices required of a running HBase cluster.
  The subject of operations is related to the topics of <a class="xref" href="trouble.html" title="Chapter&nbsp;12.&nbsp;Troubleshooting and Debugging HBase">Chapter&nbsp;12, <i>Troubleshooting and Debugging HBase</i></a>, <a class="xref" href="performance.html" title="Chapter&nbsp;11.&nbsp;Performance Tuning">Chapter&nbsp;11, <i>Performance Tuning</i></a>,
  and <a class="xref" href="configuration.html" title="Chapter&nbsp;2.&nbsp;Configuration">Chapter&nbsp;2, <i>Configuration</i></a> but is a distinct topic in itself.  
  
  <div class="section" title="14.1.&nbsp;HBase Tools and Utilities"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tools"></a>14.1.&nbsp;HBase Tools and Utilities</h2></div></div></div><p>Here we list HBase tools for administration, analysis, fixup, and
    debugging.</p><div class="section" title="14.1.1.&nbsp;Driver"><div class="titlepage"><div><div><h3 class="title"><a name="driver"></a>14.1.1.&nbsp;Driver</h3></div></div></div><p>There is a <code class="code">Driver</code> class that is executed by the HBase jar can be used to invoke frequently accessed utilities.  For example, 
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar 
</pre><p>
... will return...
</p><pre class="programlisting">An example program must be given as the first argument.
Valid program names are:
  completebulkload: Complete a bulk data load.
  copytable: Export a table from local cluster to peer cluster
  export: Write table data to HDFS.
  import: Import data written by Export.
  importtsv: Import data in TSV format.
  rowcounter: Count rows in HBase table
  verifyrep: Compare the data from tables in two different clusters. WARNING: It doesn't work for incrementColumnValues'd cells since the timestamp is chan
</pre><p>
... for allowable program names.
      </p></div><div class="section" title="14.1.2.&nbsp;HBase hbck"><div class="titlepage"><div><div><h3 class="title"><a name="hbck"></a>14.1.2.&nbsp;HBase <span class="application">hbck</span></h3></div><div><h4 class="subtitle">An <span class="emphasis"><em>fsck</em></span> for your HBase install</h4></div></div></div><p>To run <span class="application">hbck</span> against your HBase cluster run
        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
        At the end of the commands output it prints <span class="emphasis"><em>OK</em></span>
        or <span class="emphasis"><em>INCONSISTENCY</em></span>. If your cluster reports
        inconsistencies, pass <span class="command"><strong>-details</strong></span> to see more detail emitted.
        If inconsistencies, run <span class="command"><strong>hbck</strong></span> a few times because the
        inconsistency may be transient (e.g. cluster is starting up or a region is
        splitting).
        Passing <span class="command"><strong>-fix</strong></span> may correct the inconsistency (This latter
        is an experimental feature).
        </p><p>For more information, see <a class="xref" href="hbck.in.depth.html" title="Appendix&nbsp;B.&nbsp;hbck In Depth">Appendix&nbsp;B, <i>hbck In Depth</i></a>.
        </p></div><div class="section" title="14.1.3.&nbsp;HFile Tool"><div class="titlepage"><div><div><h3 class="title"><a name="hfile_tool2"></a>14.1.3.&nbsp;HFile Tool</h3></div></div></div><p>See <a class="xref" href="regions.arch.html#hfile_tool" title="9.7.5.2.2.&nbsp;HFile Tool">Section&nbsp;9.7.5.2.2, “HFile Tool”</a>.</p></div><div class="section" title="14.1.4.&nbsp;WAL Tools"><div class="titlepage"><div><div><h3 class="title"><a name="wal_tools"></a>14.1.4.&nbsp;WAL Tools</h3></div></div></div><div class="section" title="14.1.4.1.&nbsp;HLog tool"><div class="titlepage"><div><div><h4 class="title"><a name="hlog_tool"></a>14.1.4.1.&nbsp;<code class="classname">HLog</code> tool</h4></div></div></div><p>The main method on <code class="classname">HLog</code> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <code class="filename">recovered.edits</code>. directory.</p><p>You can get a textual dump of a WAL file content by doing the
        following:</p><pre class="programlisting"> <code class="code">$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </pre><p>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <code class="varname">STDOUT</code> to
        <code class="code">/dev/null</code> and testing the program return.</p><p>Similarly you can force a split of a log file directory by
        doing:</p><pre class="programlisting"> $ ./<code class="code">bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</code></pre><div class="section" title="14.1.4.1.1.&nbsp;HLogPrettyPrinter"><div class="titlepage"><div><div><h5 class="title"><a name="hlog_tool.prettyprint"></a>14.1.4.1.1.&nbsp;<code class="classname">HLogPrettyPrinter</code></h5></div></div></div><p><code class="classname">HLogPrettyPrinter</code> is a tool with configurable options to print the contents of an HLog.
          </p></div></div></div><div class="section" title="14.1.5.&nbsp;Compression Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compression.tool"></a>14.1.5.&nbsp;Compression Tool</h3></div></div></div><p>See <a class="xref" href="compression.html#compression.test" title="C.1.&nbsp;CompressionTest Tool">Section&nbsp;C.1, “CompressionTest Tool”</a>.</p></div><div class="section" title="14.1.6.&nbsp;CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="copytable"></a>14.1.6.&nbsp;CopyTable</h3></div></div></div><p>
            CopyTable is a utility that can copy part or of all of a table, either to the same cluster or another cluster. The usage is as follows:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] tablename
</pre><p>
        </p><p>
        Options:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">starttime</code>  Beginning of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">endtime</code>  End of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">versions</code>  Number of cell versions to copy.</li><li class="listitem"><code class="varname">new.name</code>  New table's name.</li><li class="listitem"><code class="varname">peer.adr</code>  Address of the peer cluster given in the format hbase.zookeeper.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent</li><li class="listitem"><code class="varname">families</code>  Comma-separated list of ColumnFamilies to copy.</li><li class="listitem"><code class="varname">all.cells</code>  Also copy delete markers and uncollected deleted cells (advanced option).</li></ul></div><p>
         Args:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">tablename  Name of table to copy.</li></ul></div><p>
        </p><p>Example of copying 'TestTable' to a cluster that uses replication for a 1 hour window:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable
--starttime=1265875194289 --endtime=1265878794289
--peer.adr=server1,server2,server3:2181:/hbase TestTable</pre><p>
        </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="section" title="14.1.7.&nbsp;Export"><div class="titlepage"><div><div><h3 class="title"><a name="export"></a>14.1.7.&nbsp;Export</h3></div></div></div><p>Export is a utility that will dump the contents of table to HDFS in a sequence file.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="section" title="14.1.8.&nbsp;Import"><div class="titlepage"><div><div><h3 class="title"><a name="import"></a>14.1.8.&nbsp;Import</h3></div></div></div><p>Import is a utility that will load data that has been exported back into HBase.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre><p>
       </p></div><div class="section" title="14.1.9.&nbsp;ImportTsv"><div class="titlepage"><div><div><h3 class="title"><a name="importtsv"></a>14.1.9.&nbsp;ImportTsv</h3></div></div></div><p>ImportTsv is a utility that will load data in TSV format into HBase.  It has two distinct usages:  loading data from TSV format in HDFS 
       into HBase via Puts, and preparing StoreFiles to be loaded via the <code class="code">completebulkload</code>.
       </p><p>To load data via Puts (i.e., non-bulk loading):
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;hdfs-inputdir&gt;
</pre><p>
       </p><p>To generate StoreFiles for bulk-loading:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c -Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;
</pre><p>
       </p><p>These generated StoreFiles can be loaded into HBase via <a class="xref" href="ops_mgt.html#completebulkload" title="14.1.10.&nbsp;CompleteBulkLoad">Section&nbsp;14.1.10, “CompleteBulkLoad”</a>. 
       </p><div class="section" title="14.1.9.1.&nbsp;ImportTsv Options"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.options"></a>14.1.9.1.&nbsp;ImportTsv Options</h4></div></div></div>
       Running ImportTsv with no arguments prints brief usage information:
<pre class="programlisting">Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;

Imports the given input directory of TSV data into the specified table.

The column names of the TSV data must be specified using the -Dimporttsv.columns
option. This option takes the form of comma-separated column names, where each
column name is either a simple column family, or a columnfamily:qualifier. The special
column name HBASE_ROW_KEY is used to designate that this column should be used
as the row key for each imported record. You must specify exactly one column
to be the row key, and you must specify a column name for every column that exists in the
input data.

By default importtsv will load data directly into HBase. To instead generate
HFiles of data to prepare for a bulk data load, pass the option:
  -Dimporttsv.bulk.output=/path/for/output
  Note: if you do not use this option, then the target table must already exist in HBase

Other options that may be specified with -D include:
  -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line
  '-Dimporttsv.separator=|' - eg separate on pipes instead of tabs
  -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import
  -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper
</pre></div><div class="section" title="14.1.9.2.&nbsp;ImportTsv Example"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.example"></a>14.1.9.2.&nbsp;ImportTsv Example</h4></div></div></div><p>For example, assume that we are loading data into a table called 'datatsv' with a ColumnFamily called 'd' with two columns "c1" and "c2".
         </p><p>Assume that an input file exists as follows:
</p><pre class="programlisting">row1	c1	c2
row2	c1	c2
row3	c1	c2
row4	c1	c2
row5	c1	c2
row6	c1	c2
row7	c1	c2
row8	c1	c2
row9	c1	c2
row10	c1	c2
</pre><p>
         </p><p>For ImportTsv to use this imput file, the command line needs to look like this:
 </p><pre class="programlisting"> HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar importtsv -Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2 -Dimporttsv.bulk.output=hdfs://storefileoutput  datatsv hdfs://inputfile
 </pre><p>
         ... and in this example the first column is the rowkey, which is why the HBASE_ROW_KEY is used.  The second and third columns in the file will be imported as "d:c1" and "d:c2", respectively.
         </p></div><div class="section" title="14.1.9.3.&nbsp;ImportTsv Warning"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.warning"></a>14.1.9.3.&nbsp;ImportTsv Warning</h4></div></div></div><p>If you have preparing a lot of data for bulk loading, make sure the target HBase table is pre-split appropriately.
         </p></div><div class="section" title="14.1.9.4.&nbsp;See Also"><div class="titlepage"><div><div><h4 class="title"><a name="importtsv.also"></a>14.1.9.4.&nbsp;See Also</h4></div></div></div>
       For more information about bulk-loading HFiles into HBase, see <a class="xref" href="arch.bulk.load.html" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, “Bulk Loading”</a></div></div><div class="section" title="14.1.10.&nbsp;CompleteBulkLoad"><div class="titlepage"><div><div><h3 class="title"><a name="completebulkload"></a>14.1.10.&nbsp;CompleteBulkLoad</h3></div></div></div><p>The <code class="code">completebulkload</code> utility will move generated StoreFiles into an HBase table.  This utility is often used
	   in conjunction with output from <a class="xref" href="ops_mgt.html#importtsv" title="14.1.9.&nbsp;ImportTsv">Section&nbsp;14.1.9, “ImportTsv”</a>.  
	   </p><p>There are two ways to invoke this utility, with explicit classname and via the driver: 
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFile &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
.. and via the Driver..
</p><pre class="programlisting">HADOOP_CLASSPATH=`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/hbase-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt; &lt;tablename&gt;
</pre><p>
	  </p><p>For more information about bulk-loading HFiles into HBase, see <a class="xref" href="arch.bulk.load.html" title="9.8.&nbsp;Bulk Loading">Section&nbsp;9.8, “Bulk Loading”</a>.
       </p></div><div class="section" title="14.1.11.&nbsp;WALPlayer"><div class="titlepage"><div><div><h3 class="title"><a name="walplayer"></a>14.1.11.&nbsp;WALPlayer</h3></div></div></div><p>WALPlayer is a utility to replay WAL files into HBase.
       </p><p>The WAL can be replayed for a set of tables or all tables, and a timerange can be provided (in milliseconds). The WAL is filtered to this set of tables. The output can optionally be mapped to another set of tables.
       </p><p>WALPlayer can also generate HFiles for later bulk importing, in that case only a single table and no mapping can be specified.
       </p><p>Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings&gt;]&gt;
</pre><p>
       </p><p>For example:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
</pre><p>
       </p></div><div class="section" title="14.1.12.&nbsp;RowCounter"><div class="titlepage"><div><div><h3 class="title"><a name="rowcounter"></a>14.1.12.&nbsp;RowCounter</h3></div></div></div><p>RowCounter is a utility that will count all the rows of a table.  This is a good utility to use
       as a sanity check to ensure that HBase can read all the blocks of a table if there are any concerns of metadata inconsistency.
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
       </p></div></div></div>
       
       <div class="section" title="14.2.&nbsp;Region Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.regionmgt"></a>14.2.&nbsp;Region Management</h2></div></div></div><div class="section" title="14.2.1.&nbsp;Major Compaction"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.majorcompact"></a>14.2.1.&nbsp;Major Compaction</h3></div></div></div><p>Major compactions can be requested via the HBase shell or <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>.
      </p><p>Note:  major compactions do NOT do region merges.  See <a class="xref" href="regions.arch.html#compaction" title="9.7.5.5.&nbsp;Compaction">Section&nbsp;9.7.5.5, “Compaction”</a> for more information about compactions.
      
      </p></div><div class="section" title="14.2.2.&nbsp;Merge"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.merge"></a>14.2.2.&nbsp;Merge</h3></div></div></div><p>Merge is a utility that can merge adjoining regions in the same table (see org.apache.hadoop.hbase.util.Merge).</p><pre class="programlisting">$ bin/hbase org.apache.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;
</pre><p>If you feel you have too many regions and want to consolidate them, Merge is the utility you need.  Merge must
      run be done when the cluster is down.  
      See the <a class="link" href="http://ofps.oreilly.com/titles/9781449396107/performance.html" target="_top">O'Reilly HBase Book</a> for
      an example of usage.
      </p><p>Additionally, there is a Ruby script attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1621" target="_top">HBASE-1621</a> 
      for region merging.
      </p></div></div>
      
      <div class="section" title="14.3.&nbsp;Node Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="node.management"></a>14.3.&nbsp;Node Management</h2></div></div></div><div class="section" title="14.3.1.&nbsp;Node Decommission"><div class="titlepage"><div><div><h3 class="title"><a name="decommission"></a>14.3.1.&nbsp;Node Decommission</h3></div></div></div><p>你可以在Hbase的特定的节点上运行下面的脚本来停止RegionServer: </p>
          <pre class="programlisting">$ ./bin/hbase-daemon.sh stop regionserver</pre>
          <p> RegionServer会首先关闭所有的region然后把它自己关闭，在停止的过程中，RegionServer的会向Zookeeper报告说他已经过期了。master会发现RegionServer已经死了，会把它当作崩溃的server来处理。他会将region分配到其他的节点上去。 </p>
          <div class="note" title="在下线节点之前要停止Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;">
            <h3 class="title">在下线节点之前要停止Load Balancer</h3>
            <p>如果在运行load balancer的时候，一个节点要关闭， 则Load Balancer和Master的recovery可能会争夺这个要下线的Regionserver。为了避免这个问题，先将load balancer停止，参见下面的 <a class="xref" href="book.htm#lb" title="Load Balancer">Load Balancer</a>. </p>
          </div>
          <p></p>
          <p> RegionServer下线有一个缺点就是其中的Region会有好一会离线。Regions是被按顺序关闭的。如果一个server上有很多region,从第一个region会被下线，到最后一个region被关闭，并且Master确认他已经死了，该region才可以上线，整个过程要花很长时间。在Hbase 0.90.2中，我们加入了一个功能，可以让节点逐渐的摆脱他的负载，最后关闭。HBase 0.90.2加入了 <code class="filename">graceful_stop.sh</code>脚本，可以这样用， </p>
          <pre class="programlisting">$ ./bin/graceful_stop.sh 
Usage: graceful_stop.sh [--config &amp;conf-dir&gt;] [--restart] [--reload] [--thrift] [--rest] &amp;hostname&gt;
 thrift      If we should stop/start thrift before/after the hbase stop/start
 rest        If we should stop/start rest before/after the hbase stop/start
 restart     If we should restart after graceful stop
 reload      Move offloaded regions back on to the stopped server
 debug       Move offloaded regions back on to the stopped server
 hostname    Hostname of server we are to stop</pre>
          <p></p>
          <p> 要下线一台RegionServer可以这样做 </p>
          <pre class="programlisting">$ ./bin/graceful_stop.sh HOSTNAME</pre>
          <p> 这里的<code class="varname">HOSTNAME</code>是RegionServer的host
            you would decommission. </p>
          <div class="note" title="On HOSTNAME" style="margin-left: 0.5in; margin-right: 0.5in;">
            <h3 class="title">On <code class="varname">HOSTNAME</code></h3>
            <p>传递到<code class="filename">graceful_stop.sh</code>的<code class="varname">HOSTNAME</code>必须和hbase使用的hostname一致，hbase用它来区分RegionServers。可以用master的UI来检查RegionServers的id。通常是hostname,也可能是FQDN。不管Hbase使用的哪一个，你可以将它传到 <code class="filename">graceful_stop.sh</code>脚本中去，目前他还不支持使用IP地址来推断hostname。所以使用IP就会发现server不在运行，也没有办法下线了。 </p>
          </div>
          <p> <code class="filename">graceful_stop.sh</code> 脚本会一个一个将region从RegionServer中移除出去，以减少改RegionServer的负载。他会先移除一个region,然后再将这个region安置到一个新的地方，再移除下一个，直到全部移除。最后<code class="filename">graceful_stop.sh</code>脚本会让RegionServer <span class="command"><strong>stop</strong></span>.,Master会注意到RegionServer已经下线了，这个时候所有的region已经重新部署好。RegionServer就可以干干净净的结束，没有WAL日志需要分割。 </p>
          <div class="note" title="Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;">
            <h3 class="title"><a name="lb"></a>Load Balancer</h3>
            <p> 当执行<span class="command"><strong>graceful_stop</strong></span>脚本的时候，要将Region Load Balancer关掉(否则balancer和下线脚本会在region部署的问题上存在冲突): </p>
            <pre class="programlisting">hbase(main):001:0&gt; balance_switch false
true
0 row(s) in 0.3590 seconds</pre>
            <p> 上面是将balancer关掉，要想开启： </p>
            <pre class="programlisting">hbase(main):001:0&gt; balance_switch true
false
0 row(s) in 0.3590 seconds</pre>
            <p></p>
          </div>
          <p></p>
          <div class="note" title="Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;"><p>
            </p></div><p>
        </p></div><div class="section" title="14.3.2.&nbsp;Rolling Restart"><div class="titlepage"><div>
          <div><h3 class="title"><a name="rolling"></a>14.3.2.&nbsp;依次重启</h3>
          </div></div></div><p>你还可以让这个脚本重启一个RegionServer,不改变上面的Region的位置。要想保留数据的位置，你可以依次重启(Rolling Restart),就像这样: </p>
          <pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre>
          <p> Tail <code class="filename">/tmp/log.txt</code>来看脚本的运行过程.上面的脚本只对RegionServer进行操作。要确认load balancer已经关掉。还需要在之前更新master。下面是一段依次重启的伪脚本,你可以借鉴它： </p>
          <div class="orderedlist">
            <ol class="orderedlist" type="1">
              <li class="listitem">
                <p>确认你的版本，保证配置已经rsync到整个集群中。如果版本是0.90.2，需要打上HBASE-3744 和 HBASE-3756两个补丁。 </p>
              </li>
              <li class="listitem">
                <p>运行hbck确保你的集群是一致的 </p>
                <pre class="programlisting">$ ./bin/hbase hbck</pre>
                <p> 当发现不一致的时候，可以修复他。 </p>
              </li>
              <li class="listitem">
                <p>重启Master: </p>
                <pre class="programlisting">$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master</pre>
                <p></p>
              </li>
              <li class="listitem">
                <p> 关闭region balancer:</p>
                <pre class="programlisting">$ echo "balance_switch false" | ./bin/hbase</pre>
                <p></p>
              </li>
              <li class="listitem">
                <p>在每个RegionServer上运行<code class="filename">graceful_stop.sh</code>： </p>
                <pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre>
                <p> 如果你在RegionServer还开起来thrift和rest server。还需要加上--thrift or --rest 选项 (参见 <code class="filename">graceful_stop.sh</code> 脚本的用法). </p>
              </li>
              <li class="listitem">
                <p>再次重启Master.这会把已经死亡的server列表清空，重新开启balancer. </p>
              </li>
              <li class="listitem">
                <p>运行 hbck 保证集群是一直的 </p>
              </li>
            </ol>
          </div>
          <div class="orderedlist"></div><p>
        </p></div></div>
        <div class="section" title="14.4.&nbsp;HBase Metrics"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase_metrics"></a>14.4.&nbsp;HBase Metrics</h2></div></div></div><div class="section" title="14.4.1.&nbsp;Metric Setup"><div class="titlepage"><div>
          <div><h3 class="title"><a name="metric_setup"></a>14.4.1.&nbsp;Metric <span class="title" style="clear: both">安装</span></h3>
          </div></div></div><p>参见 <a class="link" href="http://hbase.apache.org/metrics.html" target="_top">Metrics</a> 可以获得一个enable Metrics emission的指导。</p>
        </div><div class="section" title="14.4.2.&nbsp;RegionServer Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics"></a>14.4.2.&nbsp;RegionServer Metrics</h3></div></div></div><div class="section" title="14.4.2.1.&nbsp;hbase.regionserver.blockCacheCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheCount"></a>14.4.2.1.&nbsp;<code class="varname">hbase.regionserver.blockCacheCount</code></h4></div></div></div><p>内存中的Block cache item数量。这个是存储文件(HFiles)的缓存中的数量。</p>
        </div><div class="section" title="14.4.2.2.&nbsp;hbase.regionserver.blockCacheEvictedCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheEvictedCount"></a>14.4.2.2.&nbsp;<code class="varname">hbase.regionserver.blockCacheEvictedCount</code></h4></div></div></div><p>Number of blocks that had to be evicted from the block cache due to heap size constraints.</p></div><div class="section" title="14.4.2.3.&nbsp;hbase.regionserver.blockCacheFree"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheFree"></a>14.4.2.3.&nbsp;<code class="varname">hbase.regionserver.blockCacheFree</code></h4></div></div></div><p>内存中的Block cache memory 剩余 (单位 bytes).</p>
        </div><div class="section" title="14.4.2.4.&nbsp;hbase.regionserver.blockCacheHitCachingRatio"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitCachingRatio"></a>14.4.2.4.&nbsp;<code class="varname">hbase.regionserver.blockCacheHitCachingRatio</code></h4></div></div></div><p>Block cache hit caching ratio (0 to 100).  The cache-hit ratio for reads configured to look in the cache (i.e., cacheBlocks=true). </p></div><div class="section" title="14.4.2.5.&nbsp;hbase.regionserver.blockCacheHitCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitCount"></a>14.4.2.5.&nbsp;<code class="varname">hbase.regionserver.blockCacheHitCount</code></h4></div></div></div><p>Number of blocks of StoreFiles (HFiles) read from the cache.</p></div><div class="section" title="14.4.2.6.&nbsp;hbase.regionserver.blockCacheHitRatio"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitRatio"></a>14.4.2.6.&nbsp;<code class="varname">hbase.regionserver.blockCacheHitRatio</code></h4></div></div></div><p>Block cache 命中率(0 到 100).  Includes all read requests, although those with cacheBlocks=false
            will always read from disk and be counted as a "cache miss"</p>
          <p>&nbsp;</p>
        </div><div class="section" title="14.4.2.7.&nbsp;hbase.regionserver.blockCacheMissCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheMissCount"></a>14.4.2.7.&nbsp;<code class="varname">hbase.regionserver.blockCacheMissCount</code></h4></div></div></div><p>Number of blocks of StoreFiles (HFiles) requested but not read from the cache.</p></div><div class="section" title="14.4.2.8.&nbsp;hbase.regionserver.blockCacheSize"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheSize"></a>14.4.2.8.&nbsp;<code class="varname">hbase.regionserver.blockCacheSize</code></h4></div></div></div><p>内存中的Block cache 大小 (单位 bytes).  i.e., memory in use by the BlockCache</p></div><div class="section" title="14.4.2.9.&nbsp;hbase.regionserver.compactionQueueSize"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.compactionQueueSize"></a>14.4.2.9.&nbsp;<code class="varname">hbase.regionserver.compactionQueueSize</code></h4></div></div></div><p>compaction队列的大小. 这个值是需要进行compaction的region数目</p>
        </div><div class="section" title="14.4.2.10.&nbsp;hbase.regionserver.flushQueueSize"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.flushQueueSize"></a>14.4.2.10.&nbsp;<code class="varname">hbase.regionserver.flushQueueSize</code></h4></div></div></div><p>Number of enqueued regions in the MemStore awaiting flush.</p></div><div class="section" title="14.4.2.11.&nbsp;hbase.regionserver.fsReadLatency_avg_time"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsReadLatency_avg_time"></a>14.4.2.11.&nbsp;<code class="varname">hbase.regionserver.fsReadLatency_avg_time</code></h4></div></div></div><p>文件系统延迟 (ms). 这个值是平均读HDFS的延迟时间</p>
        </div><div class="section" title="14.4.2.12.&nbsp;hbase.regionserver.fsReadLatency_num_ops"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsReadLatency_num_ops"></a>14.4.2.12.&nbsp;<code class="varname">hbase.regionserver.fsReadLatency_num_ops</code></h4></div></div></div><p>Filesystem read operations.</p></div><div class="section" title="14.4.2.13.&nbsp;hbase.regionserver.fsSyncLatency_avg_time"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsSyncLatency_avg_time"></a>14.4.2.13.&nbsp;<code class="varname">hbase.regionserver.fsSyncLatency_avg_time</code></h4></div></div></div><p>文件系统同步延迟(ms).  Latency to sync the write-ahead log records to the filesystem.</p></div><div class="section" title="14.4.2.14.&nbsp;hbase.regionserver.fsSyncLatency_num_ops"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsSyncLatency_num_ops"></a>14.4.2.14.&nbsp;<code class="varname">hbase.regionserver.fsSyncLatency_num_ops</code></h4></div></div></div><p>Number of operations to sync the write-ahead log records to the filesystem.</p></div><div class="section" title="14.4.2.15.&nbsp;hbase.regionserver.fsWriteLatency_avg_time"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsWriteLatency_avg_time"></a>14.4.2.15.&nbsp;<code class="varname">hbase.regionserver.fsWriteLatency_avg_time</code></h4></div></div></div><p>文件系统写延迟(ms).  Total latency for all writers, including StoreFiles and write-head log.</p></div><div class="section" title="14.4.2.16.&nbsp;hbase.regionserver.fsWriteLatency_num_ops"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsWriteLatency_num_ops"></a>14.4.2.16.&nbsp;<code class="varname">hbase.regionserver.fsWriteLatency_num_ops</code></h4></div></div></div><p>Number of filesystem write operations, including StoreFiles and write-ahead log.</p></div><div class="section" title="14.4.2.17.&nbsp;hbase.regionserver.memstoreSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.memstoreSizeMB"></a>14.4.2.17.&nbsp;<code class="varname">hbase.regionserver.memstoreSizeMB</code></h4></div></div></div><p>所有的RegionServer的memstore大小 (MB)</p>
        </div><div class="section" title="14.4.2.18.&nbsp;hbase.regionserver.regions"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.regions"></a>14.4.2.18.&nbsp;<code class="varname">hbase.regionserver.regions</code></h4></div></div></div><p>RegionServer服务的regions数量</p>
        </div><div class="section" title="14.4.2.19.&nbsp;hbase.regionserver.requests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.requests"></a>14.4.2.19.&nbsp;<code class="varname">hbase.regionserver.requests</code></h4></div></div></div>
          <p>读写请求的全部数量。请求是指RegionServer的RPC数量，因此一次Get一个请求，但一个缓存设为1000的Scan也会在每次调用'next'时导致一个请求。一个批量load是一个Hfile一个请求。</p></div><div class="section" title="14.4.2.20.&nbsp;hbase.regionserver.storeFileIndexSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.storeFileIndexSizeMB"></a>14.4.2.20.&nbsp;<code class="varname">hbase.regionserver.storeFileIndexSizeMB</code></h4></div></div></div><p>当前RegionServer的storefile索引的总大小(MB)</p>
          </div><div class="section" title="14.4.2.21.&nbsp;hbase.regionserver.stores"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.stores"></a>14.4.2.21.&nbsp;<code class="varname">hbase.regionserver.stores</code></h4></div></div></div>
            <p>RegionServer打开的stores数量。一个stores对应一个column family。例如，一个包含列族的表有3个region在这个RegionServer上，对应一个 column family就会有3个store.</p></div><div class="section" title="14.4.2.22.&nbsp;hbase.regionserver.storeFiles"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.storeFiles"></a>14.4.2.22.&nbsp;<code class="varname">hbase.regionserver.storeFiles</code></h4></div></div></div><p>RegionServer打开的存储文件(HFile)数量。这个值一定大于等于store的数量。</p>
            </div></div></div>
           
           <div class="section" title="14.5.&nbsp;HBase Monitoring"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.monitoring"></a>14.5.&nbsp;HBase Monitoring</h2></div></div></div><div class="section" title="14.5.1.&nbsp;Overview"><div class="titlepage"><div><div><h3 class="title"><a name="ops.monitoring.overview"></a>14.5.1.&nbsp;Overview</h3></div></div></div><p>The following metrics are arguably the most important to monitor for each RegionServer for
      "macro monitoring", preferably with a system like <a class="link" href="http://opentsdb.net/" target="_top">OpenTSDB</a>.
      If your cluster is having performance issues it's likely that you'll see something unusual with 
      this group.
      </p><p>HBase: 
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Requests</li><li class="listitem">Compactions queue</li></ul></div><p>
      </p><p>OS: 
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">IO Wait</li><li class="listitem">User CPU</li></ul></div><p>
      </p><p>Java: 
      </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">GC</li></ul></div><p>
      </p><p>
      </p><p>
      For more information on HBase metrics, see <a class="xref" href="hbase_metrics.html" title="14.4.&nbsp;HBase Metrics">Section&nbsp;14.4, “HBase Metrics”</a>.
      </p></div><div class="section" title="14.5.2.&nbsp;Slow Query Log"><div class="titlepage"><div><div><h3 class="title"><a name="ops.slow.query"></a>14.5.2.&nbsp;Slow Query Log</h3></div></div></div><p>The HBase slow query log consists of parseable JSON structures describing the properties of those client operations (Gets, Puts, Deletes, etc.) that either took too long to run, or produced too much output. The thresholds for "too long to run" and "too much output" are configurable, as described below. The output is produced inline in the main region server logs so that it is easy to discover further details from context with other logged events. It is also prepended with identifying tags <code class="constant">(responseTooSlow)</code>, <code class="constant">(responseTooLarge)</code>, <code class="constant">(operationTooSlow)</code>, and <code class="constant">(operationTooLarge)</code> in order to enable easy filtering with grep, in case the user desires to see only slow queries.
</p><div class="section" title="14.5.2.1.&nbsp;Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e9364"></a>14.5.2.1.&nbsp;Configuration</h4></div></div></div><p>There are two configuration knobs that can be used to adjust the thresholds for when queries are logged.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">hbase.ipc.warn.response.time</code> Maximum number of milliseconds that a query can be run without being logged. Defaults to 10000, or 10 seconds. Can be set to -1 to disable logging by time.
</li><li class="listitem"><code class="varname">hbase.ipc.warn.response.size</code> Maximum byte size of response that a query can return without being logged. Defaults to 100 megabytes. Can be set to -1 to disable logging by size.
</li></ul></div></div><div class="section" title="14.5.2.2.&nbsp;Metrics"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e9378"></a>14.5.2.2.&nbsp;Metrics</h4></div></div></div><p>The slow query log exposes to metrics to JMX.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">hadoop.regionserver_rpc_slowResponse</code> a global metric reflecting the durations of all responses that triggered logging.</li><li class="listitem"><code class="varname">hadoop.regionserver_rpc_methodName.aboveOneSec</code> A metric reflecting the durations of all responses that lasted for more than one second.</li></ul></div><p>
</p></div><div class="section" title="14.5.2.3.&nbsp;Output"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e9393"></a>14.5.2.3.&nbsp;Output</h4></div></div></div><p>The output is tagged with operation e.g. <code class="constant">(operationTooSlow)</code> if the call was a client operation, such as a Put, Get, or Delete, which we expose detailed fingerprint information for. If not, it is tagged <code class="constant">(responseTooSlow)</code> and still produces parseable JSON output, but with less verbose information solely regarding its duration and size in the RPC itself. <code class="constant">TooLarge</code> is substituted for <code class="constant">TooSlow</code> if the response size triggered the logging, with <code class="constant">TooLarge</code> appearing even in the case that both size and duration triggered logging.
</p></div><div class="section" title="14.5.2.4.&nbsp;Example"><div class="titlepage"><div><div><h4 class="title"><a name="d1934e9413"></a>14.5.2.4.&nbsp;Example</h4></div></div></div><p>
</p><pre class="programlisting">2011-09-08 10:01:25,824 WARN org.apache.hadoop.ipc.HBaseServer: (operationTooSlow): {"tables":{"riley2":{"puts":[{"totalColumns":11,"families":{"actions":[{"timestamp":1315501284459,"qualifier":"0","vlen":9667580},{"timestamp":1315501284459,"qualifier":"1","vlen":10122412},{"timestamp":1315501284459,"qualifier":"2","vlen":11104617},{"timestamp":1315501284459,"qualifier":"3","vlen":13430635}]},"row":"cfcd208495d565ef66e7dff9f98764da:0"}],"families":["actions"]}},"processingtimems":956,"client":"10.47.34.63:33623","starttimems":1315501284456,"queuetimems":0,"totalPuts":1,"class":"HRegionServer","responsesize":0,"method":"multiPut"}</pre><p>
</p><p>Note that everything inside the "tables" structure is output produced by MultiPut's fingerprint, while the rest of the information is RPC-specific, such as processing time and client IP/port. Other client operations follow the same pattern and the same general structure, with necessary differences due to the nature of the individual operations. In the case that the call is not a client operation, that detailed fingerprint information will be completely absent.
</p><p>This particular example, for example, would indicate that the likely cause of slowness is simply a very large (on the order of 100MB) multiput, as we can tell by the "vlen," or value length, fields of each put in the multiPut.
</p></div></div></div>
<div class="section" title="14.6.&nbsp;Cluster Replication"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cluster_replication"></a>14.6.&nbsp;Cluster Replication</h2></div></div></div><p>See <a class="link" href="http://hbase.apache.org/replication.html" target="_top">Cluster Replication</a>.
    </p></div>
    <div class="section" title="14.7.&nbsp;HBase Backup"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.backup"></a>14.7.&nbsp;HBase Backup</h2></div></div></div><p>There are two broad strategies for performing HBase backups: backing up with a full cluster shutdown, and backing up on a live cluster. 
    Each approach has pros and cons.   
    </p><p>For additional information, see <a class="link" href="http://blog.sematext.com/2011/03/11/hbase-backup-options/" target="_top">HBase Backup Options</a> over on the Sematext Blog.
    </p><div class="section" title="14.7.1.&nbsp;Full Shutdown Backup"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.fullshutdown"></a>14.7.1.&nbsp;Full Shutdown Backup</h3></div></div></div><p>Some environments can tolerate a periodic full shutdown of their HBase cluster, for example if it is being used a back-end analytic capacity
      and not serving front-end web-pages.  The benefits are that the NameNode/Master are RegionServers are down, so there is no chance of missing
      any in-flight changes to either StoreFiles or metadata.  The obvious con is that the cluster is down.  The steps include:
      </p><div class="section" title="14.7.1.1.&nbsp;Stop HBase"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.stop"></a>14.7.1.1.&nbsp;Stop HBase</h4></div></div></div><p>
        </p></div><div class="section" title="14.7.1.2.&nbsp;Distcp"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.distcp"></a>14.7.1.2.&nbsp;Distcp</h4></div></div></div><p>Distcp could be used to either copy the contents of the HBase directory in HDFS to either the same cluster in another directory, or 
        to a different cluster.
        </p><p>Note:  Distcp works in this situation because the cluster is down and there are no in-flight edits to files.  
        Distcp-ing of files in the HBase directory is not generally recommended on a live cluster.
        </p></div><div class="section" title="14.7.1.3.&nbsp;Restore (if needed)"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.restore"></a>14.7.1.3.&nbsp;Restore (if needed)</h4></div></div></div><p>The backup of the hbase directory from HDFS is copied onto the 'real' hbase directory via distcp.  The act of copying these files 
        creates new HDFS metadata, which is why a restore of the NameNode edits from the time of the HBase backup isn't required for this kind of
        restore, because it's a restore (via distcp) of a specific HDFS directory (i.e., the HBase part) not the entire HDFS file-system.
        </p></div></div><div class="section" title="14.7.2.&nbsp;Live Cluster Backup - Replication"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.replication"></a>14.7.2.&nbsp;Live Cluster Backup - Replication</h3></div></div></div><p>This approach assumes that there is a second cluster.  
      See the HBase page on <a class="link" href="http://hbase.apache.org/replication.html" target="_top">replication</a> for more information.
      </p></div><div class="section" title="14.7.3.&nbsp;Live Cluster Backup - CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.copytable"></a>14.7.3.&nbsp;Live Cluster Backup - CopyTable</h3></div></div></div><p>The <a class="xref" href="ops_mgt.html#copytable" title="14.1.6.&nbsp;CopyTable">Section&nbsp;14.1.6, “CopyTable”</a> utility could either be used to copy data from one table to another on the 
      same cluster, or to copy data to another table on another cluster.
      </p><p>Since the cluster is up, there is a risk that edits could be missed in the copy process.
      </p></div><div class="section" title="14.7.4.&nbsp;Live Cluster Backup - Export"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.export"></a>14.7.4.&nbsp;Live Cluster Backup - Export</h3></div></div></div><p>The <a class="xref" href="ops_mgt.html#export" title="14.1.7.&nbsp;Export">Section&nbsp;14.1.7, “Export”</a> approach dumps the content of a table to HDFS on the same cluster.  To restore the data, the
      <a class="xref" href="ops_mgt.html#import" title="14.1.8.&nbsp;Import">Section&nbsp;14.1.8, “Import”</a> utility would be used.
      </p><p>Since the cluster is up, there is a risk that edits could be missed in the export process.
      </p></div></div>
      
      <div class="section" title="14.8.&nbsp;Capacity Planning"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.capacity"></a>14.8.&nbsp;Capacity Planning</h2></div></div></div><div class="section" title="14.8.1.&nbsp;Storage"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.storage"></a>14.8.1.&nbsp;Storage</h3></div></div></div><p>A common question for HBase administrators is estimating how much storage will be required for an HBase cluster.
      There are several apsects to consider, the most important of which is what data load into the cluster.  Start
      with a solid understanding of how HBase handles data internally (KeyValue).
      </p><div class="section" title="14.8.1.1.&nbsp;KeyValue"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.storage.kv"></a>14.8.1.1.&nbsp;KeyValue</h4></div></div></div><p>HBase storage will be dominated by KeyValues.  See <a class="xref" href="regions.arch.html#keyvalue" title="9.7.5.4.&nbsp;KeyValue">Section&nbsp;9.7.5.4, “KeyValue”</a> and <a class="xref" href="rowkey.design.html#keysize" title="6.3.2.&nbsp;Try to minimize row and column sizes">Section&nbsp;6.3.2, “Try to minimize row and column sizes”</a> for 
        how HBase stores data internally.  
        </p><p>It is critical to understand that there is a KeyValue instance for every attribute stored in a row, and the 
        rowkey-length, ColumnFamily name-length and attribute lengths will drive the size of the database more than any other
        factor.
        </p></div><div class="section" title="14.8.1.2.&nbsp;StoreFiles and Blocks"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.storage.sf"></a>14.8.1.2.&nbsp;StoreFiles and Blocks</h4></div></div></div><p>KeyValue instances are aggregated into blocks, and the blocksize is configurable on a per-ColumnFamily basis.
        Blocks are aggregated into StoreFile's.  See <a class="xref" href="regions.arch.html" title="9.7.&nbsp;Regions">Section&nbsp;9.7, “Regions”</a>.
        </p></div><div class="section" title="14.8.1.3.&nbsp;HDFS Block Replication"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.storage.hdfs"></a>14.8.1.3.&nbsp;HDFS Block Replication</h4></div></div></div><p>Because HBase runs on top of HDFS, factor in HDFS block replication into storage calculations.
        </p></div></div><div class="section" title="14.8.2.&nbsp;Regions"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.regions"></a>14.8.2.&nbsp;Regions</h3></div></div></div><p>Another common question for HBase administrators is determining the right number of regions per
      RegionServer.  This affects both storage and hardware planning. See <a class="xref" href="perf.configurations.html#perf.number.of.regions" title="11.4.1.&nbsp;Number of Regions">Section&nbsp;11.4.1, “Number of Regions”</a>.
      </p></div></div>
      
      <div class="chapter" title="Chapter&nbsp;15.&nbsp;Building and Developing HBase"><div class="titlepage"><div><div><h2 class="title"><a name="developer"></a>Chapter&nbsp;15.&nbsp;Building and Developing HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="developer.html#repos">15.1. HBase Repositories</a></span></dt><dd><dl><dt><span class="section"><a href="developer.html#svn">15.1.1. SVN</a></span></dt><dt><span class="section"><a href="developer.html#git">15.1.2. Git</a></span></dt></dl></dd><dt><span class="section"><a href="ides.html">15.2. IDEs</a></span></dt><dd><dl><dt><span class="section"><a href="ides.html#eclipse">15.2.1. Eclipse</a></span></dt></dl></dd><dt><span class="section"><a href="build.html">15.3. Building HBase</a></span></dt><dd><dl><dt><span class="section"><a href="build.html#build.snappy">15.3.1. Building in snappy compression support</a></span></dt><dt><span class="section"><a href="build.html#build.tgz">15.3.2. Building the HBase tarball</a></span></dt><dt><span class="section"><a href="build.html#mvn_repo">15.3.3. Adding an HBase release to Apache's Maven Repository</a></span></dt><dt><span class="section"><a href="build.html#build.gotchas">15.3.4. Build Gotchas</a></span></dt></dl></dd><dt><span class="section"><a href="hbase.site.publishing.html">15.4. Publishing a new version of hbase.apache.org</a></span></dt><dt><span class="section"><a href="hbase.tests.html">15.5. Tests</a></span></dt><dd><dl><dt><span class="section"><a href="hbase.tests.html#hbase.unittests">15.5.1. Unit Tests</a></span></dt><dt><span class="section"><a href="hbase.tests.html#integration.tests">15.5.2. Integration Tests</a></span></dt></dl></dd><dt><span class="section"><a href="maven.build.commands.html">15.6. Maven Build Commands</a></span></dt><dd><dl><dt><span class="section"><a href="maven.build.commands.html#maven.build.commands.compile">15.6.1. Compile</a></span></dt><dt><span class="section"><a href="maven.build.commands.html#maven.build.commands.unitall">15.6.2. Running all or individual Unit Tests</a></span></dt><dt><span class="section"><a href="maven.build.commands.html#maven.build.commanas.integration.tests">15.6.3. Running all or individual Integration Tests</a></span></dt><dt><span class="section"><a href="maven.build.commands.html#maven.build.hadoop">15.6.4. To build against hadoop 0.22.x or 0.23.x</a></span></dt></dl></dd><dt><span class="section"><a href="getting.involved.html">15.7. Getting Involved</a></span></dt><dd><dl><dt><span class="section"><a href="getting.involved.html#mailing.list">15.7.1. Mailing Lists</a></span></dt><dt><span class="section"><a href="getting.involved.html#jira">15.7.2. Jira</a></span></dt></dl></dd><dt><span class="section"><a href="developing.html">15.8. Developing</a></span></dt><dd><dl><dt><span class="section"><a href="developing.html#codelines">15.8.1. Codelines</a></span></dt><dt><span class="section"><a href="developing.html#unit.tests">15.8.2. Unit Tests</a></span></dt><dt><span class="section"><a href="developing.html#code.standards">15.8.3. Code Standards</a></span></dt></dl></dd><dt><span class="section"><a href="submitting.patches.html">15.9. Submitting Patches</a></span></dt><dd><dl><dt><span class="section"><a href="submitting.patches.html#submitting.patches.create">15.9.1. Create Patch</a></span></dt><dt><span class="section"><a href="submitting.patches.html#submitting.patches.naming">15.9.2. Patch File Naming</a></span></dt><dt><span class="section"><a href="submitting.patches.html#submitting.patches.tests">15.9.3. Unit Tests</a></span></dt><dt><span class="section"><a href="submitting.patches.html#submitting.patches.jira">15.9.4. Attach Patch to Jira</a></span></dt><dt><span class="section"><a href="submitting.patches.html#common.patch.feedback">15.9.5. Common Patch Feedback</a></span></dt><dt><span class="section"><a href="submitting.patches.html#reviewboard">15.9.6. ReviewBoard</a></span></dt><dt><span class="section"><a href="submitting.patches.html#committing.patches">15.9.7. Committing Patches</a></span></dt></dl></dd></dl></div><p>This chapter will be of interest only to those building and developing HBase (i.e., as opposed to
    just downloading the latest distribution).
    </p><div class="section" title="15.1.&nbsp;HBase Repositories"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="repos"></a>15.1.&nbsp;HBase Repositories</h2></div></div></div><div class="section" title="15.1.1.&nbsp;SVN"><div class="titlepage"><div><div><h3 class="title"><a name="svn"></a>15.1.1.&nbsp;SVN</h3></div></div></div><pre class="programlisting">svn co http://svn.apache.org/repos/asf/hbase/trunk hbase-core-trunk 
        </pre></div><div class="section" title="15.1.2.&nbsp;Git"><div class="titlepage"><div><div><h3 class="title"><a name="git"></a>15.1.2.&nbsp;Git</h3></div></div></div><pre class="programlisting">git clone git://git.apache.org/hbase.git
        </pre></div></div></div>
        
        <div class="section" title="15.2.&nbsp;IDEs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ides"></a>15.2.&nbsp;IDEs</h2></div></div></div><div class="section" title="15.2.1.&nbsp;Eclipse"><div class="titlepage"><div><div><h3 class="title"><a name="eclipse"></a>15.2.1.&nbsp;Eclipse</h3></div></div></div>
            <div title="6.1.1. Schema Updates">
              <div>
                <div></div>
              </div>
            </div>
            <div class="section" title="6.1. IDEs">
              <div class="titlepage">
                <div>
                  <div></div>
                  </div>
                </div>
              <div class="section" title="6.1.1. Eclipse">
                <div class="titlepage">
                  <div>
                    <div></div>
                    </div>
                  </div>
                <p>参见 <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3678" target="_top">HBASE-3678 Add Eclipse-based Apache Formatter to HBase Wiki</a>可以看到一个eclipse的格式化文件，可以帮你把编码转换成符合Hbase的格式。
                  这个issue还包含有使用这个formatter的指导。 </p>
                </div>
            </div>
            <p>&nbsp;</p>
            <div class="section" title="15.2.1.1.&nbsp;Code Formatting"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.code.formatting"></a>15.2.1.1.&nbsp;Code Formatting</h4></div></div></div><p>See <a class="link" href="https://issues.apache.org/jira/browse/HBASE-3678" target="_top">HBASE-3678 Add Eclipse-based Apache Formatter to HBase Wiki</a>
              for an Eclipse formatter to help ensure your code conforms to HBase'y coding convention.
            The issue includes instructions for loading the attached formatter.</p><p>Also, no @author tags - that's a rule.  Quality Javadoc comments are appreciated.  And include the Apache license.</p></div><div class="section" title="15.2.1.2.&nbsp;Subversive Plugin"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.svn"></a>15.2.1.2.&nbsp;Subversive Plugin</h4></div></div></div><p>Download and install the Subversive plugin.</p><p>Set up an SVN Repository target from <a class="xref" href="developer.html#svn" title="15.1.1.&nbsp;SVN">Section&nbsp;15.1.1, “SVN”</a>, then check out the code.</p></div><div class="section" title="15.2.1.3.&nbsp;HBase Project Setup"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.maven.setup"></a>15.2.1.3.&nbsp;HBase Project Setup</h4></div></div></div>
            To set up your Eclipse environment for HBase, close Eclipse and execute...
            <pre class="programlisting">mvn eclipse:eclipse
            </pre>
            ... from your local HBase project directory in your workspace to generate some new <code class="filename">.project</code> 
            and <code class="filename">.classpath</code>files.  Then reopen Eclipse.
            </div><div class="section" title="15.2.1.4.&nbsp;Maven Plugin"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.maven.plugin"></a>15.2.1.4.&nbsp;Maven Plugin</h4></div></div></div><p>Download and install the Maven plugin.  For example, Help -&gt; Install New Software -&gt; (search for Maven Plugin)</p></div><div class="section" title="15.2.1.5.&nbsp;Maven Classpath Variable"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.maven.class"></a>15.2.1.5.&nbsp;Maven Classpath Variable</h4></div></div></div><p>The <code class="varname">M2_REPO</code> classpath variable needs to be set up for the project.  This needs to be set to 
            your local Maven repository, which is usually <code class="filename">~/.m2/repository</code></p>
            If this classpath variable is not configured, you will see compile errors in Eclipse like this...
            <pre class="programlisting">Description	Resource	Path	Location	Type
The project cannot be built until build path errors are resolved	hbase		Unknown	Java Problem 
Unbound classpath variable: 'M2_REPO/asm/asm/3.1/asm-3.1.jar' in project 'hbase'	hbase		Build path	Build Path Problem
Unbound classpath variable: 'M2_REPO/com/github/stephenc/high-scale-lib/high-scale-lib/1.1.1/high-scale-lib-1.1.1.jar' in project 'hbase'	hbase		Build path	Build Path Problem 
Unbound classpath variable: 'M2_REPO/com/google/guava/guava/r09/guava-r09.jar' in project 'hbase'	hbase		Build path	Build Path Problem
Unbound classpath variable: 'M2_REPO/com/google/protobuf/protobuf-java/2.3.0/protobuf-java-2.3.0.jar' in project 'hbase'	hbase		Build path	Build Path Problem Unbound classpath variable:
            </pre></div><div class="section" title="15.2.1.6.&nbsp;Import via m2eclipse"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.m2eclipse"></a>15.2.1.6.&nbsp;Import via m2eclipse</h4></div></div></div><p>If you install the m2eclipse and import the HBase pom.xml in your workspace, you will have to fix your eclipse Build Path.
            Remove <code class="filename">target</code> folder, add <code class="filename">target/generated-jamon</code>
            and <code class="filename">target/generated-sources/java</code> folders. You may also remove from your Build Path
            the exclusions on the <code class="filename">src/main/resources</code> and <code class="filename">src/test/resources</code>
            to avoid error message in the console 'Failed to execute goal org.apache.maven.plugins:maven-antrun-plugin:1.6:run (default) on project hbase: 
            'An Ant BuildException has occured: Replace: source file .../target/classes/hbase-default.xml doesn't exist'. This will also
            reduce the eclipse build cycles and make your life easier when developing.</p></div><div class="section" title="15.2.1.7.&nbsp;Eclipse Known Issues"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.issues"></a>15.2.1.7.&nbsp;Eclipse Known Issues</h4></div></div></div><p>Eclipse will currently complain about <code class="filename">Bytes.java</code>.  It is not possible to turn these errors off.</p><pre class="programlisting">            
Description	Resource	Path	Location	Type
Access restriction: The method arrayBaseOffset(Class) from the type Unsafe is not accessible due to restriction on required library /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Classes/classes.jar	Bytes.java	/hbase/src/main/java/org/apache/hadoop/hbase/util	line 1061	Java Problem
Access restriction: The method arrayIndexScale(Class) from the type Unsafe is not accessible due to restriction on required library /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Classes/classes.jar	Bytes.java	/hbase/src/main/java/org/apache/hadoop/hbase/util	line 1064	Java Problem
Access restriction: The method getLong(Object, long) from the type Unsafe is not accessible due to restriction on required library /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Classes/classes.jar	Bytes.java	/hbase/src/main/java/org/apache/hadoop/hbase/util	line 1111	Java Problem
             </pre></div><div class="section" title="15.2.1.8.&nbsp;Eclipse - More Information"><div class="titlepage"><div><div><h4 class="title"><a name="eclipse.more"></a>15.2.1.8.&nbsp;Eclipse - More Information</h4></div></div></div><p>For additional information on setting up Eclipse for HBase development on Windows, see 
             <a class="link" href="http://michaelmorello.blogspot.com/2011/09/hbase-subversion-eclipse-windows.html" target="_top">Michael Morello's blog</a> on the topic.
             </p></div></div></div>
             
             <div class="section" title="15.3.&nbsp;Building HBase"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="build"></a>15.3.&nbsp;Building HBase</h2></div></div></div><p>This section will be of interest only to those building HBase from source.
      </p><div class="section" title="15.3.1.&nbsp;Building in snappy compression support"><div class="titlepage"><div><div><h3 class="title"><a name="build.snappy"></a>15.3.1.&nbsp;Building in snappy compression support</h3></div></div></div><p>Pass <code class="code">-Dsnappy</code> to trigger the snappy maven profile for building
            snappy native libs into hbase.</p></div><div class="section" title="15.3.2.&nbsp;Building the HBase tarball"><div class="titlepage"><div><div><h3 class="title"><a name="build.tgz"></a>15.3.2.&nbsp;Building the HBase tarball</h3></div></div></div><p>Do the following to build the HBase tarball.
        Passing the -Drelease will generate javadoc and run the RAT plugin to verify licenses on source.
        </p><pre class="programlisting">% MAVEN_OPTS="-Xmx2g" mvn clean site install assembly:single -Dmaven.test.skip -Prelease</pre><p>
</p></div><div class="section" title="15.3.3.&nbsp;Adding an HBase release to Apache's Maven Repository"><div class="titlepage"><div><div><h3 class="title"><a name="mvn_repo"></a>15.3.3.&nbsp;Adding an HBase release to Apache's Maven Repository</h3></div></div></div><p>Follow the instructions at
        <a class="link" href="http://www.apache.org/dev/publishing-maven-artifacts.html" target="_top">Publishing Maven Artifacts</a>.
            The 'trick' to making it all work is answering the questions put to you by the mvn release plugin properly,
            making sure it is using the actual branch AND before doing the <span class="command"><strong>mvn release:perform</strong></span> step,
            VERY IMPORTANT, check and if necessary hand edit the release.properties file that was put under <code class="varname">${HBASE_HOME}</code>
            by the previous step, <span class="command"><strong>release:perform</strong></span>. You need to edit it to make it point at
            right locations in SVN.
        </p><p>Use maven 3.0.x.
        </p><p>At the <span class="command"><strong>mvn release:perform</strong></span> step, before starting, if you are for example
        releasing hbase 0.92.0, you need to make sure the pom.xml version is 0.92.0-SNAPSHOT.  This needs
        to be checked in.  Since we do the maven release after actual release, I've been doing this
        checkin into a particular tag rather than into the actual release tag.  So, say we released
        hbase 0.92.0 and now we want to do the release to the maven repository, in svn, the 0.92.0
        release will be tagged 0.92.0.  Making the maven release, copy the 0.92.0 tag to 0.92.0mvn.
        Check out this tag and change the version therein and commit.
        </p><p>Here is how I'd answer the questions at <span class="command"><strong>release:prepare</strong></span> time:
        </p><pre class="programlisting">What is the release version for "HBase"? (org.apache.hbase:hbase) 0.92.0: : 
What is SCM release tag or label for "HBase"? (org.apache.hbase:hbase) hbase-0.92.0: : 0.92.0mvnrelease
What is the new development version for "HBase"? (org.apache.hbase:hbase) 0.92.1-SNAPSHOT: : 
[INFO] Transforming 'HBase'...</pre><p>
        </p><p>A strange issue I ran into was the one where the upload into the apache
        repository was being sprayed across multiple apache machines making it so I could
        not release.  See <a class="link" href="https://issues.apache.org/jira/browse/INFRA-4482" target="_top">INFRA-4482 Why is my upload to mvn spread across multiple repositories?</a>.</p><p><a name="mvn.settings.file"></a>Here is my <code class="filename">~/.m2/settings.xml</code>.
        </p><pre class="programlisting">&lt;settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0
                      http://maven.apache.org/xsd/settings-1.0.0.xsd"&gt;
  &lt;servers&gt;
    &lt;!- To publish a snapshot of some part of Maven --&gt;
    &lt;server&gt;
      &lt;id&gt;apache.snapshots.https&lt;/id&gt;
      &lt;username&gt;YOUR_APACHE_ID
      &lt;/username&gt;
      &lt;password&gt;YOUR_APACHE_PASSWORD
      &lt;/password&gt;
    &lt;/server&gt;
    &lt;!-- To publish a website using Maven --&gt;
    &lt;!-- To stage a release of some part of Maven --&gt;
    &lt;server&gt;
      &lt;id&gt;apache.releases.https&lt;/id&gt;
      &lt;username&gt;YOUR_APACHE_ID
      &lt;/username&gt;
      &lt;password&gt;YOUR_APACHE_PASSWORD
      &lt;/password&gt;
    &lt;/server&gt;
  &lt;/servers&gt;
  &lt;profiles&gt;
    &lt;profile&gt;
      &lt;id&gt;apache-release&lt;/id&gt;
      &lt;properties&gt;
    &lt;gpg.keyname&gt;YOUR_KEYNAME&lt;/gpg.keyname&gt;
    &lt;!--Keyname is something like this ... 00A5F21E... do gpg --list-keys to find it--&gt;
    &lt;gpg.passphrase&gt;YOUR_KEY_PASSWORD
    &lt;/gpg.passphrase&gt;
      &lt;/properties&gt;
    &lt;/profile&gt;
  &lt;/profiles&gt;
&lt;/settings&gt;
        </pre><p>
        </p><p>When you run <span class="command"><strong>release:perform</strong></span>, pass <span class="command"><strong>-Papache-release</strong></span>
        else it will not 'sign' the artifacts it uploads.
        </p><p>If you see run into the below, its because you need to edit version in the pom.xml and add
        <code class="code">-SNAPSHOT</code> to the version (and commit).
        </p><pre class="programlisting">[INFO] Scanning for projects...
[INFO] Searching repository for plugin with prefix: 'release'.
[INFO] ------------------------------------------------------------------------
[INFO] Building HBase
[INFO]    task-segment: [release:prepare] (aggregator-style)
[INFO] ------------------------------------------------------------------------
[INFO] [release:prepare {execution: default-cli}]
[INFO] ------------------------------------------------------------------------
[ERROR] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] You don't have a SNAPSHOT project in the reactor projects list.
[INFO] ------------------------------------------------------------------------
[INFO] For more information, run Maven with the -e switch
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 3 seconds
[INFO] Finished at: Sat Mar 26 18:11:07 PDT 2011
[INFO] Final Memory: 35M/423M
[INFO] -----------------------------------------------------------------------</pre><p>
        </p></div><div class="section" title="15.3.4.&nbsp;Build Gotchas"><div class="titlepage"><div><div><h3 class="title"><a name="build.gotchas"></a>15.3.4.&nbsp;Build Gotchas</h3></div></div></div><p>If you see <code class="code">Unable to find resource 'VM_global_library.vm'</code>, ignore it.  
			Its not an error.  It is <a class="link" href="http://jira.codehaus.org/browse/MSITE-286" target="_top">officially ugly</a> though.
           </p></div></div>
           
           <div class="section" title="15.4.&nbsp;Publishing a new version of hbase.apache.org"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.site.publishing"></a>15.4.&nbsp;Publishing a new version of hbase.apache.org</h2></div></div></div><p>Set up your apache credentials and the target site location locally in a place and
    form that maven can pick it up, in <code class="filename">~/.m2/settings.xml</code>.  See ??? for an example.
    Next, run the following:
    </p><pre class="programlisting">$ mvn -DskipTests -Papache-release site site:deploy</pre><p>
    You will be asked for your password.  It can take a little time.
    Remember that it can take a few hours for your site changes to show up.
    </p></div>
    
    <div class="section" title="15.5.&nbsp;Tests"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.tests"></a>15.5.&nbsp;Tests</h2></div></div></div><p>HBase tests are divided into two groups: <a class="xref" href="hbase.tests.html#hbase.unittests" title="15.5.1.&nbsp;Unit Tests">Section&nbsp;15.5.1, “Unit Tests”</a> and
<a class="xref" href="hbase.tests.html#integration.tests" title="15.5.2.&nbsp;Integration Tests">Section&nbsp;15.5.2, “Integration Tests”</a> (As of this writing, Integration tests are little
developed).
Unit tests are run by the Apache Continuous Integration server, Jenkins at
builds.apache.org, and by developers when they are verifying a fix does not cause breakage elsewhere in the code base.
Integration tests are generally long-running tests that are invoked out-of-bound of
the CI server when you want to do more intensive testing beyond the unit test set.
Integration tests, for example, are run proving a release candidate or a production
deploy. Below we go into more detail on each of these test types.  Developers at a
minimum should familiarize themselves with the unit test detail; unit tests in
HBase have a character not usually seen in other projects.</p><div class="section" title="15.5.1.&nbsp;Unit Tests"><div class="titlepage"><div><div><h3 class="title"><a name="hbase.unittests"></a>15.5.1.&nbsp;<span class="title" style="clear: both">单元测试</span></h3>
</div></div></div>
  <p>HBase unit tests are subdivided into three categories: small, medium and large, with
  corresponding JUnit <a class="link" href="http://www.junit.org/node/581" target="_top">categories</a>:
  <code class="classname">SmallTests</code>, <code class="classname">MediumTests</code>,
  <code class="classname">LargeTests</code>.  JUnit categories are denoted using java annotations
  and look like this in your unit test code.</p>
<pre class="programlisting">...
@Category(SmallTests.class)
public class TestHRegionInfo {

  @Test
  public void testCreateHRegionInfoName() throws Exception {
    // ...
  }
...
  @org.junit.Rule
  public org.apache.hadoop.hbase.ResourceCheckerJUnitRule cu =
    new org.apache.hadoop.hbase.ResourceCheckerJUnitRule();
}</pre><p>
The above example shows how to mark a test as belonging to the small category.  The <code class="code">@org.junit.Rule</code>
lines on the end are also necessary. Add them to each new unit test file.  They are needed by the categorization process.
HBase uses a patched maven surefire plugin and maven profiles to implement its unit test characterizations.
</p><div class="section" title="15.5.1.1.&nbsp;"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.small"></a>15.5.1.1.&nbsp;<a class="indexterm" name="d1934e9786"></a></h4></div></div></div><p>
<span class="emphasis"><em>Small</em></span> tests are executed in a shared JVM. We put in this category all the tests that can
be executed quickly in a shared JVM.  The maximum execution time for a small test is 15 seconds,
and small tests should not use a (mini)cluster.</p></div><div class="section" title="15.5.1.2.&nbsp;"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.medium"></a>15.5.1.2.&nbsp;<a class="indexterm" name="d1934e9796"></a></h4></div></div></div><p><span class="emphasis"><em>Medium</em></span> tests represent tests that must be executed
before proposing a patch. They are designed to run in less than 30 minutes altogether,
and are quite stable in their results. They are designed to last less than 50 seconds
individually. They can use a cluster, and each of them is executed in a separate JVM.
</p></div><div class="section" title="15.5.1.3.&nbsp;"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.large"></a>15.5.1.3.&nbsp;<a class="indexterm" name="d1934e9805"></a></h4></div></div></div><p><span class="emphasis"><em>Large</em></span> tests are everything else. They are typically integration-like
tests (yes, some large tests should be moved out to be HBase <a class="xref" href="hbase.tests.html#integration.tests" title="15.5.2.&nbsp;Integration Tests">Section&nbsp;15.5.2, “Integration Tests”</a>),
regression tests for specific bugs, timeout tests, performance tests.
They are executed before a commit on the pre-integration machines. They can be run on
the developer machine as well.
</p></div><div class="section" title="15.5.1.4.&nbsp;Running tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.unittests.cmds"></a>15.5.1.4.&nbsp;Running tests</h4></div></div></div><p>Below we describe how to run the HBase junit categories.</p><div class="section" title="15.5.1.4.1.&nbsp;Default: small and medium category tests"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.unittests.cmds.test"></a>15.5.1.4.1.&nbsp;Default: small and medium category tests
</h5></div></div></div><p>Running </p><pre class="programlisting">mvn test</pre><p> will execute all small tests in a single JVM
(no fork) and then medium tests in a separate JVM for each test instance.
Medium tests are NOT executed if there is an error in a small test.
Large tests are NOT executed.  There is one report for small tests, and one report for
medium tests if they are executed. To run small and medium tests with the security
profile enabled, do </p><pre class="programlisting">mvn test -P security</pre><p>
</p></div><div class="section" title="15.5.1.4.2.&nbsp;Running all tests"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.unittests.cmds.test.runAllTests"></a>15.5.1.4.2.&nbsp;Running all tests</h5></div></div></div><p>Running </p><pre class="programlisting">mvn test -P runAllTests</pre><p>
will execute small tests in a single JVM then medium and large tests in a separate JVM for each test.
Medium and large tests are NOT executed if there is an error in a small test.
Large tests are NOT executed if there is an error in a small or medium test.
There is one report for small tests, and one report for medium and large tests if they are executed
</p></div><div class="section" title="15.5.1.4.3.&nbsp;Running a single test or all tests in a package"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.unittests.cmds.test.localtests.mytest"></a>15.5.1.4.3.&nbsp;Running a single test or all tests in a package</h5></div></div></div><p>To run an individual test, e.g. <code class="classname">MyTest</code>, do
</p><pre class="programlisting">mvn test -P localTests -Dtest=MyTest</pre><p>  You can also
pass multiple, individual tests as a comma-delimited list:
</p><pre class="programlisting">mvn test -P localTests -Dtest=MyTest1,MyTest2,MyTest3</pre><p>
You can also pass a package, which will run all tests under the package:
</p><pre class="programlisting">mvn test -P localTests -Dtest=org.apache.hadoop.hbase.client.*</pre><p>
To run a single test with the security profile enabled:
</p><pre class="programlisting">mvn test -P security,localTests -Dtest=TestGet</pre><p>
</p><p>
The <code class="code">-P localTests</code>  will remove the JUnit category effect (without this specific profile,
the categories are taken into account).  It will actually use the official release of surefire
and the old connector (The HBase build uses a patched version of the maven surefire plugin).
Each junit tests is executed in a separate JVM (A fork per test class).  There is no
parallelization when <code class="code">localTests</code> profile is set.  You will see a new message at the end of the
report: "[INFO] Tests are skipped". It's harmless.
</p></div><div class="section" title="15.5.1.4.4.&nbsp;Other test invocation permutations"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.unittests.cmds.test.profiles"></a>15.5.1.4.4.&nbsp;Other test invocation permutations</h5></div></div></div><p>Running </p><pre class="programlisting">mvn test -P runSmallTests</pre><p> will execute small tests only, in a single JVM.
</p><p>Running </p><pre class="programlisting">mvn test -P runMediumTests</pre><p> will execute medium tests in a single JVM.
</p><p>Running </p><pre class="programlisting">mvn test -P runLargeTests</pre><p> execute medium tests in a single JVM.
</p></div><div class="section" title="15.5.1.4.5.&nbsp;hbasetests.sh"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.unittests.cmds.test.hbasetests"></a>15.5.1.4.5.&nbsp;<span class="command"><strong>hbasetests.sh</strong></span></h5></div></div></div><p>It's also possible to use the script <span class="command"><strong>hbasetests.sh</strong></span>. This script runs the medium and
large tests in parallel with two maven instances, and provides a single report.  This script does not use
the hbase version of surefire so no parallelization is being done other than the two maven instances the
script sets up.
It must be executed from the directory which contains the <code class="filename">pom.xml</code>.</p><p>For example running
</p><pre class="programlisting">./dev-support/hbasetests.sh</pre><p> will execute small and medium tests.
Running </p><pre class="programlisting">./dev-support/hbasetests.sh runAllTests</pre><p> will execute all tests.
Running </p><pre class="programlisting">./dev-support/hbasetests.sh replayFailed</pre><p> will rerun the failed tests a
second time, in a separate jvm and without parallelisation.
</p></div></div><div class="section" title="15.5.1.5.&nbsp;Writing Tests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.tests.writing"></a>15.5.1.5.&nbsp;Writing Tests</h4></div></div></div><div class="section" title="15.5.1.5.1.&nbsp;General rules"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.tests.rules"></a>15.5.1.5.1.&nbsp;General rules</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
As much as possible, tests should be written as category small tests.
</li><li class="listitem">
All tests must be written to support parallel execution on the same machine, hence they should not use shared resources as fixed ports or fixed file names.
</li><li class="listitem">
Tests should not overlog. More than 100 lines/second makes the logs complex to read and use i/o that are hence not available for the other tests.
</li><li class="listitem">
Tests can be written with <code class="classname">HBaseTestingUtility</code>.
This class offers helper functions to create a temp directory and do the cleanup, or to start a cluster.
Categories and execution time
</li><li class="listitem">
All tests must be categorized, if not they could be skipped.
</li><li class="listitem">
All tests should be written to be as fast as possible.
</li><li class="listitem">
Small category tests should last less than 15 seconds, and must not have any side effect.
</li><li class="listitem">
Medium category tests should last less than 50 seconds.
</li><li class="listitem">
Large category tests should last less than 3 minutes.  This should ensure a good parallelization for people using it, and ease the analysis when the test fails.
</li></ul></div></div><div class="section" title="15.5.1.5.2.&nbsp;Sleeps in tests"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.tests.sleeps"></a>15.5.1.5.2.&nbsp;Sleeps in tests</h5></div></div></div><p>Whenever possible, tests should not use <code class="methodname">Thread.sleep</code>, but rather waiting for the real event they need. This is faster and clearer for the reader.
Tests should not do a <code class="methodname">Thread.sleep</code> without testing an ending condition. This allows understanding what the test is waiting for. Moreover, the test will work whatever the machine performance is.
Sleep should be minimal to be as fast as possible. Waiting for a variable should be done in a 40ms sleep loop. Waiting for a socket operation should be done in a 200 ms sleep loop.
</p></div><div class="section" title="15.5.1.5.3.&nbsp;Tests using a cluster"><div class="titlepage"><div><div><h5 class="title"><a name="hbase.tests.cluster"></a>15.5.1.5.3.&nbsp;Tests using a cluster
</h5></div></div></div><p>Tests using a HRegion do not have to start a cluster: A region can use the local file system.
Start/stopping a cluster cost around 10 seconds. They should not be started per test method but per test class.
Started cluster must be shutdown using <code class="methodname">HBaseTestingUtility#shutdownMiniCluster</code>, which cleans the directories.
As most as possible, tests should use the default settings for the cluster. When they don't, they should document it. This will allow to share the cluster later.
</p></div></div></div><div class="section" title="15.5.2.&nbsp;Integration Tests"><div class="titlepage"><div><div><h3 class="title"><a name="integration.tests"></a>15.5.2.&nbsp;Integration Tests</h3></div></div></div><p>HBase integration Tests are tests that are beyond HBase unit tests.  They
are generally long-lasting, sizeable (the test can be asked to 1M rows or 1B rows),
targetable (they can take configuration that will point them at the ready-made cluster
they are to run against; integration tests do not include cluster start/stop code),
and verifying success, integration tests rely on public APIs only; they do not
attempt to examine server internals asserring success/fail. Integration tests
are what you would run when you need to more elaborate proofing of a release candidate
beyond what unit tests can do. They are not generally run on the Apache Continuous Integration
build server.
</p><p>
Integration tests currently live under the <code class="filename">src/test</code> directory and
will match the regex: <code class="filename">**/IntegrationTest*.java</code>. 
</p><p>HBase 0.92 added a <code class="varname">verify</code> maven target.
Invoking it, for example by doing <code class="code">mvn verify</code>, will
run all the phases up to and including the verify phase via the
maven <a class="link" href="http://maven.apache.org/plugins/maven-failsafe-plugin/" target="_top">failsafe plugin</a>,
running all the above mentioned HBase unit tests as well as tests that are in the HBase integration test group.
If you just want to run the integration tests, you need to run two commands. First:
          </p><pre class="programlisting">mvn failsafe:integration-test</pre><p>
This actually runs ALL the integration tests.
          </p><div class="note" title="Note" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Note</h3><p>This command will always output <code class="code">BUILD SUCCESS</code> even if there are test failures.
          </p></div><p>
          At this point, you could grep the output by hand looking for failed tests. However, maven will do this for us; just use:
          </p><pre class="programlisting">mvn failsafe:verify</pre><p>
          The above command basically looks at all the test results (so don't remove the 'target' directory) for test failures and reports the results.</p><div class="section" title="15.5.2.1.&nbsp;Running a subset of Integration tests"><div class="titlepage"><div><div><h4 class="title"><a name="maven.build.commanas.integration.tests2"></a>15.5.2.1.&nbsp;Running a subset of Integration tests</h4></div></div></div><p>This is very similar to how you specify running a subset of unit tests (see above).
To just run <code class="classname">IntegrationTestClassXYZ.java</code>, use:
          </p><pre class="programlisting">mvn failsafe:integration-test -Dtest=IntegrationTestClassXYZ</pre><p> 
          Pretty similar, right?
          The next thing you might want to do is run groups of integration tests, say all integration tests that are named IntegrationTestClassX*.java:
          </p><pre class="programlisting">mvn failsafe:integration-test -Dtest=*ClassX*</pre><p> 
          This runs everything that is an integration test that matches *ClassX*. This means anything matching: "**/IntegrationTest*ClassX*".
          You can also run multiple groups of integration tests using comma-delimited lists (similar to unit tests). Using a list of matches still supports full regex matching for each of the groups.This would look something like:
          </p><pre class="programlisting">mvn failsafe:integration-test -Dtest=*ClassX*, *ClassY</pre><p>  
          </p></div></div></div>
          <div class="section" title="15.6.&nbsp;Maven Build Commands"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="maven.build.commands"></a>15.6.&nbsp;Maven Build Commands</h2></div></div></div><p>All commands executed from the local HBase project directory.
       </p><p>Note: use Maven 3 (Maven 2 may work but we suggest you use Maven 3).
       </p><div class="section" title="15.6.1.&nbsp;Compile"><div class="titlepage"><div><div><h3 class="title"><a name="maven.build.commands.compile"></a>15.6.1.&nbsp;Compile</h3></div></div></div><pre class="programlisting">mvn compile
          </pre></div><div class="section" title="15.6.2.&nbsp;Running all or individual Unit Tests"><div class="titlepage"><div><div><h3 class="title"><a name="maven.build.commands.unitall"></a>15.6.2.&nbsp;Running all or individual Unit Tests</h3></div></div></div><p>See the <a class="xref" href="hbase.tests.html#hbase.unittests.cmds" title="15.5.1.4.&nbsp;Running tests">Section&nbsp;15.5.1.4, “Running tests”</a> section
          above in <a class="xref" href="hbase.tests.html#hbase.unittests" title="15.5.1.&nbsp;Unit Tests">Section&nbsp;15.5.1, “Unit Tests”</a></p></div><div class="section" title="15.6.3.&nbsp;Running all or individual Integration Tests"><div class="titlepage"><div><div><h3 class="title"><a name="maven.build.commanas.integration.tests"></a>15.6.3.&nbsp;Running all or individual Integration Tests</h3></div></div></div><p>See <a class="xref" href="hbase.tests.html#integration.tests" title="15.5.2.&nbsp;Integration Tests">Section&nbsp;15.5.2, “Integration Tests”</a>
          </p></div><div class="section" title="15.6.4.&nbsp;To build against hadoop 0.22.x or 0.23.x"><div class="titlepage"><div><div><h3 class="title"><a name="maven.build.hadoop"></a>15.6.4.&nbsp;To build against hadoop 0.22.x or 0.23.x</h3></div></div></div><pre class="programlisting">mvn -Dhadoop.profile=22 ...
          </pre><p>That is, designate build with hadoop.profile 22.  Pass 23 for hadoop.profile to build against hadoop 0.23.
Tests do not all pass as of this writing so you may need ot pass <code class="code">-DskipTests</code> unless you are inclined
to fix the failing tests.
</p></div></div>
<div class="section" title="15.7.&nbsp;Getting Involved"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="getting.involved"></a>15.7.&nbsp;Getting Involved</h2></div></div></div><p>HBase gets better only when people contribute!
        </p><p>As HBase is an Apache Software Foundation project, see <a class="xref" href="asf.html" title="Appendix&nbsp;H.&nbsp;HBase and the Apache Software Foundation">Appendix&nbsp;H, <i>HBase and the Apache Software Foundation</i></a> for more information about how the ASF functions.
        </p><div class="section" title="15.7.1.&nbsp;Mailing Lists"><div class="titlepage"><div><div><h3 class="title"><a name="mailing.list"></a>15.7.1.&nbsp;Mailing Lists</h3></div></div></div><p>Sign up for the dev-list and the user-list.  See the 
          <a class="link" href="http://hbase.apache.org/mail-lists.html" target="_top">mailing lists</a> page.
          Posing questions - and helping to answer other people's questions - is encouraged!  
          There are varying levels of experience on both lists so patience and politeness are encouraged (and please 
          stay on topic.)  
          </p></div><div class="section" title="15.7.2.&nbsp;Jira"><div class="titlepage"><div><div><h3 class="title"><a name="jira"></a>15.7.2.&nbsp;Jira</h3></div></div></div><p>Check for existing issues in <a class="link" href="https://issues.apache.org/jira/browse/HBASE" target="_top">Jira</a>.  
          If it's either a new feature request, enhancement, or a bug, file a ticket.
          </p><div class="section" title="15.7.2.1.&nbsp;Jira Priorities"><div class="titlepage"><div><div><h4 class="title"><a name="jira.priorities"></a>15.7.2.1.&nbsp;Jira Priorities</h4></div></div></div><p>The following is a guideline on setting Jira issue priorities:
                </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">Blocker: Should only be used if the issue WILL cause data loss or cluster instability reliably.</li><li class="listitem">Critical: The issue described can cause data loss or cluster instability in some cases.</li><li class="listitem">Major: Important but not tragic issues, like updates to the client API that will add a lot of much-needed functionality or significant
                bugs that need to be fixed but that don't cause data loss.</li><li class="listitem">Minor: Useful enhancements and annoying but not damaging bugs.</li><li class="listitem">Trivial: Useful enhancements but generally cosmetic.</li></ul></div><p>  
             </p></div><div class="section" title="15.7.2.2.&nbsp;Code Blocks in Jira Comments"><div class="titlepage"><div><div><h4 class="title"><a name="submitting.patches.jira.code"></a>15.7.2.2.&nbsp;Code Blocks in Jira Comments</h4></div></div></div><p>A commonly used macro in Jira is {code}. If you do this in a Jira comment...
</p><pre class="programlisting">{code}
   code snippet
{code}
</pre><p>
              ... Jira will format the code snippet like code, instead of a regular comment.  It improves readability.
          </p></div></div></div>
          
          <div class="section" title="15.8.&nbsp;Developing"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="developing"></a>15.8.&nbsp;Developing</h2></div></div></div><div class="section" title="15.8.1.&nbsp;Codelines"><div class="titlepage"><div><div><h3 class="title"><a name="codelines"></a>15.8.1.&nbsp;Codelines</h3></div></div></div><p>Most development is done on TRUNK.  However, there are branches for minor releases (e.g., 0.90.1, 0.90.2, and 0.90.3 are on the 0.90 branch).</p><p>If you have any questions on this just send an email to the dev dist-list.</p></div><div class="section" title="15.8.2.&nbsp;Unit Tests"><div class="titlepage"><div><div><h3 class="title"><a name="unit.tests"></a>15.8.2.&nbsp;Unit Tests</h3></div></div></div><p>In HBase we use <a class="link" href="http://junit.org" target="_top">JUnit</a> 4.
            If you need to run miniclusters of HDFS, ZooKeeper, HBase, or MapReduce testing,
            be sure to checkout the <code class="classname">HBaseTestingUtility</code>.
            Alex Baranau of Sematext describes how it can be used in
            <a class="link" href="http://blog.sematext.com/2010/08/30/hbase-case-study-using-hbasetestingutility-for-local-testing-development/" target="_top">HBase Case-Study: Using HBaseTestingUtility for Local Testing and Development</a> (2010).
          </p><div class="section" title="15.8.2.1.&nbsp;Mockito"><div class="titlepage"><div><div><h4 class="title"><a name="mockito"></a>15.8.2.1.&nbsp;Mockito</h4></div></div></div><p>Sometimes you don't need a full running server
              unit testing.  For example, some methods can make do with a
              a <code class="classname">org.apache.hadoop.hbase.Server</code> instance
              or a <code class="classname">org.apache.hadoop.hbase.master.MasterServices</code>
              Interface reference rather than a full-blown
              <code class="classname">org.apache.hadoop.hbase.master.HMaster</code>.
              In these cases, you maybe able to get away with a mocked
              <code class="classname">Server</code> instance.  For example:
              </p><pre class="programlisting">              TODO...
              </pre><p>
           </p></div></div><div class="section" title="15.8.3.&nbsp;Code Standards"><div class="titlepage"><div><div><h3 class="title"><a name="code.standards"></a>15.8.3.&nbsp;Code Standards</h3></div></div></div><p>See <a class="xref" href="ides.html#eclipse.code.formatting" title="15.2.1.1.&nbsp;Code Formatting">Section&nbsp;15.2.1.1, “Code Formatting”</a> and <a class="xref" href="submitting.patches.html#common.patch.feedback" title="15.9.5.&nbsp;Common Patch Feedback">Section&nbsp;15.9.5, “Common Patch Feedback”</a>.
           </p><p>Also, please pay attention to the interface stability/audience classifications that you
           will see all over our code base.   They look like this at the head of the class:
           </p><pre class="programlisting">@InterfaceAudience.Public
@InterfaceStability.Stable</pre><p>
           </p><p>If the <code class="classname">InterfaceAudience</code> is <code class="varname">Private</code>,
           we can change the class (and we do not need to include a <code class="classname">InterfaceStability</code> mark).
           If a class is marked <code class="varname">Public</code> but its <code class="classname">InterfaceStability</code>
           is marked <code class="varname">Unstable</code>, we can change it. If it's 
           marked <code class="varname">Public</code>/<code class="varname">Evolving</code>, we're allowed to change it
           but should try not to. If it's <code class="varname">Public</code> and <code class="varname">Stable</code>
           we can't change it without a deprecation path or with a really GREAT reason.</p><p>When you add new classes, mark them with the annotations above if publically accessible.
           If you are not cleared on how to mark your additions, ask up on the dev list.
           </p><p>This convention comes from our parent project Hadoop.</p></div></div>
           
           <div class="section" title="15.9.&nbsp;Submitting Patches"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="submitting.patches"></a>15.9.&nbsp;Submitting Patches</h2></div></div></div><div class="section" title="15.9.1.&nbsp;Create Patch"><div class="titlepage"><div><div><h3 class="title"><a name="submitting.patches.create"></a>15.9.1.&nbsp;Create Patch</h3></div></div></div><p>Patch files can be easily generated from Eclipse, for example by selecting "Team -&gt; Create Patch".
          Patches can also be created by git diff and svn diff.
          </p><p>Please submit one patch-file per Jira.  For example, if multiple files are changed make sure the 
          selected resource when generating the patch is a directory.  Patch files can reflect changes in multiple files. </p><p>Make sure you review <a class="xref" href="ides.html#eclipse.code.formatting" title="15.2.1.1.&nbsp;Code Formatting">Section&nbsp;15.2.1.1, “Code Formatting”</a> for code style. </p></div><div class="section" title="15.9.2.&nbsp;Patch File Naming"><div class="titlepage"><div><div><h3 class="title"><a name="submitting.patches.naming"></a>15.9.2.&nbsp;Patch File Naming</h3></div></div></div><p>The patch file should have the HBase Jira ticket in the name.  For example, if a patch was submitted for <code class="filename">Foo.java</code>, then
          a patch file called <code class="filename">Foo_HBASE_XXXX.patch</code> would be acceptable where XXXX is the HBase Jira number.
          </p><p>If you generating from a branch, then including the target branch in the filename is advised, e.g., <code class="filename">HBASE-XXXX-0.90.patch</code>.
          </p></div><div class="section" title="15.9.3.&nbsp;Unit Tests"><div class="titlepage"><div><div><h3 class="title"><a name="submitting.patches.tests"></a>15.9.3.&nbsp;Unit Tests</h3></div></div></div><p>Yes, please.  Please try to include unit tests with every code patch (and especially new classes and large changes).
            Make sure unit tests pass locally before submitting the patch.</p><p>Also, see <a class="xref" href="developing.html#mockito" title="15.8.2.1.&nbsp;Mockito">Section&nbsp;15.8.2.1, “Mockito”</a>.</p><p>If you are creating a new unit test class, notice how other unit test classes have classification/sizing
            annotations at the top and a static method on the end.  Be sure to include these in any new unit test files
            you generate.  See <a class="xref" href="hbase.tests.html" title="15.5.&nbsp;Tests">Section&nbsp;15.5, “Tests”</a> for more on how the annotations work. 
            </p></div><div class="section" title="15.9.4.&nbsp;Attach Patch to Jira"><div class="titlepage"><div><div><h3 class="title"><a name="submitting.patches.jira"></a>15.9.4.&nbsp;Attach Patch to Jira</h3></div></div></div><p>The patch should be attached to the associated Jira ticket "More Actions -&gt; Attach Files".  Make sure you click the
            ASF license inclusion, otherwise the patch can't be considered for inclusion.
            </p><p>Once attached to the ticket, click "Submit Patch" and 
            the status of the ticket will change.  Committers will review submitted patches for inclusion into the codebase.  Please
            understand that not every patch may get committed, and that feedback will likely be provided on the patch.  Fear not, though,
            because the HBase community is helpful!
            </p></div><div class="section" title="15.9.5.&nbsp;Common Patch Feedback"><div class="titlepage"><div><div><h3 class="title"><a name="common.patch.feedback"></a>15.9.5.&nbsp;Common Patch Feedback</h3></div></div></div><p>The following items are representative of common patch feedback. Your patch process will go faster if these are
          taken into account <span class="emphasis"><em>before</em></span> submission.
          </p><p>
          See the <a class="link" href="http://www.oracle.com/technetwork/java/codeconv-138413.html" target="_top">Java coding standards</a> 
          for more information on coding conventions in Java.
          </p><div class="section" title="15.9.5.1.&nbsp;Space Invaders"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.space.invaders"></a>15.9.5.1.&nbsp;Space Invaders</h4></div></div></div><p>Rather than do this...
</p><pre class="programlisting">if ( foo.equals( bar ) ) {     // don't do this
</pre><p>
			... do this instead...        
</p><pre class="programlisting">if (foo.equals(bar)) {
</pre><p>
          </p><p>Also, rather than do this...
</p><pre class="programlisting">foo = barArray[ i ];     // don't do this
</pre><p>
			... do this instead...        
</p><pre class="programlisting">foo = barArray[i];   
</pre><p>
          </p></div><div class="section" title="15.9.5.2.&nbsp;Auto Generated Code"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.autogen"></a>15.9.5.2.&nbsp;Auto Generated Code</h4></div></div></div><p>Auto-generated code in Eclipse often looks like this...
</p><pre class="programlisting"> public void readFields(DataInput arg0) throws IOException {    // don't do this
   foo = arg0.readUTF();                                       // don't do this
</pre><p>
			... do this instead ...        
</p><pre class="programlisting"> public void readFields(DataInput di) throws IOException {
   foo = di.readUTF();
</pre><p>
           See the difference?  'arg0' is what Eclipse uses for arguments by default.
           </p></div><div class="section" title="15.9.5.3.&nbsp;Long Lines"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.longlines"></a>15.9.5.3.&nbsp;Long Lines</h4></div></div></div><p>
            Keep lines less than 100 characters.
</p><pre class="programlisting">Bar bar = foo.veryLongMethodWithManyArguments(argument1, argument2, argument3, argument4, argument5, argument6, argument7, argument8, argument9);  // don't do this
</pre><p>
			... do something like this instead ...        
</p><pre class="programlisting">Bar bar = foo.veryLongMethodWithManyArguments(
 argument1, argument2, argument3,argument4, argument5, argument6, argument7, argument8, argument9); 
</pre><p>
           </p></div><div class="section" title="15.9.5.4.&nbsp;Trailing Spaces"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.trailingspaces"></a>15.9.5.4.&nbsp;Trailing Spaces</h4></div></div></div><p>
            This happens more than people would imagine.
</p><pre class="programlisting">Bar bar = foo.getBar();     &lt;--- imagine there's an extra space(s) after the semicolon instead of a line break.
</pre><p>
            Make sure there's a line-break after the end of your code, and also avoid lines that have nothing
            but whitespace. 
            </p></div><div class="section" title="15.9.5.5.&nbsp;Implementing Writable"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.writable"></a>15.9.5.5.&nbsp;Implementing Writable</h4></div></div></div><p>Every class returned by RegionServers must implement <code class="code">Writable</code>.  If you
            are creating a new class that needs to implement this interface, don't forget the default constructor.
            </p></div><div class="section" title="15.9.5.6.&nbsp;Javadoc"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.javadoc"></a>15.9.5.6.&nbsp;Javadoc</h4></div></div></div><p>This is also a very common feedback item.  Don't forget Javadoc!
            </p></div><div class="section" title="15.9.5.7.&nbsp;Javadoc - Useless Defaults"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.javadoc.defaults"></a>15.9.5.7.&nbsp;Javadoc - Useless Defaults</h4></div></div></div><p>Don't just leave the @param arguments the way your IDE generated them.  Don't do this...
</p><pre class="programlisting">  /**
   * 
   * @param bar             &lt;---- don't do this!!!!
   * @return                &lt;---- or this!!!!
   */
  public Foo getFoo(Bar bar);
</pre><p> 
            ... either add something descriptive to the @param and @return lines, or just remove them. 
            But the preference is to add something descriptive and useful.          
            </p></div><div class="section" title="15.9.5.8.&nbsp;One Thing At A Time, Folks"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.onething"></a>15.9.5.8.&nbsp;One Thing At A Time, Folks</h4></div></div></div><p>If you submit a patch for one thing, don't do auto-reformatting or unrelated reformatting of code on a completely
            different area of code. 
            </p><p>Likewise, don't add unrelated cleanup or refactorings outside the scope of your Jira. 
            </p></div><div class="section" title="15.9.5.9.&nbsp;Ambigious Unit Tests"><div class="titlepage"><div><div><h4 class="title"><a name="common.patch.feedback.tests"></a>15.9.5.9.&nbsp;Ambigious Unit Tests</h4></div></div></div><p>Make sure that you're clear about what you are testing in your unit tests and why. 
            </p></div></div><div class="section" title="15.9.6.&nbsp;ReviewBoard"><div class="titlepage"><div><div><h3 class="title"><a name="reviewboard"></a>15.9.6.&nbsp;ReviewBoard</h3></div></div></div><p>Larger patches should go through <a class="link" href="http://reviews.apache.org" target="_top">ReviewBoard</a>.
          </p><p>For more information on how to use ReviewBoard, see
           <a class="link" href="http://www.reviewboard.org/docs/manual/1.5/" target="_top">the ReviewBoard documentation</a>.
          </p></div><div class="section" title="15.9.7.&nbsp;Committing Patches"><div class="titlepage"><div><div><h3 class="title"><a name="committing.patches"></a>15.9.7.&nbsp;Committing Patches</h3></div></div></div><p>
          Committers do this.  See <a class="link" href="http://wiki.apache.org/hadoop/Hbase/HowToCommit" target="_top">How To Commit</a> in the HBase wiki.
          </p><p>Commiters will also resolve the Jira, typically after the patch passes a build.
          </p></div></div>
          
                     <div class="appendix" title="Appendix&nbsp;A.&nbsp;FAQ"><div class="titlepage"><div><div><h2 class="title"><a name="faq"></a>Appendix&nbsp;A.&nbsp;FAQ</h2></div></div></div><div class="qandaset" title="Frequently Asked Questions"><a name="d1934e10368"></a><dl><dt>A.1.  <a href="book.htm#d1934e10369">General</a></dt><dd><dl><dt> <a href="book.htm#d1934e10372">When should I use HBase?</a></dt><dt> <a href="book.htm#d1934e10381">Are there other HBase FAQs?</a></dt><dt> <a href="book.htm#faq.sql">Does HBase support SQL?</a></dt><dt> <a href="book.htm#d1934e10403">How can I find examples of NoSQL/HBase?</a></dt><dt> <a href="book.htm#d1934e10412">What is the history of HBase?</a></dt></dl></dd><dt>A.2.  <a href="book.htm#faq.arch">Architecture</a></dt><dd><dl><dt> <a href="book.htm#faq.arch.regions">How does HBase handle Region-RegionServer assignment and locality?</a></dt></dl></dd><dt>A.3.  <a href="book.htm#faq.config">Configuration</a></dt><dd><dl><dt> <a href="book.htm#faq.config.started">How can I get started with my first cluster?</a></dt><dt> <a href="book.htm#faq.config.started">Where can I learn about the rest of the configuration options?</a></dt></dl></dd><dt>A.4.  <a href="book.htm#faq.design">Schema Design / Data Access</a></dt><dd><dl><dt> <a href="book.htm#faq.design.schema">How should I design my schema in HBase?</a></dt><dt> <a href="book.htm#d1934e10468">
                    How can I store (fill in the blank) in HBase?
            </a></dt><dt> <a href="book.htm#secondary.indices">
                    How can I handle secondary indexes in HBase?
            </a></dt><dt> <a href="book.htm#faq.changing.rowkeys">Can I change a table's rowkeys?</a></dt><dt> <a href="book.htm#faq.apis">What APIs does HBase support?</a></dt></dl></dd><dt>A.5.  <a href="book.htm#faq.mapreduce">MapReduce</a></dt><dd><dl><dt> <a href="book.htm#faq.mapreduce.use">How can I use MapReduce with HBase?</a></dt></dl></dd><dt>A.6.  <a href="book.htm#d1934e10520">Performance and Troubleshooting</a></dt><dd><dl><dt> <a href="book.htm#d1934e10523">
                   How can I improve HBase cluster performance?
            </a></dt><dt> <a href="book.htm#d1934e10532">
                    How can I troubleshoot my HBase cluster?
            </a></dt></dl></dd><dt>A.7.  <a href="book.htm#ec2">Amazon EC2</a></dt><dd><dl><dt> <a href="book.htm#d1934e10544">
            I am running HBase on Amazon EC2 and...
            </a></dt></dl></dd><dt>A.8.  <a href="book.htm#d1934e10555">Operations</a></dt><dd><dl><dt> <a href="book.htm#d1934e10558">
                    How do I manage my HBase cluster?
            </a></dt><dt> <a href="book.htm#d1934e10567">
                    How do I back up my HBase cluster?
            </a></dt></dl></dd><dt>A.9.  <a href="book.htm#d1934e10576">HBase in Action</a></dt><dd><dl><dt> <a href="book.htm#d1934e10579">Where can I find interesting videos and presentations on HBase?</a></dt></dl></dd></dl><table border="0" width="100%" summary="Q and A Set"><colgroup><col align="left" width="1%"><col></colgroup><tbody><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="d1934e10369"></a>A.1. General</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#d1934e10372">When should I use HBase?</a></dt><dt> <a href="book.htm#d1934e10381">Are there other HBase FAQs?</a></dt><dt> <a href="book.htm#faq.sql">Does HBase support SQL?</a></dt><dt> <a href="book.htm#d1934e10403">How can I find examples of NoSQL/HBase?</a></dt><dt> <a href="book.htm#d1934e10412">What is the history of HBase?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10372"></a><a name="d1934e10373"></a></td><td align="left" valign="top"><p>When should I use HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>See the <a class="xref" href="architecture.html#arch.overview" title="9.1.&nbsp;Overview">Section&nbsp;9.1, “Overview”</a> in the Architecture chapter.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10381"></a><a name="d1934e10382"></a></td><td align="left" valign="top"><p>Are there other HBase FAQs?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
              See the FAQ that is up on the wiki, <a class="link" href="http://wiki.apache.org/hadoop/Hbase/FAQ" target="_top">HBase Wiki FAQ</a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.sql"></a><a name="d1934e10392"></a></td><td align="left" valign="top"><p>Does HBase support SQL?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    Not really.  SQL-ish support for HBase via <a class="link" href="http://hive.apache.org/" target="_top">Hive</a> is in development, however Hive is based on MapReduce which is not generally suitable for low-latency requests.
                    See the <a class="xref" href="datamodel.html" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a> section for examples on the HBase client.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10403"></a><a name="d1934e10404"></a></td><td align="left" valign="top"><p>How can I find examples of NoSQL/HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>See the link to the BigTable paper in <a class="xref" href="other.info.html" title="Appendix&nbsp;F.&nbsp;Other Information About HBase">Appendix&nbsp;F, <i>Other Information About HBase</i></a> in the appendix, as
                well as the other papers.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10412"></a><a name="d1934e10413"></a></td><td align="left" valign="top"><p>What is the history of HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>See <a class="xref" href="hbase.history.html" title="Appendix&nbsp;G.&nbsp;HBase History">Appendix&nbsp;G, <i>HBase History</i></a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="faq.arch"></a>A.2. Architecture</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#faq.arch.regions">How does HBase handle Region-RegionServer assignment and locality?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.arch.regions"></a><a name="d1934e10425"></a></td><td align="left" valign="top"><p>How does HBase handle Region-RegionServer assignment and locality?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="regions.arch.html" title="9.7.&nbsp;Regions">Section&nbsp;9.7, “Regions”</a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="faq.config"></a>A.3. Configuration</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#faq.config.started">How can I get started with my first cluster?</a></dt><dt> <a href="book.htm#faq.config.started">Where can I learn about the rest of the configuration options?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.config.started"></a><a name="d1934e10437"></a></td><td align="left" valign="top"><p>How can I get started with my first cluster?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="quickstart.html" title="1.2.&nbsp;Quick Start">Section&nbsp;1.2, “Quick Start”</a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.config.started"></a><a name="d1934e10446"></a></td><td align="left" valign="top"><p>Where can I learn about the rest of the configuration options?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="configuration.html" title="Chapter&nbsp;2.&nbsp;Configuration">Chapter&nbsp;2, <i>Configuration</i></a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="faq.design"></a>A.4. Schema Design / Data Access</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#faq.design.schema">How should I design my schema in HBase?</a></dt><dt> <a href="book.htm#d1934e10468">
                    How can I store (fill in the blank) in HBase?
            </a></dt><dt> <a href="book.htm#secondary.indices">
                    How can I handle secondary indexes in HBase?
            </a></dt><dt> <a href="book.htm#faq.changing.rowkeys">Can I change a table's rowkeys?</a></dt><dt> <a href="book.htm#faq.apis">What APIs does HBase support?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.design.schema"></a><a name="d1934e10458"></a></td><td align="left" valign="top"><p>How should I design my schema in HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="datamodel.html" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a> and <a class="xref" href="schema.html" title="Chapter&nbsp;6.&nbsp;HBase and Schema Design">Chapter&nbsp;6, <i>HBase and Schema Design</i></a>
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10468"></a><a name="d1934e10469"></a></td><td align="left" valign="top"><p>
                    How can I store (fill in the blank) in HBase?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                See <a class="xref" href="supported.datatypes.html" title="6.5.&nbsp; Supported Datatypes">Section&nbsp;6.5, “
  Supported Datatypes
  ”</a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="secondary.indices"></a><a name="d1934e10478"></a></td><td align="left" valign="top"><p>
                    How can I handle secondary indexes in HBase?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                See <a class="xref" href="secondary.indexes.html" title="6.9.&nbsp; Secondary Indexes and Alternate Query Paths">Section&nbsp;6.9, “
  Secondary Indexes and Alternate Query Paths
  ”</a>
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.changing.rowkeys"></a><a name="d1934e10487"></a></td><td align="left" valign="top"><p>Can I change a table's rowkeys?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    This is a very common quesiton.  You can't.  See <a class="xref" href="rowkey.design.html#changing.rowkeys" title="6.3.5.&nbsp;Immutability of Rowkeys">Section&nbsp;6.3.5, “Immutability of Rowkeys”</a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.apis"></a><a name="d1934e10496"></a></td><td align="left" valign="top"><p>What APIs does HBase support?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="datamodel.html" title="Chapter&nbsp;5.&nbsp;Data Model">Chapter&nbsp;5, <i>Data Model</i></a>, <a class="xref" href="client.html" title="9.3.&nbsp;Client">Section&nbsp;9.3, “Client”</a> and <a class="xref" href="external_apis.html#nonjava.jvm" title="10.1.&nbsp;Non-Java Languages Talking to the JVM">Section&nbsp;10.1, “Non-Java Languages Talking to the JVM”</a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="faq.mapreduce"></a>A.5. MapReduce</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#faq.mapreduce.use">How can I use MapReduce with HBase?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="faq.mapreduce.use"></a><a name="d1934e10512"></a></td><td align="left" valign="top"><p>How can I use MapReduce with HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="mapreduce.html" title="Chapter&nbsp;7.&nbsp;HBase and MapReduce">Chapter&nbsp;7, <i>HBase and MapReduce</i></a>
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="d1934e10520"></a>A.6. Performance and Troubleshooting</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#d1934e10523">
                   How can I improve HBase cluster performance?
            </a></dt><dt> <a href="book.htm#d1934e10532">
                    How can I troubleshoot my HBase cluster?
            </a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10523"></a><a name="d1934e10524"></a></td><td align="left" valign="top"><p>
                   How can I improve HBase cluster performance?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                See <a class="xref" href="performance.html" title="Chapter&nbsp;11.&nbsp;Performance Tuning">Chapter&nbsp;11, <i>Performance Tuning</i></a>.
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10532"></a><a name="d1934e10533"></a></td><td align="left" valign="top"><p>
                    How can I troubleshoot my HBase cluster?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                See <a class="xref" href="trouble.html" title="Chapter&nbsp;12.&nbsp;Troubleshooting and Debugging HBase">Chapter&nbsp;12, <i>Troubleshooting and Debugging HBase</i></a>.
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="ec2"></a>A.7. Amazon EC2</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#d1934e10544">
            I am running HBase on Amazon EC2 and...
            </a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10544"></a><a name="d1934e10545"></a></td><td align="left" valign="top"><p>
            I am running HBase on Amazon EC2 and...
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
 	            EC2 issues are a special case.  See Troubleshooting <a class="xref" href="trouble.ec2.html" title="12.12.&nbsp;Amazon EC2">Section&nbsp;12.12, “Amazon EC2”</a> and Performance <a class="xref" href="perf.ec2.html" title="11.11.&nbsp;Amazon EC2">Section&nbsp;11.11, “Amazon EC2”</a> sections.                
               </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="d1934e10555"></a>A.8. Operations</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#d1934e10558">
                    How do I manage my HBase cluster?
            </a></dt><dt> <a href="book.htm#d1934e10567">
                    How do I back up my HBase cluster?
            </a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10558"></a><a name="d1934e10559"></a></td><td align="left" valign="top"><p>
                    How do I manage my HBase cluster?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="ops_mgt.html" title="Chapter&nbsp;14.&nbsp;HBase Operational Management">Chapter&nbsp;14, <i>HBase Operational Management</i></a>
                </p></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10567"></a><a name="d1934e10568"></a></td><td align="left" valign="top"><p>
                    How do I back up my HBase cluster?
            </p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="ops.backup.html" title="14.7.&nbsp;HBase Backup">Section&nbsp;14.7, “HBase Backup”</a>
                </p></td></tr><tr class="qandadiv"><td align="left" valign="top" colspan="2"><h3 class="title"><a name="d1934e10576"></a>A.9. HBase in Action</h3></td></tr><tr class="toc"><td align="left" valign="top" colspan="2"><dl><dt> <a href="book.htm#d1934e10579">Where can I find interesting videos and presentations on HBase?</a></dt></dl></td></tr><tr class="question"><td align="left" valign="top"><a name="d1934e10579"></a><a name="d1934e10580"></a></td><td align="left" valign="top"><p>Where can I find interesting videos and presentations on HBase?</p></td></tr><tr class="answer"><td align="left" valign="top"></td><td align="left" valign="top"><p>
                    See <a class="xref" href="other.info.html" title="Appendix&nbsp;F.&nbsp;Other Information About HBase">Appendix&nbsp;F, <i>Other Information About HBase</i></a>
                </p></td></tr></tbody></table></div></div>
                
            <div class="appendix" title="Appendix&nbsp;B.&nbsp;hbck In Depth"><div class="titlepage"><div><div><h2 class="title"><a name="hbck.in.depth"></a>Appendix&nbsp;B.&nbsp;hbck In Depth</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="book.htm#d1934e10593">B.1. Running hbck to identify inconsistencies</a></span></dt><dt><span class="section"><a href="book.htm#apbs02">B.2. Inconsistencies</a></span></dt><dt><span class="section"><a href="book.htm#apbs03">B.3. Localized repairs</a></span></dt><dt><span class="section"><a href="book.htm#apbs04">B.4. Region Overlap Repairs</a></span></dt><dd><dl><dt><span class="section"><a href="book.htm#d1934e10702">B.4.1. Special cases: Meta is not properly assigned</a></span></dt><dt><span class="section"><a href="book.htm#d1934e10711">B.4.2. Special cases: HBase version file is missing</a></span></dt><dt><span class="section"><a href="book.htm#d1934e10718">B.4.3. Special case: Root and META are corrupt.</a></span></dt></dl></dd></dl></div><p>HBaseFsck (hbck) is a tool for checking for region consistency and table integrity problems
and repairing a corrupted HBase. It works in two basic modes -- a read-only inconsistency
identifying mode and a multi-phase read-write repair mode.
	</p><div class="section" title="B.1.&nbsp;Running hbck to identify inconsistencies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d1934e10593"></a>B.1.&nbsp;Running hbck to identify inconsistencies</h2></div></div></div>
To check to see if your HBase cluster has corruptions, run hbck against your HBase cluster:
<pre class="programlisting">$ ./bin/hbase hbck
</pre><p>
At the end of the commands output it prints OK or tells you the number of INCONSISTENCIES
present. You may also want to run run hbck a few times because some inconsistencies can be
transient (e.g. cluster is starting up or a region is splitting). Operationally you may want to run
hbck regularly and setup alert (e.g. via nagios) if it repeatedly reports inconsistencies .
A run of hbck will report a list of inconsistencies along with a brief description of the regions and
tables affected. The using the <code class="code">-details</code> option will report more details including a representative
listing of all the splits present in all the tables.	
	</p><pre class="programlisting">$ ./bin/hbase hbck -details
</pre></div></div>    
                
                <div class="section" title="B.2.&nbsp;Inconsistencies"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="apbs02" id="apbs02"></a>B.2.&nbsp;Inconsistencies</h2></div></div></div><p>
	If after several runs, inconsistencies continue to be reported, you may have encountered a
corruption. These should be rare, but in the event they occur newer versions of HBase include
the hbck tool enabled with automatic repair options.
	</p><p>
	There are two invariants that when violated create inconsistencies in HBase:
	</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">HBase’s region consistency invariant is satisfied if every region is assigned and
deployed on exactly one region server, and all places where this state kept is in
accordance.
	</li><li class="listitem">HBase’s table integrity invariant is satisfied if for each table, every possible row key
resolves to exactly one region.
	</li></ul></div><p>
Repairs generally work in three phases -- a read-only information gathering phase that identifies
inconsistencies, a table integrity repair phase that restores the table integrity invariant, and then
finally a region consistency repair phase that restores the region consistency invariant.
Starting from version 0.90.0, hbck could detect region consistency problems report on a subset
of possible table integrity problems. It also included the ability to automatically fix the most
common inconsistency, region assignment and deployment consistency problems. This repair
could be done by using the <code class="code">-fix</code> command line option. These problems close regions if they are
open on the wrong server or on multiple region servers and also assigns regions to region
servers if they are not open.
</p><p>
Starting from HBase versions 0.90.7, 0.92.2 and 0.94.0, several new command line options are
introduced to aid repairing a corrupted HBase. This hbck sometimes goes by the nickname
“uberhbck”. Each particular version of uber hbck is compatible with the HBase’s of the same
major version (0.90.7 uberhbck can repair a 0.90.4). However, versions &lt;=0.90.6 and versions
&lt;=0.92.1 may require restarting the master or failing over to a backup master.
</p></div>
<div class="section" title="B.3.&nbsp;Localized repairs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="apbs03" id="apbs03"></a>B.3.&nbsp;Localized repairs</h2></div></div></div><p>
	When repairing a corrupted HBase, it is best to repair the lowest risk inconsistencies first.
These are generally region consistency repairs -- localized single region repairs, that only modify
in-memory data, ephemeral zookeeper data, or patch holes in the META table.
Region consistency requires that the HBase instance has the state of the region’s data in HDFS
(.regioninfo files), the region’s row in the .META. table., and region’s deployment/assignments on
region servers and the master in accordance. Options for repairing region consistency include:
	</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-fixAssignments</code> (equivalent to the 0.90 <code class="code">-fix</code> option) repairs unassigned, incorrectly
assigned or multiply assigned regions.
		</li><li class="listitem"><code class="code">-fixMeta</code> which removes meta rows when corresponding regions are not present in
HDFS and adds new meta rows if they regions are present in HDFS while not in META.
		</li></ul></div><p>
	To fix deployment and assignment problems you can run this command:
</p><pre class="programlisting">$ ./bin/hbase hbck -fixAssignments
</pre>
To fix deployment and assignment problems as well as repairing incorrect meta rows you can
run this command:.
<pre class="programlisting">$ ./bin/hbase hbck -fixAssignments -fixMeta
</pre>
There are a few classes of table integrity problems that are low risk repairs. The first two are
degenerate (startkey == endkey) regions and backwards regions (startkey &gt; endkey). These are
automatically handled by sidelining the data to a temporary directory (/hbck/xxxx).
The third low-risk class is hdfs region holes. This can be repaired by using the:
	<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-fixHdfsHoles</code> option for fabricating new empty regions on the file system.
If holes are detected you can use -fixHdfsHoles and should include -fixMeta and -fixAssignments to make the new region consistent.
		</li></ul></div><pre class="programlisting">$ ./bin/hbase hbck -fixAssignments -fixMeta -fixHdfsHoles
</pre>
Since this is a common operation, we’ve added a the <code class="code">-repairHoles</code> flag that is equivalent to the
previous command:
<pre class="programlisting">$ ./bin/hbase hbck -repairHoles
</pre>
If inconsistencies still remain after these steps, you most likely have table integrity problems
related to orphaned or overlapping regions.
	</div>
	<div class="section" title="B.4.&nbsp;Region Overlap Repairs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="apbs04" id="apbs04"></a>B.4.&nbsp;Region Overlap Repairs</h2></div></div></div>
Table integrity problems can require repairs that deal with overlaps. This is a riskier operation
because it requires modifications to the file system, requires some decision making, and may
require some manual steps. For these repairs it is best to analyze the output of a <code class="code">hbck -details</code>
run so that you isolate repairs attempts only upon problems the checks identify. Because this is
riskier, there are safeguard that should be used to limit the scope of the repairs.
WARNING: This is a relatively new and have only been tested on online but idle HBase instances
(no reads/writes). Use at your own risk in an active production environment!
The options for repairing table integrity violations include:
	<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-fixHdfsOrphans</code> option for “adopting” a region directory that is missing a region
metadata file (the .regioninfo file).
		</li><li class="listitem"><code class="code">-fixHdfsOverlaps</code> ability for fixing overlapping regions
		</li></ul></div>
When repairing overlapping regions, a region’s data can be modified on the file system in two
ways: 1) by merging regions into a larger region or 2) by sidelining regions by moving data to
“sideline” directory where data could be restored later. Merging a large number of regions is
technically correct but could result in an extremely large region that requires series of costly
compactions and splitting operations. In these cases, it is probably better to sideline the regions
that overlap with the most other regions (likely the largest ranges) so that merges can happen on
a more reasonable scale. Since these sidelined regions are already laid out in HBase’s native
directory and HFile format, they can be restored by using HBase’s bulk load mechanism.
The default safeguard thresholds are conservative. These options let you override the default
thresholds and to enable the large region sidelining feature.
	<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-maxMerge &lt;n&gt;</code> maximum number of overlapping regions to merge
		</li><li class="listitem"><code class="code">-sidelineBigOverlaps</code> if more than maxMerge regions are overlapping, sideline attempt
to sideline the regions overlapping with the most other regions.
		</li><li class="listitem"><code class="code">-maxOverlapsToSideline &lt;n&gt;</code> if sidelining large overlapping regions, sideline at most n
regions.
		</li></ul></div>
		
Since often times you would just want to get the tables repaired, you can use this option to turn
on all repair options:
	<div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="code">-repair</code> includes all the region consistency options and only the hole repairing table
integrity options.
		</li></ul></div>
Finally, there are safeguards to limit repairs to only specific tables. For example the following
command would only attempt to repair table TableFoo and TableBar.
<pre class="programlisting">$ ./bin/hbase/ hbck -repair TableFoo TableBar
</pre><div class="section" title="B.4.1.&nbsp;Special cases: Meta is not properly assigned"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e10702"></a>B.4.1.&nbsp;Special cases: Meta is not properly assigned</h3></div></div></div>
There are a few special cases that hbck can handle as well.
Sometimes the meta table’s only region is inconsistently assigned or deployed. In this case
there is a special <code class="code">-fixMetaOnly</code> option that can try to fix meta assignments.
<pre class="programlisting">$ ./bin/hbase hbck -fixMetaOnly -fixAssignments
</pre></div><div class="section" title="B.4.2.&nbsp;Special cases: HBase version file is missing"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e10711"></a>B.4.2.&nbsp;Special cases: HBase version file is missing</h3></div></div></div>
HBase’s data on the file system requires a version file in order to start. If this flie is missing, you
can use the <code class="code">-fixVersionFile</code> option to fabricating a new HBase version file. This assumes that
the version of hbck you are running is the appropriate version for the HBase cluster.	
	</div><div class="section" title="B.4.3.&nbsp;Special case: Root and META are corrupt."><div class="titlepage"><div><div><h3 class="title"><a name="d1934e10718"></a>B.4.3.&nbsp;Special case: Root and META are corrupt.</h3></div></div></div>
The most drastic corruption scenario is the case where the ROOT or META is corrupted and
HBase will not start. In this case you can use the OfflineMetaRepair tool create new ROOT
and META regions and tables.
This tool assumes that HBase is offline. It then marches through the existing HBase home
directory, loads as much information from region metadata files (.regioninfo files) as possible
from the file system. If the region metadata has proper table integrity, it sidelines the original root
and meta table directories, and builds new ones with pointers to the region directories and their
data.
<pre class="programlisting">$ ./bin/hbase org.apache.hadoop.hbase.util.OfflineMetaRepair
</pre>
NOTE: This tool is not as clever as uberhbck but can be used to bootstrap repairs that uberhbck
can complete.
If the tool succeeds you should be able to start hbase and run online repairs if necessary.
	</div></div>
	
        </p></div></div></div><div class="appendix" title="Appendix B. HBase中的压缩"><div class="titlepage"><div><div>
          <h2 class="title"><a name="compression"></a>Appendix&nbsp;C.&nbsp;HBase中的压缩</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="book.htm#compression.test">B.1. 测试压缩工具</a></span></dt><dt><span class="section"><a href="book.htm#hbase.regionserver.codecs">B.2. 
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    </a></span></dt><dt><span class="section"><a href="book.htm#lzo.compression">B.3. 
    LZO
    </a></span></dt><dt><span class="section"><a href="book.htm#gzip.compression">B.4. 
    GZIP
    </a></span></dt></dl></div><div class="section" title="B.1. 测试压缩工具"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="compression.test"></a>C.1.&nbsp;测试压缩工具</h2></div></div></div><p>
    HBase有一个用来测试压缩新的工具。要想运行它，输入<code class="code">/bin/hbase org.apache.hadoop.hbase.util.CompressionTest</code>. 就会有提示这个工具的具体用法
    </p></div><div class="section" title="B.2.  hbase.regionserver.codecs"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase.regionserver.codecs"></a>C.2.&nbsp;
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    </h2></div></div></div><p>
    如果你的安装错误，就会测试不成功，或者无法启动。可以在你的<code class="filename">hbase-site.xml</code>加上配置
    <code class="varname">
    hbase.regionserver.codecs
    </code>
    值你需要的codecs。例如，如果
    <code class="varname">
    hbase.regionserver.codecs
    </code> 的值是 <code class="code">lzo,gz</code> 同时lzo不存在或者没有正确安装， RegionServer在启动的时候会提示配置错误。
    </p><p>
     当一台新机器加入到集群中的时候，管理员一定要注意，这台新机器有可能要安装特定的压缩解码器。

    </p></div><div class="section" title="B.3.  LZO"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="lzo.compression"></a>C.3.&nbsp;
    LZO
    </h2></div></div></div>
      <p>很不幸，Hbase是Apache的协议，而LZO是GPL的协议。Hbase不能自带LZO，因此LZO需要在安装Hbase之前安装。参见 <a class="link" href="http://wiki.apache.org/hadoop/UsingLzoCompression" target="_top">使用 LZO 压缩</a>介绍了如何在Hbase中使用LZO </p>
    <p>一个常见的问题是，用户在一开始使用LZO的时候会很好，但是数月过去，管理员在给集群添加集群的时候，他们忘记了LZO的事情。在0.90.0版本之后，我们会运行失败，但也有可能不。</p>
    <p> See <a href="book.htm#hbase.regionserver.codecs.html" title="C.2.  hbase.regionserver.codecs">Section C.2, “ hbase.regionserver.codecs ”</a> for a feature to help protect against failed LZO install. </p>
    </div><div class="section" title="B.4.  GZIP"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="gzip.compression"></a>C.4.&nbsp;
    GZIP
    </h2></div></div></div><p>
	相对于LZO，GZIP的压缩率更高但是速度更慢。在某些特定情况下，压缩率是优先考量的。Java会使用Java自带的GZIP，除非Hadoop的本地库在CLASSPATH中。在这种情况下，最好使用本地压缩器。(如果本地库不存在，可以在Log看到很多<span class="emphasis"><em>Got brand-new compressor</em></span>。参见<a class="xref" href="book.htm#brand.new.compressor">Q:&nbsp;</a>)
<div class="section" title="C.5.&nbsp; SNAPPY"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="snappy.compression"></a>C.5.&nbsp;
    SNAPPY
    </h2></div></div></div><p>
        If snappy is installed, HBase can make use of it (courtesy of
        <a class="link" href="http://code.google.com/p/hadoop-snappy/" target="_top">hadoop-snappy</a>
        <sup>[<a name="d1934e10793" href="#ftn.d1934e10793" class="footnote">29</a>]</sup>).

        </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>
                    Build and install <a class="link" href="http://code.google.com/p/snappy/" target="_top">snappy</a> on all nodes
                    of your cluster.
                </p></li><li class="listitem"><p>
        Use CompressionTest to verify snappy support is enabled and the libs can be loaded ON ALL NODES of your cluster:
        </p><pre class="programlisting">$ hbase org.apache.hadoop.hbase.util.CompressionTest hdfs://host/path/to/hbase snappy</pre><p>
                </p></li><li class="listitem"><p>
        Create a column family with snappy compression and verify it in the hbase shell:
        </p><pre class="programlisting">$ hbase&gt; create 't1', { NAME =&gt; 'cf1', COMPRESSION =&gt; 'SNAPPY' }
hbase&gt; describe 't1'</pre><p>
        In the output of the "describe" command, you need to ensure it lists "COMPRESSION =&gt; 'SNAPPY'"
                </p></li></ol></div><p>

    </p><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d1934e10793" href="#d1934e10793" class="para">29</a>] </sup>See <a class="link" href="http://search-hadoop.com/m/Ds8d51c263B1/%2522Hadoop-Snappy+in+synch+with+Hadoop+trunk%2522&amp;subj=Hadoop+Snappy+in+synch+with+Hadoop+trunk" target="_top">Alejandro's note</a> up on the list on difference between Snappy in Hadoop
        and Snappy in HBase</p></div></div></div>
        <div class="section" title="C.6.&nbsp;Changing Compression Schemes"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="changing.compression"></a>C.6.&nbsp;Changing Compression Schemes</h2></div></div></div><p>A frequent question on the dist-list is how to change compression schemes for ColumnFamilies.  This is actually quite simple, 
      and can be done via an alter command.  Because the compression scheme is encoded at the block-level in StoreFiles, the table does 
      <span class="emphasis"><em>not</em></span> need to be re-created and the data does <span class="emphasis"><em>not</em></span> copied somewhere else.  Just make sure
      the old codec is still available until you are sure that all of the old StoreFiles have been compacted.
      </p></div>
      
    </p></div></div><div class="appendix" title="Appendix C. FAQ"><div class="titlepage"><div><div></div></div></div></div><div class="appendix" title="Appendix D. YCSB: 雅虎云服务 测试 和Hbase"><div class="titlepage"><div><div><h2 class="title"><a name="apd" id="apd"></a>Appendix&nbsp;D.&nbsp;<a class="link" href="https://github.com/brianfrankcooper/YCSB/" target="_top">YCSB: 雅虎云服务 测试</a> 和Hbase</h2></div></div></div><p>TODO: YCSB不能很多的增加集群负载.</p><p>TODO: 如果给Hbase安装</p><p>Ted Dunning重做了YCSV,这个是用maven管理了，加入了核实工作量的功能。参见 <a class="link" href="https://github.com/tdunning/YCSB" target="_top">Ted Dunning's YCSB</a>.</p></div>
    
    <div class="appendix" title="Appendix&nbsp;E.&nbsp;HFile format version 2"><div class="titlepage"><div><div><h2 class="title"><a name="hfilev2"></a>Appendix&nbsp;E.&nbsp;HFile format version 2</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="book.htm#d1934e10848">E.1. Motivation </a></span></dt><dt><span class="section"><a href="book.htm#apes02">E.2. HFile format version 1 overview </a></span></dt><dd><dl><dt><span class="section"><a href="book.htm#d1934e10883">E.2.1.  Block index format in version 1 </a></span></dt></dl></dd><dt><span class="section"><a href="book.htm#apes03">E.3. 
      HBase file format with inline blocks (version 2)
      </a></span></dt><dd><dl><dt><span class="section"><a href="book.htm#d1934e10910">E.3.1.  Overview</a></span></dt><dt><span class="section"><a href="book.htm#d1934e10925">E.3.2. Unified version 2 block format</a></span></dt><dt><span class="section"><a href="book.htm#d1934e10994">E.3.3.  Block index in version 2</a></span></dt><dt><span class="section"><a href="book.htm#d1934e11019">E.3.4. 
      Root block index format in version 2</a></span></dt><dt><span class="section"><a href="book.htm#d1934e11072">E.3.5. 
      Non-root block index format in version 2</a></span></dt><dt><span class="section"><a href="book.htm#d1934e11097">E.3.6. 
      Bloom filters in version 2</a></span></dt><dt><span class="section"><a href="book.htm#d1934e11134">E.3.7. File Info format in versions 1 and 2</a></span></dt><dt><span class="section"><a href="book.htm#d1934e11180">E.3.8. 
      Fixed file trailer format differences between versions 1 and 2</a></span></dt></dl></dd></dl></div><div class="section" title="E.1.&nbsp;Motivation"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="d1934e10848"></a>E.1.&nbsp;Motivation </h2></div></div></div><p>Note:  this feature was introduced in HBase 0.92</p><p>We found it necessary to revise the HFile format after encountering high memory usage and slow startup times caused by large Bloom filters and block indexes in the region server. Bloom filters can get as large as 100 MB per HFile, which adds up to 2 GB when aggregated over 20 regions. Block indexes can grow as large as 6 GB in aggregate size over the same set of regions. A region is not considered opened until all of its block index data is loaded. Large Bloom filters produce a different performance problem: the first get request that requires a Bloom filter lookup will incur the latency of loading the entire Bloom filter bit array.</p><p>To speed up region server startup we break Bloom filters and block indexes into multiple blocks and write those blocks out as they fill up, which also reduces the HFile writer’s memory footprint. In the Bloom filter case, “filling up a block” means accumulating enough keys to efficiently utilize a fixed-size bit array, and in the block index case we accumulate an “index block” of the desired size. Bloom filter blocks and index blocks (we call these “inline blocks”) become interspersed with data blocks, and as a side effect we can no longer rely on the difference between block offsets to determine data block length, as it was done in version 1.</p><p>HFile is a low-level file format by design, and it should not deal with application-specific details such as Bloom filters, which are handled at StoreFile level. Therefore, we call Bloom filter blocks in an HFile "inline" blocks. We also supply HFile with an interface to write those inline blocks. </p><p>Another format modification aimed at reducing the region server startup time is to use a contiguous “load-on-open” section that has to be loaded in memory at the time an HFile is being opened. Currently, as an HFile opens, there are separate seek operations to read the trailer, data/meta indexes, and file info. To read the Bloom filter, there are two more seek operations for its “data” and “meta” portions. In version 2, we seek once to read the trailer and seek again to read everything else we need to open the file from a contiguous block.</p></div></div>
      
      <div class="section" title="E.2.&nbsp;HFile format version 1 overview"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="apes02" id="apes02"></a>E.2.&nbsp;HFile format version 1 overview </h2></div></div></div><p>As we will be discussing the changes we are making to the HFile format, it is useful to give a short overview of the previous (HFile version 1) format. An HFile in the existing format is structured as follows:
           <span class="inlinemediaobject"><img src="src/hfile.png" alt="HFile Version 1" align="middle"></span>
           <sup>[<a name="d1934e10876" href="#ftn.d1934e10876" class="footnote">30</a>]</sup>
       </p><div class="section" title="E.2.1.&nbsp; Block index format in version 1"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e10883"></a>E.2.1.&nbsp; Block index format in version 1 </h3></div></div></div><p>The block index in version 1 is very straightforward. For each entry, it contains: </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Offset (long)</p></li><li class="listitem"><p>Uncompressed size (int)</p></li><li class="listitem"><p>Key (a serialized byte array written using Bytes.writeByteArray) </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Key length as a variable-length integer (VInt)
                  </p></li><li class="listitem"><p>
                     Key bytes
                 </p></li></ol></div></li></ol></div><p>The number of entries in the block index is stored in the fixed file trailer, and has to be passed in to the method that reads the block index. One of the limitations of the block index in version 1 is that it does not provide the compressed size of a block, which turns out to be necessary for decompression. Therefore, the HFile reader has to infer this compressed size from the offset difference between blocks. We fix this limitation in version 2, where we store on-disk block size instead of uncompressed size, and get uncompressed size from the block header.</p></div><div class="footnotes"><br><hr width="100" align="left"><div class="footnote"><p><sup>[<a id="ftn.d1934e10876" href="#d1934e10876" class="para">30</a>] </sup>Image courtesy of Lars George, <a class="link" href="http://www.larsgeorge.com/2009/10/hbase-architecture-101-storage.html" target="_top">hbase-architecture-101-storage.html</a>.</p></div></div></div>
                 
                 <div class="section" title="E.3.&nbsp; HBase file format with inline blocks (version 2)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="apes03" id="apes03"></a>E.3.&nbsp;
      HBase file format with inline blocks (version 2)
      </h2></div></div></div><div class="section" title="E.3.1.&nbsp; Overview"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e10910"></a>E.3.1.&nbsp; Overview</h3></div></div></div><p>The version of HBase introducing the above features reads both version 1 and 2 HFiles, but only writes version 2 HFiles. A version 2 HFile is structured as follows:
           <span class="inlinemediaobject"><img src="src/hfilev2.png" align="middle" alt="HFile Version 2"></span>

   </p></div><div class="section" title="E.3.2.&nbsp;Unified version 2 block format"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e10925"></a>E.3.2.&nbsp;Unified version 2 block format</h3></div></div></div><p>In the version 2 every block in the data section contains the following fields: </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>8 bytes: Block type, a sequence of bytes equivalent to version 1's "magic records". Supported block types are: </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>DATA – data blocks
                  </p></li><li class="listitem"><p>
                     LEAF_INDEX – leaf-level index blocks in a multi-level-block-index
                 </p></li><li class="listitem"><p>
                     BLOOM_CHUNK – Bloom filter chunks
                  </p></li><li class="listitem"><p>
                     META – meta blocks (not used for Bloom filters in version 2 anymore) 
                  </p></li><li class="listitem"><p>
                     INTERMEDIATE_INDEX – intermediate-level index blocks in a multi-level blockindex
                  </p></li><li class="listitem"><p>
                     ROOT_INDEX – root&gt;level index blocks in a multi&gt;level block index
                  </p></li><li class="listitem"><p>
                     FILE_INFO – the “file info” block, a small key&gt;value map of metadata
                  </p></li><li class="listitem"><p>
                     BLOOM_META – a Bloom filter metadata block in the load&gt;on&gt;open section
                  </p></li><li class="listitem"><p>
                     TRAILER – a fixed&gt;size file trailer. As opposed to the above, this is not an 
                     HFile v2 block but a fixed&gt;size (for each HFile version) data structure
                  </p></li><li class="listitem"><p>
                      INDEX_V1 – this block type is only used for legacy HFile v1 block
                  </p></li></ol></div></li><li class="listitem"><p>Compressed size of the block's data, not including the header (int).
         </p><p>
Can be used for skipping the current data block when scanning HFile data. 
                  </p></li><li class="listitem"><p>Uncompressed size of the block's data, not including the header (int)</p><p>
 This is equal to the compressed size if the compression algorithm is NON
                  </p></li><li class="listitem"><p>File offset of the previous block of the same type (long)</p><p>
 Can be used for seeking to the previous data/index block
                  </p></li><li class="listitem"><p>Compressed data (or uncompressed data if the compression algorithm is NONE).</p></li></ol></div><p>The above format of blocks is used in the following HFile sections:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Scanned block section. The section is named so because it contains all data blocks that need to be read when an HFile is scanned sequentially. &nbsp;Also contains leaf block index and Bloom chunk blocks. </p></li><li class="listitem"><p>Non-scanned block section. This section still contains unified-format v2 blocks but it does not have to be read when doing a sequential scan. This section contains “meta” blocks and intermediate-level index blocks.
         </p></li></ol></div><p>We are supporting “meta” blocks in version 2 the same way they were supported in version 1, even though we do not store Bloom filter data in these blocks anymore. </p></div><div class="section" title="E.3.3.&nbsp; Block index in version 2"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e10994"></a>E.3.3.&nbsp; Block index in version 2</h3></div></div></div><p>There are three types of block indexes in HFile version 2, stored in two different formats (root and non-root): </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Data index — version 2 multi-level block index, consisting of:</p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
 Version 2 root index, stored in the data block index section of the file
             </p></li><li class="listitem"><p>
Optionally, version 2 intermediate levels, stored in the non%root format in   the data index section of the file.    Intermediate levels can only be present if leaf level blocks are present
             </p></li><li class="listitem"><p>
Optionally, version 2 leaf levels, stored in the non%root format inline with   data blocks
             </p></li></ol></div></li><li class="listitem"><p>Meta index — version 2 root index format only, stored in the meta index section of the file</p></li><li class="listitem"><p>Bloom index — version 2 root index format only, stored in the “load-on-open” section as part of Bloom filter metadata.</p></li></ol></div></div><div class="section" title="E.3.4.&nbsp; Root block index format in version 2"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e11019"></a>E.3.4.&nbsp;
      Root block index format in version 2</h3></div></div></div><p>This format applies to:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Root level of the version 2 data index</p></li><li class="listitem"><p>Entire meta and Bloom indexes in version 2, which are always single-level. </p></li></ol></div><p>A version 2 root index block is a sequence of entries of the following format, similar to entries of a version 1 block index, but storing on-disk size instead of uncompressed size. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Offset (long) </p><p>
This offset may point to a data block or to a deeper&gt;level index block.
             </p></li><li class="listitem"><p>On-disk size (int) </p></li><li class="listitem"><p>Key (a serialized byte array stored using Bytes.writeByteArray) </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>Key (VInt)
             </p></li><li class="listitem"><p>Key bytes
             </p></li></ol></div></li></ol></div><p>A single-level version 2 block index consists of just a single root index block. To read a root index block of version 2, one needs to know the number of entries. For the data index and the meta index the number of entries is stored in the trailer, and for the Bloom index it is stored in the compound Bloom filter metadata.</p><p>For a multi-level block index we also store the following fields in the root index block in the load-on-open section of the HFile, in addition to the data structure described above:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Middle leaf index block offset</p></li><li class="listitem"><p>Middle leaf block on-disk size (meaning the leaf index block containing the reference to the “middle” data block of the file) </p></li><li class="listitem"><p>The index of the mid-key (defined below) in the middle leaf-level block.</p></li></ol></div><p></p><p>These additional fields are used to efficiently retrieve the mid-key of the HFile used in HFile splits, which we define as the first key of the block with a zero-based index of (n – 1) / 2, if the total number of blocks in the HFile is n. This definition is consistent with how the mid-key was determined in HFile version 1, and is reasonable in general, because blocks are likely to be the same size on average, but we don’t have any estimates on individual key/value pair sizes. </p><p></p><p>When writing a version 2 HFile, the total number of data blocks pointed to by every leaf-level index block is kept track of. When we finish writing and the total number of leaf-level blocks is determined, it is clear which leaf-level block contains the mid-key, and the fields listed above are computed. &nbsp;When reading the HFile and the mid-key is requested, we retrieve the middle leaf index block (potentially from the block cache) and get the mid-key value from the appropriate position inside that leaf block.</p></div><div class="section" title="E.3.5.&nbsp; Non-root block index format in version 2"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e11072"></a>E.3.5.&nbsp;
      Non-root block index format in version 2</h3></div></div></div><p>This format applies to intermediate-level and leaf index blocks of a version 2 multi-level data block index. Every non-root index block is structured as follows. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>numEntries: the number of entries (int). </p></li><li class="listitem"><p>entryOffsets: the “secondary index” of offsets of entries in the block, to facilitate a quick binary search on the key (numEntries + 1 int values). The last value is the total length of all entries in this index block. For example, in a non-root index block with entry sizes 60, 80, 50 the “secondary index” will contain the following int array: {0, 60, 140, 190}.</p></li><li class="listitem"><p>Entries. Each entry contains: </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
Offset of the block referenced by this entry in the file (long) 
             </p></li><li class="listitem"><p>
On&gt;disk size of the referenced block (int) 
             </p></li><li class="listitem"><p>
Key. The length can be calculated from entryOffsets.
             </p></li></ol></div></li></ol></div></div><div class="section" title="E.3.6.&nbsp; Bloom filters in version 2"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e11097"></a>E.3.6.&nbsp;
      Bloom filters in version 2</h3></div></div></div><p>In contrast with version 1, in a version 2 HFile Bloom filter metadata is stored in the load-on-open section of the HFile for quick startup. </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>A compound Bloom filter. </p><div class="orderedlist"><ol class="orderedlist" type="a"><li class="listitem"><p>
 Bloom filter version = 3 (int). There used to be a DynamicByteBloomFilter class that had the Bloom   filter version number 2
             </p></li><li class="listitem"><p>
The total byte size of all compound Bloom filter chunks (long)
             </p></li><li class="listitem"><p>
 Number of hash functions (int
             </p></li><li class="listitem"><p>
Type of hash functions (int)
             </p></li><li class="listitem"><p>
The total key count inserted into the Bloom filter (long)
             </p></li><li class="listitem"><p>
The maximum total number of keys in the Bloom filter (long)
             </p></li><li class="listitem"><p>
The number of chunks (int)
             </p></li><li class="listitem"><p>
Comparator class used for Bloom filter keys, a UTF&gt;8 encoded string stored   using Bytes.writeByteArray
             </p></li><li class="listitem"><p>
 Bloom block index in the version 2 root block index format
             </p></li></ol></div></li></ol></div></div><div class="section" title="E.3.7.&nbsp;File Info format in versions 1 and 2"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e11134"></a>E.3.7.&nbsp;File Info format in versions 1 and 2</h3></div></div></div><p>The file info block is a serialized <a class="ulink" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/io/HbaseMapWritable.html" target="_top">HbaseMapWritable</a> (essentially a map from byte arrays to byte arrays) with the following keys, among others. StoreFile-level logic adds more keys to this.</p><div class="informaltable"><table border="1"><colgroup><col><col></colgroup><tbody><tr><td>
               <p>hfile.LASTKEY </p>
            </td><td>
               <p>The last key of the file (byte array) </p>
            </td></tr><tr><td>
               <p>hfile.AVG_KEY_LEN </p>
            </td><td>
               <p>The average key length in the file (int) </p>
            </td></tr><tr><td>
               <p>hfile.AVG_VALUE_LEN </p>
            </td><td>
               <p>The average value length in the file (int) </p>
            </td></tr></tbody></table></div><p>File info format did not change in version 2. However, we moved the file info to the final section of the file, which can be loaded as one block at the time the HFile is being opened. Also, we do not store comparator in the version 2 file info anymore. Instead, we store it in the fixed file trailer. This is because we need to know the comparator at the time of parsing the load-on-open section of the HFile.</p></div><div class="section" title="E.3.8.&nbsp; Fixed file trailer format differences between versions 1 and 2"><div class="titlepage"><div><div><h3 class="title"><a name="d1934e11180"></a>E.3.8.&nbsp;
      Fixed file trailer format differences between versions 1 and 2</h3></div></div></div><p>The following table shows common and different fields between fixed file trailers in versions 1 and 2. Note that the size of the trailer is different depending on the version, so it is “fixed” only within one version. However, the version is always stored as the last four-byte integer in the file. </p><p></p><div class="informaltable"><table border="1"><colgroup><col class="c1"><col class="c2"></colgroup><tbody><tr><td>
               <p>Version 1 </p>
            </td><td>
               <p>Version 2 </p>
            </td></tr><tr><td colspan="2" align="center">
               <p>File info offset (long) </p>
            </td></tr><tr><td>
               <p>Data index offset (long) </p>
            </td><td>
                <p>loadOnOpenOffset (long)</p>
                <p><span class="emphasis"><em>The offset of the section that we need toload when opening the file.</em></span></p>
            </td></tr><tr><td colspan="2" align="center">
               <p>Number of data index entries (int) </p>
            </td></tr><tr><td>
               <p>metaIndexOffset (long)</p>
               <p>This field is not being used by the version 1 reader, so we removed it from version 2.</p>
            </td><td>
               <p>uncompressedDataIndexSize (long)</p>
               <p>The total uncompressed size of the whole data block index, including root-level, intermediate-level, and leaf-level blocks.</p>
            </td></tr><tr><td colspan="2" align="center">
               <p>Number of meta index entries (int) </p>
            </td></tr><tr><td colspan="2" align="center">
               <p>Total uncompressed bytes (long) </p>
            </td></tr><tr><td>
               <p>numEntries (int) </p>
            </td><td>
               <p>numEntries (long) </p>
            </td></tr><tr><td colspan="2" align="center">
               <p>Compression codec: 0 = LZO, 1 = GZ, 2 = NONE (int) </p>
            </td></tr><tr><td>
               <p></p>
            </td><td>
               <p>The number of levels in the data block index (int) </p>
            </td></tr><tr><td>
               <p></p>
            </td><td>
               <p>firstDataBlockOffset (long)</p>
               <p>The offset of the first first data block. Used when scanning. </p>
            </td></tr><tr><td>
               <p></p>
            </td><td>
               <p>lastDataBlockEnd (long)</p>
               <p>The offset of the first byte after the last key/value data block. We don't need to go beyond this offset when scanning. </p>
            </td></tr><tr><td>
               <p>Version: 1 (int) </p>
            </td><td>
               <p>Version: 2 (int) </p>
            </td></tr></tbody></table></div><p></p></div></div>
            
            <div class="appendix" title="Appendix&nbsp;F.&nbsp;Other Information About HBase"><div class="titlepage"><div><div><h2 class="title"><a name="other.info"></a>Appendix&nbsp;F.&nbsp;Other Information About HBase</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="other.info.html#other.info.videos">F.1. HBase Videos</a></span></dt><dt><span class="section"><a href="other.info.pres.html">F.2. HBase Presentations (Slides)</a></span></dt><dt><span class="section"><a href="other.info.papers.html">F.3. HBase Papers</a></span></dt><dt><span class="section"><a href="other.info.sites.html">F.4. HBase Sites</a></span></dt><dt><span class="section"><a href="other.info.books.html">F.5. HBase Books</a></span></dt><dt><span class="section"><a href="other.info.books.hadoop.html">F.6. Hadoop Books</a></span></dt></dl></div><div class="section" title="F.1.&nbsp;HBase Videos"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.videos"></a>F.1.&nbsp;HBase Videos</h2></div></div></div><p>Introduction to HBase
            </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><a class="link" href="http://www.cloudera.com/videos/chicago_data_summit_apache_hbase_an_introduction_todd_lipcon" target="_top">Introduction to HBase</a> by Todd Lipcon (Chicago Data Summit 2011).
			  </li><li class="listitem"><a class="link" href="http://www.cloudera.com/videos/intorduction-hbase-todd-lipcon" target="_top">Introduction to HBase</a> by Todd Lipcon (2010).
			  </li></ul></div><p>
         </p><p><a class="link" href="http://www.cloudera.com/videos/hadoop-world-2011-presentation-video-building-realtime-big-data-services-at-facebook-with-hadoop-and-hbase" target="_top">Building Real Time Services at Facebook with HBase</a> by Jonathan Gray (Hadoop World 2011).
         </p><p><a class="link" href="http://www.cloudera.com/videos/hw10_video_how_stumbleupon_built_and_advertising_platform_using_hbase_and_hadoop" target="_top">HBase and Hadoop, Mixing Real-Time and Batch Processing at StumbleUpon</a> by JD Cryans (Hadoop World 2010).
         </p></div></div>
         
         <div class="section" title="F.2.&nbsp;HBase Presentations (Slides)"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.pres"></a>F.2.&nbsp;HBase Presentations (Slides)</h2></div></div></div><p><a class="link" href="http://www.cloudera.com/resource/hadoop-world-2011-presentation-slides-advanced-hbase-schema-design" target="_top">Advanced HBase Schema Design</a> by Lars George (Hadoop World 2011).
         </p><p><a class="link" href="http://www.slideshare.net/cloudera/chicago-data-summit-apache-hbase-an-introduction" target="_top">Introduction to HBase</a> by Todd Lipcon (Chicago Data Summit 2011).
         </p><p><a class="link" href="http://www.slideshare.net/cloudera/hw09-practical-h-base-getting-the-most-from-your-h-base-install" target="_top">Getting The Most From Your HBase Install</a> by Ryan Rawson, Jonathan Gray (Hadoop World 2009).
         </p></div>
         <div class="section" title="F.3.&nbsp;HBase Papers"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.papers"></a>F.3.&nbsp;HBase Papers</h2></div></div></div><p><a class="link" href="http://research.google.com/archive/bigtable.html" target="_top">BigTable</a> by Google (2006).
         </p><p><a class="link" href="http://www.larsgeorge.com/2010/05/hbase-file-locality-in-hdfs.html" target="_top">HBase and HDFS Locality</a> by Lars George (2010).
         </p><p><a class="link" href="http://ianvarley.com/UT/MR/Varley_MastersReport_Full_2009-08-07.pdf" target="_top">No Relation: The Mixed Blessings of Non-Relational Databases</a> by Ian Varley (2009).
         </p></div>
         <div class="section" title="F.4.&nbsp;HBase Sites"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.sites"></a>F.4.&nbsp;HBase Sites</h2></div></div></div><p><a class="link" href="http://www.cloudera.com/blog/category/hbase/" target="_top">Cloudera's HBase Blog</a> has a lot of links to useful HBase information.
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><a class="link" href="http://www.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/" target="_top">CAP Confusion</a> is a relevant entry for background information on
			distributed storage systems.
			</li></ul></div><p>
         </p><p><a class="link" href="http://wiki.apache.org/hadoop/HBase/HBasePresentations" target="_top">HBase Wiki</a> has a page with a number of presentations.
         </p></div>
         
         <div class="section" title="F.5.&nbsp;HBase Books"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.books"></a>F.5.&nbsp;HBase Books</h2></div></div></div><p><a class="link" href="http://shop.oreilly.com/product/0636920014348.do" target="_top">HBase:  The Definitive Guide</a> by Lars George.
         </p></div>
         
         <div class="section" title="F.6.&nbsp;Hadoop Books"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="other.info.books.hadoop"></a>F.6.&nbsp;Hadoop Books</h2></div></div></div><p><a class="link" href="http://shop.oreilly.com/product/9780596521981.do" target="_top">Hadoop:  The Definitive Guide</a> by Tom White.
         </p></div>
         
         <div class="appendix" title="Appendix&nbsp;G.&nbsp;HBase History"><div class="titlepage"><div><div><h2 class="title"><a name="hbase.history"></a>Appendix&nbsp;G.&nbsp;HBase History</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">2006:  <a class="link" href="http://research.google.com/archive/bigtable.html" target="_top">BigTable</a> paper published by Google.
	  </li><li class="listitem">2006 (end of year):  HBase development starts.
	  </li><li class="listitem">2008:  HBase becomes Hadoop sub-project.
	  </li><li class="listitem">2010:  HBase becomes Apache top-level project.
	  </li></ul></div></div>
	  
	  <div class="appendix" title="Appendix&nbsp;H.&nbsp;HBase and the Apache Software Foundation"><div class="titlepage"><div><div><h2 class="title"><a name="asf"></a>Appendix&nbsp;H.&nbsp;HBase and the Apache Software Foundation</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="asf.html#asf.devprocess">H.1. ASF Development Process</a></span></dt><dt><span class="section"><a href="asf.reporting.html">H.2. ASF Board Reporting</a></span></dt></dl></div><p>HBase is a project in the Apache Software Foundation and as such there are responsibilities to the ASF to ensure
    a healthy project.</p><div class="section" title="H.1.&nbsp;ASF Development Process"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="asf.devprocess"></a>H.1.&nbsp;ASF Development Process</h2></div></div></div><p>See the <a class="link" href="http://www.apache.org/dev/#committers" target="_top">Apache Development Process page</a> 
        for all sorts of information on how the ASF is structured (e.g., PMC, committers, contributors), to tips on contributing
        and getting involved, and how open-source works at ASF.     
        </p></div></div>
        
        <div class="section" title="H.2.&nbsp;ASF Board Reporting"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="asf.reporting"></a>H.2.&nbsp;ASF Board Reporting</h2></div></div></div><p>Once a quarter, each project in the ASF portfolio submits a report to the ASF board.  This is done by the HBase project
         lead and the committers.  See <a class="link" href="http://www.apache.org/foundation/board/reporting" target="_top">ASF board reporting</a> for more information.
         </p></div>
         
    <div class="index" title="Index"><div class="titlepage"><div><div><h2 class="title"><a name="book_index"></a>Index</h2></div></div></div><div class="index"><div class="indexdiv"><h3>C</h3><dl><dt>Cells, <a class="indexterm" href="book.htm#cells">Cells</a></dt><dt>Column Family, <a class="indexterm" href="book.htm#columnfamily">Column Family</a></dt>
          <dt>Column Family Qualifier, <a href="book.htm#columnfamily.html">Column Family</a></dt>
            <dt>Compression, <a href="book.htm#compression.html">Compression In HBase</a></dt>
    </dl>
          <dl>
            <dd>&nbsp;</dd>
          </dl>
    </div><div class="indexdiv"><h3>H</h3><dl><dt>Hadoop, <a class="indexterm" href="book.htm#hadoop">hadoop</a></dt></dl></div><div class="indexdiv"><h3>L</h3><dl><dt> LargeTests </dt>
    </dl></div><div class="indexdiv"><h3>M</h3>
      <dl>
        <dt>MediumTests,</dt>
        <dt>MSLAB, <a href="book.htm#jvm.html#gcpause">Long GC pauses</a></dt>
      </dl>
      <h3>N</h3>
      <dl><dt>nproc, <a class="indexterm" href="book.htm#ulimit">
          ulimit
            和
          nproc
        </a></dt></dl></div><div class="indexdiv"><h3>S</h3>
          <dl>
            <dt>SmallTests,</dt>
          </dl>
          <h3>U</h3>
          <dl><dt>ulimit, <a class="indexterm" href="book.htm#ulimit">
          ulimit
            和
          nproc
        </a></dt></dl></div><div class="indexdiv"><h3>V</h3><dl><dt>Versions, <a class="indexterm" href="book.htm#versions">版本</a></dt></dl></div><div class="indexdiv"><h3>X</h3><dl><dt>xcievers, <a class="indexterm" href="book.htm#dfs.datanode.max.xcievers">dfs.datanode.max.xcievers</a></dt></dl></div><div class="indexdiv"><h3>Z</h3><dl><dt>ZooKeeper, <a class="indexterm" href="book.htm#zookeeper">ZooKeeper</a></dt></dl></div></div></div></div><script type="text/javascript" async="" src="src/ga.js"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-10148377-3']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script><style id="_clearly_component__css" type="text/css">#next_pages_container { width: 5px; hight: 5px; position: absolute; top: -100px; left: -100px; z-index: 2147483647 !important; } </style><div id="_clearly_component__next_pages_container"></div></body></html>